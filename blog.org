* Introduction
For my web scraping project, I've chosen to extract politics data from craigslist.org. My original ambition, though it proved difficult to affirm, was to prove a negative correlation to pro-trump chatter with conservatism. That is, I suspected that, somewhat counter-intuitively, the politics sections of more conservative states would be the less likely source of pro-trump posts. More generally, I sought to analyze the trends of politcs discussion of craiglist.

** notes
- synopsis
* Methodology
To extract data from craigslist, I used the Python Scrapy package. I executed some 500 requests, spread over 5 hours, to obtain roughly 40,000 posts titles/times. For each of these titles, there is a corresponding state and region, with some regions are additionally divided into subregions (the New York City region, for example, consists of Brooklyn, Queens, Manhattan, etc). Each post, its time and its geographical origin are represented with a single row in a 40k row Pandas DataFrame. Data corruption was not an issue. Additionally, I employed the 2010 U.S. census, which is available from http://www.census.gov, as well as 2016 election results data, which was also extracted from http://uselectionatlas.org/ using a simple BeautifulSoup extraction script. 
* TODO Data [0/2]
** TODO number of posts
** TODO data corruption

* State Usage
** DONE Introduction
*** intro
Although the post data has attached a fairly fine-grain geographical description, I found the CL regions in general to not line up well with any census bureau categories, and even in the event of such naming correspondence, the division of regions was at least questionable. One of the datasets most prominent outliers, District of Columbia, for example, has a census population of about 600k, yet a practical metropolitan area population in the several millions, grossly skewing its contributions to state-wide statistics. Therefore, regions and subregions were largely found to be unmanageably tedious to consider seriously in any analysis. States, however, having relatively little variation between practical occupancy and census population, and have indisputable borders, are ideal for inspection. 
*** terms
**** patronage
Patronage is the raw number of posts on the politics board. 
**** usage
Usage is my measure for a states proportional interest in the politics board. It is simply the normalized ratio of patronage and state population. 
**** trumpism
Trumpism is the name for a states republican vote percentage in the general election. It is used as a rough measure of how pro-Trump that state is.
** Population
*** Sample
We can get a feel for the distribution by taking a look at the following sample from the state_usage table
#+BEGIN_SRC ipython :session :results output raw drawer :noweb yes  :exports both
print(pd.concat([state_usage[:5], state_usage_space, state_usage[-5:].sort_values("usage")]))
#+END_SRC
#+RESULTS:
:RESULTS:
             patronage population       usage
Colorado          1982    5029196           1
Hawaii             445    1360301     0.81696
Montana            286     989415     0.71289
Oregon            1094    3831074    0.703323
Nevada             770    2700551    0.702141
*SPACE*         ------     ------      ------
North Dakota        19     672591           0
Vermont             18     625741  0.00141296
Kansas             106    2853118   0.0243361
Wyoming             22     563626   0.0294766
New Jersey         400    8791894   0.0471436
:END:
    
Seemingly some correlation between low population and low usage is evident from this table. However, the states for which the politics board is most popular are also generally small. This correlation is explored more by some political investigation, however, first outliers musbt be removed from the data.

*** Outliers
There are two major outlying states in the dataset: Colorodo and District of Columbia. 
**** Colorodo
We can see from the following that Colorado is an extreme outlier, being the fifth most popular state, yet the 23rd most populous. 
#+BEGIN_SRC ipython :session :file /home/dodge/workspace/craig-politics/py6320WCb.png :exports both
top_five = state_usage.sort_values("patronage")[-5:][::-1]
fig = plt.figure() # Create matplotlib figure

ax = fig.add_subplot(111) # Create matplotlib axes
ax2 = ax.twinx() # Create another axes that shares the same x-axis as ax.

width = 0.2

top_five.patronage.plot(kind='bar', color='#992255', ax=ax, width=width, position=1)
top_five.population.plot(kind='bar', color='#CC7733', ax=ax2, width=width, position=0)

ax.set_ylabel('Patronage')
ax2.set_ylabel('Population')

plt.show()
#+END_SRC
#+RESULTS:
[[file:/home/dodge/workspace/craig-politics/py6320WCb.png]]
Denver, for example, is especially large. Despite having a metropolitan area of less than 3 million people, Denver sees a patronage of 1187.
#+BEGIN_SRC ipython :session :results output raw drawer :noweb yes  :exports both
num_denver_posts = len(usa[usa.region == "denver, CO"])
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

By comparison, the "new york city" region, which is expansive enough as to include subregions like "new jersey", "long island", "fairfield", etc, has fewer posts, at 1006. 
#+BEGIN_SRC ipython :session :results output raw drawer :noweb yes :exports both
nyc_subregions = usa.groupby("region").get_group("new york city").subregion.unique().tolist()

num_nyc_posts = len(usa[usa.region == "new york city"])
#+END_SRC
#+RESULTS:
:RESULTS:
1006
:END:

**** District of Columbia
While I found Colorado to be an inexplicable anamoly, it was also justifiably accurate. District of Columbia, having a Republican voting rate of ~4% and the strict usage rate similar to that of Colorado, coupled with it's nebulous geographic distinction, meant its results were too extreme to consider in analysis.
** Correlations
*** Distributions
We can see the correlations between patronage, population, and usage, here. We of course expect correlation between patronage and population. Coolness represents lack of correlation.
#+BEGIN_SRC ipython :session :file /home/dodge/workspace/craig-politics/img/py224159fd.png :exports both
corr = state_usage.corr()
fig, ax = plt.subplots(figsize=(4, 4))
ax.matshow(corr)
plt.xticks(range(len(corr.columns)), corr.columns);
plt.yticks(range(len(corr.columns)), corr.columns);
#+END_SRC

#+RESULTS:
[[file:/home/dodge/workspace/craig-politics/img/py224159fd.png]]
We can see that usage and population correlate considerably. In more concrete numerical terms, using the pearson correlation coefficient:
#+BEGIN_SRC ipython :session :results output raw drawer :noweb yes
norm_usage = (state_usage - state_usage.min()) / (state_usage.max() - state_usage.min())
stats = pd.DataFrame({"mean": norm_usage.mean(), "median": norm_usage.median()})
print(("Mean/median of normalized state usage metrics:\n{0}").format(stats))
#+end_src
#+RESULTS:
:RESULTS:
Mean/median of normalized state usage metrics:
                mean    median
patronage   0.197488  0.091557
population  0.152608  0.105552
usage       0.264764  0.203740
:END:
We can see that usage has less variance than patronage and population, which we should expect. Perhaps it is somewhat more than expected, however. We expect (perhaps naively) for usage to coincide with population/patronage closely. 
#+BEGIN_SRC ipython :session :file /home/dodge/workspace/craig-politics/py6320cwT.png :exports both
norm_usage.plot(kind="density", title="Normalized PDF estimations", sharey=True)
#+END_SRC

#+RESULTS:
[[file:/home/dodge/workspace/craig-politics/py6320cwT.png]]

*** Usage per state
The distribution of usage among states seems reasonable.
#+BEGIN_SRC ipython :session :file /home/dodge/workspace/craig-politics/img/py22415jSF.png :exports both
ax = plt.subplot(111)  
ax.spines["top"].set_visible(False)  
ax.spines["right"].set_visible(False)  
    
ax.get_xaxis().tick_bottom()  
ax.get_yaxis().tick_left()  

plt.xlabel("Usage", fontsize=12)  
plt.ylabel("States", fontsize=12)     

plt.suptitle('State Usage Distribution', fontsize=14) 

plt.hist(state_usage.usage,
         color="#661111", bins=17)  
#+END_SRC

#+RESULTS:
[[file:/home/dodge/workspace/craig-politics/img/py22415jSF.png]]

#+END_SRC
*** Politics
#+BEGIN_SRC ipython :session :file /home/dodge/workspace/craig-politics/img/py22415k-v.png :exports both
post_politics = usa.join(states.trumpism, how="outer", on="state")

post_politics.filter(["trumpism", "state"]).plot(kind="hist", bins=14, color=["#FF9911"], title="Trumpism frequency distribution")
#+END_SRC

#+RESULTS:
[[file:/home/dodge/workspace/craig-politics/img/py22415k-v.png]]

* Text Qualities
** Introduction
   Text usage is interesting to consider, but difficult to evaluate. While sampling provides surpritising intuitions about the data, demonstration of any following ideas is quite difficult. The follow demonstrates some efforts to support some of my intuitive ideas about the obtained dat
** General
*** Vocabulary
- percentage
- number
**** Make bar graph of most popular words [0/0]
*** word cloud
*** semantics
*** unicode
** Politics
*** what words are used most by democrats
#+BEGIN_SRC ipython :session :file  :exports both
  lib_words = words(df=post_politics[post_politics.trumpism < .45], no_pop=True).rename("libs")
  conserv_words = words(df=post_politics[post_politics.trumpism > .55], no_pop=True).rename("conservs")
   
  rat = lambda df: df.libs/df.conservs
  ratio = pd.DataFrame().join([lib_words[lib_words >= 10], conserv_words[conserv_words >= 10]],
                                      how="outer").apply(rat, axis=1).dropna()
  ratio = ratio.rename("dem/rep ratio")
  lib_con_ratio = pd.DataFrame(posts_corpus).join(ratio.sort_values(ascending=False), how="inner")
  lib_con_ratio.sort("dem/rep ratio", ascending=False, inplace=True)
  lib_con_ratio[:10]

#+END_SRC
**** "against"
**** "won"
***** sample of when being used by liberals
***** semantics
*** diversity of words vs trumpism
#+BEGIN_SRC ipython :session :file  :exports both

#+END_SRC
*** "liberals" vs "conservatives"
**** pluralization
#+BEGIN_SRC ipython :session :file  :exports both
  ("singular/plural:\n" +
   "'conservative': {0:.3f}\n" +
   "'liberal': {1:.3f}\n").format(word_counts["conservative"]/float(word_counts["conservatives"]),
                                  word_counts["liberal"]/float(word_counts["liberals"]))

#+END_SRC
#+RESULTS:
: singular/plural:
: 'conservative': 0.628
: 'liberal': 2.198

**** How much more often is "liberal" mentioned than "conservative"?
Best way to visualize this?
#+BEGIN_SRC ipython :session :file  :exports both
  liberal = float(word_counts["liberal"])
  liberal_p = float(word_counts["liberals"])
  conserv = float(word_counts["conservative"])
  conserv_p = float(word_counts["conservatives"])


  print ("liberal/conservative: {0:.2f}\n" +
   "liberals/conservatives: {1:.2f}\n" +
   "liberal(s)/conservative(s): {2:.2f}" +
    "\n") .format(liberal/conserv,
                  liberal_p/conserv_p,
                  (liberal+liberal_p)/(conserv+conserv_p))

#+END_SRC

#+RESULTS:

**** How much more often is "liberals" capitalized?
**** How much more often is "liberals" mentioned in liberal states?
*** "trump" vs "clinton" vs "obama"
**** "trump" usage / total usage
#+BEGIN_SRC ipython :session :file  :exports both

#+END_SRC
**** "trump" usage / trumpism
**** upcase usage / trumpism
**** trumpism
#+BEGIN_SRC ipython :session :file /home/dodge/workspace/craig-politics/py6320hB1.png :exports both
trump_posts = usa.join(voting, on="state").join(find_strs("trump"), how="inner")

print "Sampling posts from especially anti-trump states:\n{0}".format(t[t.trumpism < .4].title.sample(10))

print "\nPolitically liberal states composing the above sampling:\n{0}".format(t[t.trumpism < .4].groupby("state").sum().index.tolist())
#+END_SRC
*** Semantics
#+BEGIN_SRC ipython :session :file /home/dodge/workspace/craig-politics/py6320TLE.png :exports both
  from textblob import TextBlob

  def semants(text):
      blob = TextBlob(text)
      ss = 0
      for sentence in blob.sentences:
          ss += sentence.sentiment.polarity

      return float(ss)/len(blob.sentences)

 semantics = ascii_posts.title.map(lambda x: semants(x)).rename("semants")
 semant = eval_strs("trump", df=ascii_posts).join(pd.DataFrame(semantics))
 sems_usa = semant.join(usa, how="inner")
 trumps_semantics = sems_usa.groupby("state").mean().join(voting, how="inner").sort_values("semants").corr()

trumps_semantics
#+END_SRC

*** Unicode
* conclusion
** "liberals" more likely to be used in liberal states than conservative states
