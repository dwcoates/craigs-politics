<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2017-01-09 Mon 05:13 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title><b><font color = "C2492F">Scraping the Bottom of the Barrel</font></b></title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Dodge Coates" />
<style type="text/css">
  html{font-family:sans-serif;
       -ms-text-size-adjust:100%;
       -webkit-text-size-adjust:100%}
  body{margin:0}
  article,aside,details,figcaption,figure,footer,header,main,menu,nav,section,summary{display:block}
  audio,canvas,progress,video{display:inline-block}
  audio:not([controls]){display:none;
                        height:0}
  progress{vertical-align:baseline}
  [hidden],template{display:none}
  a{background-color:transparent;
    -webkit-text-decoration-skip:objects}
  a:active,a:hover{outline-width:0}
  abbr[title]{border-bottom:none;
              text-decoration:underline;
              text-decoration:underline dotted}
  b,strong{font-weight:inherit;
           font-weight:bolder}
  dfn{font-style:italic}
  h1{font-size:2em;
     margin:.67em 0}
  mark{background-color:#ff0;
       color:#000}
  small{font-size:80%}
  sub,sup{font-size:75%;
          line-height:0;
          position:relative;
          vertical-align:baseline}
  sub{bottom:-.25em}
  sup{top:-.5em}
  img{border-style:none}
  svg:not(:root){overflow:hidden}
  code,kbd,pre,samp{font-family:monospace;
                    font-size:1em}
  figure{margin:1em 40px}
  hr{box-sizing:content-box;
     height:0;
     overflow:visible}
  button,input,select,textarea{font:inherit;
                               margin:0}
  optgroup{font-weight:700}
  button,input{overflow:visible}
  button,select{text-transform:none}
  [type=reset],[type=submit],button,html [type=button]{-webkit-appearance:button}
  [type=button]::-moz-focus-inner,[type=reset]::-moz-focus-inner,[type=submit]::-moz-focus-inner,button::-moz-focus-inner{border-style:none;
                                                                                                                          padding:0}
  [type=button]:-moz-focusring,[type=reset]:-moz-focusring,[type=submit]:-moz-focusring,button:-moz-focusring{outline:1px dotted ButtonText}
  fieldset{border:1px solid silver;
           margin:0 2px;
           padding:.35em .625em .75em}
  legend{box-sizing:border-box;
         color:inherit;
         display:table;
         max-width:100%;
         padding:0;
         white-space:normal}
  textarea{overflow:auto}
  [type=checkbox],[type=radio]{box-sizing:border-box;
                               padding:0}
  [type=number]::-webkit-inner-spin-button,[type=number]::-webkit-outer-spin-button{height:auto}
  [type=search]{-webkit-appearance:textfield;
                outline-offset:-2px}
  [type=search]::-webkit-search-cancel-button,[type=search]::-webkit-search-decoration{-webkit-appearance:none}
  ::-webkit-input-placeholder{color:inherit;
                              opacity:.54}
  ::-webkit-file-upload-button{-webkit-appearance:button;
                               font:inherit}
  body{width:95%;
       margin:2%;
       font:normal normal normal 17px/1.6em Helvetica,sans-serif;
       color:#333}
  @media (min-width:769px){body{width:700px;
                                margin-left:5vw}
  }
  .title{margin:auto;
         color:#000}
  .subtitle,.title{text-align:center}
  .subtitle{font-size:medium;
            font-weight:700}
  .abstract{margin:auto;
            width:80%;
            font-style:italic}
  .abstract p:last-of-type:before{content:"    ";
                                  white-space:pre}
  .status{font-size:90%;
          margin:2em auto}
  [class^=section-number-]{margin-right:.5em}
  #footnotes{font-size:90%}
  .footpara{display:inline;
            margin:.2em auto}
  .footdef{margin-bottom:1em}
  .footdef sup{padding-right:.5em}
  a{color:#527d9a;
    text-decoration:none}
  a:hover{color:#035;
          border-bottom:1px dotted}
  figure{padding:0;
         margin:0;
         text-align:center}
  img{max-width:100%;
      vertical-align:middle}
  @media (min-width:769px){img{max-width:85vw;
                               margin:auto}
  }
  .MathJax_Display{margin:0!important;
                   width:90%!important}
  h1,h2,h3,h4,h5,h6{color:#a5573e;
                    line-height:1.6em;
                    font-family:Georgia,serif}
  h4,h5,h6{font-size:1em}
  dt{font-weight:700}
  table{margin:auto;
        border-top:2px solid;
        border-collapse:collapse}
  table,thead{border-bottom:2px solid}
  table td+td,table th+th{border-left:1px solid gray}
  table tr{border-top:1px solid #d3d3d3}
  td,th{padding:5px 10px;
        vertical-align:middle}
  caption.t-above{caption-side:top}
  caption.t-bottom{caption-side:bottom}
  th.org-center,th.org-left,th.org-right{text-align:center}
  td.org-right{text-align:right}
  td.org-left{text-align:left}
  td.org-center{text-align:center}
  code{padding:2px 5px;
       margin:auto 1px;
       border:1px solid #ddd;
       border-radius:3px;
       background-clip:padding-box;
       color:#333;
       font-size:80%}
  blockquote{margin:1em 2em;
             padding-left:1em;
             border-left:3px solid #ccc}
  kbd{background-color:#f7f7f7;
      font-size:80%;
      margin:0 .1em;
      padding:.1em .6em}
  .todo{color:red}
  .done,.todo{font-family:Lucida Console,monospace}
  .done{color:green}
  .priority{color:orange}
  .priority,.tag{font-family:Lucida Console,monospace}
  .tag{background-color:#eee;
       font-size:80%;
       font-weight:400;
       padding:2px}
  .timestamp{color:#bebebe}
  .timestamp-kwd{color:#5f9ea0}
  .org-right{margin-left:auto;
             margin-right:0;
             text-align:right}
  .org-left{margin-left:0;
            margin-right:auto;
            text-align:left}
  .org-center{margin-left:auto;
              margin-right:auto;
              text-align:center}
  .underline{text-decoration:underline}
  #postamble p,#preamble p{font-size:90%;
                           margin:.2em}
  p.verse{margin-left:3%}
  pre{border:1px solid #ccc;

      box-shadow:3px 3px 3px #eee;
      font-family:Lucida Console,monospace;
      margin:1.2em;
      padding:8pt}
  pre.src{overflow:auto;
          padding-top:1.2em;
          position:relative;
          font-size:80%}
  pre.src:before{background-color:#fff;
                 border:1px solid #000;
                 display:none;
                 padding:3px;
                 position:absolute;
                 right:10px;
                 top:.6em}
  pre.src:hover:before{display:inline}
  pre.src-sh:before{content:'sh'}
  pre.src-bash:before{content:'bash'}
  pre.src-emacs-lisp:before{content:'Emacs Lisp'}
  pre.src-R:before{content:'R'}
  pre.src-org:before{content:'Org'}
  pre.src-c+:before{content:'C++'}
  pre.src-c:before{content:'C'}
  pre.src-html:before{content:'HTML'}
  pre.example{overflow:auto;
              padding-top:1.2em;
              position:relative;
              font-size:80%}
  .inlinetask{background:#ffc;
              border:2px solid gray;
              margin:10px;
              padding:10px}
  #org-div-home-and-up{font-size:70%;
                       text-align:right;
                       white-space:nowrap}
  .linenr{font-size:smaller}
  .code-highlighted{background-color:#ff0}
  #bibliography{font-size:90%}
  #bibliography table{width:100%}
  .creator{display:block}@media (min-width:769px){.creator{display:inline;
                                                           float:right}}</style>
<link href="/home/dodge/.emacs.d/leuven-theme.css" rel="stylesheet">
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<h1 class="title"><b><font color = "C2492F">Scraping the Bottom of the Barrel</font></b></h1>


<div id="outline-container-orgc1e46e4" class="outline-2">
<h2 id="orgc1e46e4">Introduction</h2>
<div class="outline-text-2" id="text-orgc1e46e4">
<p>
For my web scraping project, I've chosen to extract some of the
politics data from craigslist.org. My original ambition, though it
proved difficult to affirm, was to prove a small, non-existant, or
negative correllation of pro-trump chatter to expected
conservatism. That is, I suspected that, somewhat counter-intuitively,
the politics sections of more conservative states would a
disproportionately less likely source of pro-trump posts. My basis for
this suspicion was my general observation that less regulated areas
for discussion on the internet tend to be very attractive to those
members of a publically socially disparaged minority. Recognizing
that, among other clues, Trump supporters in largely pro-Clinton
geographic areas are disparaged for their support in amounts
disproportionate to their surprisingly high representation, it
followed that I could expect a surprising amount of pro-Trump (mostly
trollish) chatter in mostly liberal places (e.g., New York City). The
positive sentiment aspect of that hypothesis proved to be difficult to
convincingly affirm. More generally, also I sought to analyze the
trends of politcs discussion of craiglist, mostly in the area of text
usage (capitalization, word frequency, etc) vs political leaning.
</p>


<div class="figure">
<p><img src="./img/Trump_cloud_proper.png" alt="Trump_cloud_proper.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-orga4c90e7" class="outline-2">
<h2 id="orga4c90e7">Methodology</h2>
<div class="outline-text-2" id="text-orga4c90e7">
<p>
To extract data from craigslist, I used the Python Scrapy package,
which was probably overkill. Originally, I intended to collect post
bodies as well as the titles, however this would require about 100
times as many request, too many for me to reponsibly exectute in a
reasonable amount of time. I resigned to limiting myself to titles,
which involved about 500 requests, spread over 5 hours, to obtain
roughly 40,000 posts titles/times. For each of these titles, there is
a corresponding state and region, with some regions additionally
divided into subregions (the New York City region, for example,
consists of Brooklyn, Queens, Manhattan, etc). Each post, its time and
its geographical origin are represented with a single row in a 40k row
Pandas DataFrame, <code>usa</code>. Data corruption was not an issue, as the CL
layout is quite uniform, though I did need to take into account data
redundancy (e.g., occaisionally "regions" are also "subregions" of
sibling regions). To make use of the extracted post title data, I
employed the 2010 U.S. census, which is available from
<a href="http://www.census.gov">http://www.census.gov</a>, as well as the 2016 election results data,
which I scraped from <a href="http://uselectionatlas.org/">http://uselectionatlas.org/</a> using a BeautifulSoup
extraction script.
</p>
</div>
</div>

<div id="outline-container-orgfd53e1a" class="outline-2">
<h2 id="orgfd53e1a">Preparing data</h2>
<div class="outline-text-2" id="text-orgfd53e1a">
<p>
The <a href="https://github.com/dwcoates/craigs-politics/tree/master/craigcrawler">craigslist extractor</a>, written as a scrapy program, collected
titles and dates, as well as the corresponding geographical
designations. To keep it simple, these are all stored together as rows
in a csv file. 
</p>
</div>
<div class="outline-text-2" id="text-orgfd53e1a">
<p>
Reading this data from craigcrawler's file:
</p>
<div class="org-src-container">
<pre class="src src-org">38,692 total posts exctracted from 416 regions over 52 states. The most 
frequented state was 'California', and the most frequented region was,
surprisingly, 'denver, CO'.
</pre>
</div>
</div>
<div id="outline-container-org335e50d" class="outline-3">
<h3 id="org335e50d"><code>usa</code> Sample</h3>
<div class="outline-text-3" id="text-org335e50d">
<p>
Sample of posts in the <code>usa</code> DataFrame before preprocessing, which is the DF for storing all CL politics posts:
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="all" frame="all">


<colgroup>
<col  class="org-right" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-right">&#xa0;</th>
<th scope="col" class="org-left">title</th>
<th scope="col" class="org-left">date</th>
<th scope="col" class="org-left">state</th>
<th scope="col" class="org-left">region</th>
<th scope="col" class="org-left">subregion</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">18708</td>
<td class="org-left">Let there be no confusion about the definition.</td>
<td class="org-left">2016-12-28 13:10</td>
<td class="org-left">Pennsylvania</td>
<td class="org-left">york, PA</td>
<td class="org-left">nan</td>
</tr>

<tr>
<td class="org-right">7369</td>
<td class="org-left">Trump't Music Video</td>
<td class="org-left">2016-11-30 15:57</td>
<td class="org-left">Michigan</td>
<td class="org-left">detroit metro</td>
<td class="org-left">macomb co</td>
</tr>

<tr>
<td class="org-right">14070</td>
<td class="org-left">re Crime facts</td>
<td class="org-left">2016-12-24 10:50</td>
<td class="org-left">California</td>
<td class="org-left">los angeles</td>
<td class="org-left">long beach / 562</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="outline-container-org63559bb" class="outline-3">
<h3 id="org63559bb">U.S. Census 2010</h3>
<div class="outline-text-3" id="text-org63559bb">
</div><div id="outline-container-orge71af92" class="outline-4">
<h4 id="orge71af92">Census Data</h4>
<div class="outline-text-4" id="text-orge71af92">
<p>
Census data is collected from U.S. Census Bureau for <a href="http://www.census.gov/2010census/">2010
census</a>. Here's a sample:
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="all" frame="all">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">state</th>
<th scope="col" class="org-right">population</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Illinois</td>
<td class="org-right">1.28306e+07</td>
</tr>

<tr>
<td class="org-left">Michigan</td>
<td class="org-right">9.88364e+06</td>
</tr>

<tr>
<td class="org-left">Maryland</td>
<td class="org-right">5.77355e+06</td>
</tr>

<tr>
<td class="org-left">Mississippi</td>
<td class="org-right">2.9673e+06</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div id="outline-container-orgc6fe5c6" class="outline-3">
<h3 id="orgc6fe5c6">U.S. 2016 Election</h3>
<div class="outline-text-3" id="text-orgc6fe5c6">
<p>
The 2016 Election results will be useful. They are grabbed from a really nice site, <a href="http://uselectionatlas.org/RESULTS/data.php?year=2016&amp;datatype=national&amp;def=1&amp;f=1&amp;off=0&amp;elect=0">uselectionsatlas.org</a>:
</p>
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> requests
<span class="org-keyword">from</span> scrapy <span class="org-keyword">import</span> Selector

<span class="org-variable-name">atlas_url</span> = (<span class="org-string">"http://uselectionatlas.org/RESULTS/data.php?year"</span> +
<span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span><span class="org-string">"=2016&amp;datatype=national&amp;def=1&amp;f=1&amp;off=0&amp;elect=0"</span>)
<span class="org-variable-name">atlas_source</span> = requests.get(atlas_url).text
<span class="org-variable-name">select</span> = Selector(text=atlas_source).xpath(<span class="org-string">'//*[@id="datatable"]/tbody/tr'</span>)

<span class="org-variable-name">convert</span> = <span class="org-keyword">lambda</span> s: <span class="org-builtin">int</span>(s.replace(<span class="org-string">','</span>, <span class="org-string">''</span>))
<span class="org-variable-name">vote_names</span> = <span class="org-builtin">map</span>(<span class="org-builtin">str</span>, select.xpath(<span class="org-string">'td[3]/a/text()'</span>).extract())
<span class="org-comment-delimiter"># </span><span class="org-comment">Correct name for DC</span>
<span class="org-variable-name">vote_names</span>[8] = <span class="org-string">"District of Columbia"</span>
<span class="org-variable-name">clinton_votes</span> = <span class="org-builtin">map</span>(convert, select.xpath(<span class="org-string">'td[17]/text()'</span>).extract())
<span class="org-variable-name">trump_votes</span> = <span class="org-builtin">map</span>(convert, select.xpath(<span class="org-string">'td[18]/text()'</span>).extract())

<span class="org-variable-name">gen_votes</span> = pd.DataFrame({<span class="org-string">"clinton"</span>: clinton_votes, <span class="org-string">"trump"</span>: trump_votes},
<span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>index=vote_names)

<span class="org-comment-delimiter"># </span><span class="org-comment">Dub a states Rebublican vote rate "trumpism"</span>
<span class="org-variable-name">trump_favor</span> = pd.DataFrame(gen_votes[<span class="org-string">"trump"</span>]/gen_votes.<span class="org-builtin">sum</span>(axis=1),
<span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>  columns=[<span class="org-string">"trumpism"</span>],
<span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>  index=vote_names)
<span class="org-variable-name">voting</span> = gen_votes.join(trump_favor).sort_values(<span class="org-string">"trumpism"</span>, ascending=<span class="org-constant">False</span>)
<span class="org-variable-name">voting</span> = voting.drop(<span class="org-string">"District of Columbia"</span>)
</pre>
</div>

<p>
Sample of voting table:
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="all" frame="all">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-right">clinton</th>
<th scope="col" class="org-right">trump</th>
<th scope="col" class="org-right">trumpism</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Wyoming</td>
<td class="org-right">55973</td>
<td class="org-right">174419</td>
<td class="org-right">0.757</td>
</tr>

<tr>
<td class="org-left">West Virginia</td>
<td class="org-right">188794</td>
<td class="org-right">489371</td>
<td class="org-right">0.722</td>
</tr>

<tr>
<td class="org-left">North Dakota</td>
<td class="org-right">93758</td>
<td class="org-right">216794</td>
<td class="org-right">0.698</td>
</tr>

<tr>
<td class="org-left"><b>SPACE</b></td>
<td class="org-right">------</td>
<td class="org-right">------</td>
<td class="org-right">------</td>
</tr>

<tr>
<td class="org-left">Hawaii</td>
<td class="org-right">266891</td>
<td class="org-right">128847</td>
<td class="org-right">0.326</td>
</tr>

<tr>
<td class="org-left">California</td>
<td class="org-right">8753788</td>
<td class="org-right">4483810</td>
<td class="org-right">0.339</td>
</tr>

<tr>
<td class="org-left">Vermont</td>
<td class="org-right">178573</td>
<td class="org-right">95369</td>
<td class="org-right">0.348</td>
</tr>
</tbody>
</table>
</div>
</div>

<div id="outline-container-orgba6d064" class="outline-3">
<h3 id="orgba6d064">Preprocess Data</h3>
<div class="outline-text-3" id="text-orgba6d064">
<p>
A small bit of preprocessing to check data for corruption and
unexpected results was necessary. There was no missing data, and no
corruption. I suspected that I might encounter some amount of
redundancy, but the extractor was written to exclude duplicated links,
and it happened to be the case that CL keys areas unique across highly
related (sub)regions (e.g., "long island" subregion and "long island,
NY" region seem like they might be the same, but are actually
completely distinct).
</p>
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-comment-delimiter"># </span><span class="org-comment">Drop empty regions. Some regions are too small to have any posts.</span>
<span class="org-variable-name">usa</span> = usa_raw.dropna(subset=[<span class="org-string">"title"</span>, <span class="org-string">"date"</span>], how=<span class="org-string">"any"</span>, axis=0)
<span class="org-keyword">assert</span> <span class="org-builtin">len</span>(postless_regions) == <span class="org-builtin">len</span>(usa_raw)-<span class="org-builtin">len</span>(usa)

<span class="org-comment-delimiter"># </span><span class="org-comment">Get rid of territories (Guam, Puerto Rico).</span>
<span class="org-variable-name">usa</span> = usa[usa[<span class="org-string">"state"</span>] != <span class="org-string">"Territories"</span>]
<span class="org-comment-delimiter"># </span><span class="org-comment">Get rid of "District of Columbia"</span>
<span class="org-variable-name">usa</span> = usa[usa[<span class="org-string">"state"</span>] != <span class="org-string">"District of Columbia"</span>]
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-orgbbb70a1" class="outline-2">
<h2 id="orgbbb70a1">State Usage</h2>
<div class="outline-text-2" id="text-orgbbb70a1">
<p>
Although the post data has attached a fairly fine-grain geographical
description, I found the CL regions in general to not line up well
with any census bureau categories. Moreover, even in the lucky event
of such name correspondence, the division of regions was at least
questionable. For example, by far the datasets most prominent "state"
outliers, District of Columbia, has a census population of about 600k,
yet a practical metropolitan area population in the several millions,
a disparity that gross skews its contributions to state-wide
statistics. Therefore, regions and subregions were largely found to be
unmanageably tedious to consider seriously in any analysis. States,
however, having relatively little variation between practical
occupancy and census population, and have indisputable borders,
barring District of Columbia, are ideal for inspection.
</p>
</div>
<div id="outline-container-org587fa6f" class="outline-3">
<h3 id="org587fa6f">Terms</h3>
<div class="outline-text-3" id="text-org587fa6f">
<ol class="org-ol">
<li><b>Patronage</b>
Patronage is the raw number of posts on a politics board.</li>
<li><b>Usage</b>
Usage is my measure for a states proportional interest in the
politics board. It is simply the normalized ratio of patronage and
state population.</li>
<li><b>Trumpism</b>
Trumpism is the name for a states republican vote percentage in the
general election. It is used as a rough measure of how pro-Trump
rate of a given state, and is a column in the <code>voting</code> DataFrame,
which is comprised of scraped data on the 2016 General Election
results.</li>
</ol>
</div>
</div>
<div class="outline-text-2" id="text-orgbbb70a1">
<p>
The <code>state_usage</code> table is the census table concatenated with patronage usage.
</p>
</div>
<div id="outline-container-org08e71cf" class="outline-3">
<h3 id="org08e71cf"><code>states</code> Sample</h3>
<div class="outline-text-3" id="text-org08e71cf">
<p>
Joining <code>state_usage</code> with <code>voting</code> gives us a decent top down view of
state political tendencies on CL.
</p>
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">states</span> = state_usage.join(voting, how=<span class="org-string">"left"</span>).sort_values(<span class="org-string">"usage"</span>)
</pre>
</div>

<table border="2" cellspacing="0" cellpadding="6" rules="all" frame="all">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">state</th>
<th scope="col" class="org-right">patronage</th>
<th scope="col" class="org-right">population</th>
<th scope="col" class="org-right">usage</th>
<th scope="col" class="org-right">clinton</th>
<th scope="col" class="org-right">trump</th>
<th scope="col" class="org-right">trumpism</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Tennessee</td>
<td class="org-right">487</td>
<td class="org-right">6.34610e+06</td>
<td class="org-right">0.132544</td>
<td class="org-right">870695</td>
<td class="org-right">1.52292e+06</td>
<td class="org-right">0.636243</td>
</tr>

<tr>
<td class="org-left">Connecticut</td>
<td class="org-right">272</td>
<td class="org-right">3.5741e+06</td>
<td class="org-right">0.130803</td>
<td class="org-right">897572</td>
<td class="org-right">673215</td>
<td class="org-right">0.428585</td>
</tr>

<tr>
<td class="org-left">Delaware</td>
<td class="org-right">100</td>
<td class="org-right">897934</td>
<td class="org-right">0.227191</td>
<td class="org-right">235603</td>
<td class="org-right">185127</td>
<td class="org-right">0.440014</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="outline-container-orgdb3d27f" class="outline-3">
<h3 id="orgdb3d27f">Outliers</h3>
<div class="outline-text-3" id="text-orgdb3d27f">
<p>
There are two major outlying states in the dataset: Colorodo and
District of Columbia.
</p>
</div>
<div id="outline-container-org138c7bd" class="outline-4">
<h4 id="org138c7bd">Colorodo</h4>
<div class="outline-text-4" id="text-org138c7bd">
<p>
We can see from the following that Colorado is an extreme outlier,
being the fifth most popular state, yet the 23rd most populous.
</p>

<div class="figure">
<p><img src="./img/py6320WCb.png" alt="py6320WCb.png" />
</p>
</div>

<p>
Denver, as a region, is also especially large. Despite having a
population of 650,000 people (and a metropolitcan area of 3 million),
Denver sees a patronage of 1187.
By comparison, the "new york city" region, which is expansive enough
as to include metropolitan area subregions like "new jersey", "long island",
"fairfield", etc, has fewer posts, at 1006.
</p>
<div class="org-src-container">
<pre class="src src-org">1006 posts in NYC spread over:
manhattan,
brooklyn,
queens,
bronx,
staten island,
new jersey,
long island,
westchester,
and fairfield.

This is ~6.5% the usage rate of Denver
</pre>
</div>

<p>
This is a remarkably popular region, clearly. I suspect that it has to
do with the region granularity CL mostly likely arbitrarily assigned
to the state. They might want to consider providing mode regions to
the state of Colorado.
</p>
</div>
</div>
<div id="outline-container-orga6987af" class="outline-4">
<h4 id="orga6987af">District of Columbia</h4>
<div class="outline-text-4" id="text-orga6987af">
<p>
While I found Colorado to be an inexplicable anamoly, it was also
justifiably accurate. District of Columbia, having a Republican voting
rate of ~4% and the usage similar to that of Colorado, coupled with
it's unclear geographic distinction and population, meant its results
were too extreme and variable to consider in analysis. Besides, it's
not even a real state&#x2026;
</p>
</div>
</div>
</div>

<div id="outline-container-orge4d7234" class="outline-3">
<h3 id="orge4d7234">Patronage</h3>
<div class="outline-text-3" id="text-orge4d7234">

<div class="figure">
<p><img src="./img/py6320oYD.png" alt="py6320oYD.png" />
</p>
</div>

<p>
We can get a feel for the usage distribution by taking a look at the
following sample from the <code>state_usage</code> table:
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="all" frame="all">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-right">patronage</th>
<th scope="col" class="org-right">population</th>
<th scope="col" class="org-right">usage</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Colorado</td>
<td class="org-right">1982</td>
<td class="org-right">5029196</td>
<td class="org-right">1.0</td>
</tr>

<tr>
<td class="org-left">Hawaii</td>
<td class="org-right">445</td>
<td class="org-right">1360301</td>
<td class="org-right">0.817</td>
</tr>

<tr>
<td class="org-left">Montana</td>
<td class="org-right">286</td>
<td class="org-right">989415</td>
<td class="org-right">0.713</td>
</tr>

<tr>
<td class="org-left">Oregon</td>
<td class="org-right">1094</td>
<td class="org-right">3831074</td>
<td class="org-right">0.703</td>
</tr>

<tr>
<td class="org-left">Nevada</td>
<td class="org-right">770</td>
<td class="org-right">2700551</td>
<td class="org-right">0.702</td>
</tr>

<tr>
<td class="org-left"><b>SPACE</b></td>
<td class="org-right">------</td>
<td class="org-right">------</td>
<td class="org-right">------</td>
</tr>

<tr>
<td class="org-left">North Dakota</td>
<td class="org-right">19</td>
<td class="org-right">672591</td>
<td class="org-right">0.0</td>
</tr>

<tr>
<td class="org-left">Vermont</td>
<td class="org-right">18</td>
<td class="org-right">625741</td>
<td class="org-right">0.001</td>
</tr>

<tr>
<td class="org-left">Kansas</td>
<td class="org-right">106</td>
<td class="org-right">2853118</td>
<td class="org-right">0.024</td>
</tr>

<tr>
<td class="org-left">Wyoming</td>
<td class="org-right">22</td>
<td class="org-right">563626</td>
<td class="org-right">0.029</td>
</tr>

<tr>
<td class="org-left">New Jersey</td>
<td class="org-right">400</td>
<td class="org-right">8791894</td>
<td class="org-right">0.047</td>
</tr>
</tbody>
</table>

<p>
Seemingly some correlation between low population and low usage is
evident from this table. However, the states for which the politics
board is most popular are also fairly small. It may be that the
popularity doesn't relate to state size, directly, but to political
orientation, which itself correlated with state population. I suspect
that political discussion is most charged currently in Democratic
states, where discenting opinion is the that held by the triumphant
party. It may also be that board popularity relation to patronage is
non-linear. This correlation is explored more by some political
investigation. However, first outliers must be determined and possibly
removed from the data.
</p>
</div>
</div>
<div id="outline-container-org92e55e5" class="outline-3">
<h3 id="org92e55e5">Usage</h3>
<div class="outline-text-3" id="text-org92e55e5">

<div class="figure">
<p><img src="./img/py6320LXp.png" alt="py6320LXp.png" />
</p>
</div>

<p>
These are the PDF estimations for normalized patronage, population,
usage. They are estimations, so they extend beyond 0 and 1 on the
graph. Usage distribution is the ratio distribution of patronage and
population. 
</p>
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">min_norm</span> = state_usage - state_usage.<span class="org-builtin">min</span>()
<span class="org-variable-name">range_norm</span> = state_usage.<span class="org-builtin">max</span>() - state_usage.<span class="org-builtin">min</span>()
<span class="org-variable-name">norm_usage</span> = min_norm / range_norm

norm_usage.plot(kind=<span class="org-string">"density"</span>, 
<span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   title=<span class="org-string">"Normalized PDF estimations"</span>,
<span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   sharey=<span class="org-constant">True</span>)
</pre>
</div>

<div class="figure">
<p><img src="./img/py6320jfT.png" alt="py6320jfT.png" />
</p>
</div>

<p>
We can see that usage has less variance than patronage and population,
which we should expect. Perhaps it is somewhat more than expected,
however.
</p>

<p>
Mean/median of normalized state usage metrics:
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="all" frame="all">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-right">mean</th>
<th scope="col" class="org-right">median</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">patronage</td>
<td class="org-right">0.197488</td>
<td class="org-right">0.0915567</td>
</tr>

<tr>
<td class="org-left">population</td>
<td class="org-right">0.152608</td>
<td class="org-right">0.105552</td>
</tr>

<tr>
<td class="org-left">usage</td>
<td class="org-right">0.264764</td>
<td class="org-right">0.20374</td>
</tr>
</tbody>
</table>


<p>
Here we can see illustrated what's been already hinted at: the states
with the most and least usage are generally less populated and less
patronaged, and, of course, there is a tight correlation between
patronage and population. In the graph, redness relates to usage
positively. The most red and most yellow dots are all in the least
populated states/least patroned boards.
</p>



<div class="figure">
<p><img src="./img/py6320Yhv.png" alt="py6320Yhv.png" />
</p>
</div>
</div>
</div>
<div id="outline-container-org11b48cf" class="outline-3">
<h3 id="org11b48cf">Politics</h3>
<div class="outline-text-3" id="text-org11b48cf">
<p>
It seems that the distribution of posts is weighted on the Democrat's
side of the spectrum:
</p>

<div class="figure">
<p><img src="./img/py22415X0p.png" alt="py22415X0p.png" />
</p>
</div>

<p>
However, Democratic registration outweighs Rebpublican voting rates
slightly. We can visualize this preference a bit differently by
finding the average post trumpism, and comparing it to national voting
trends:
</p>
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">avg_post_trumpism</span> = post_politics.trumpism.mean()
<span class="org-variable-name">trump_votes</span> = voting.<span class="org-builtin">sum</span>().trump
<span class="org-variable-name">clinton_votes</span> = voting.<span class="org-builtin">sum</span>().clinton
<span class="org-variable-name">national_trumpism</span> = trump_votes/(trump_votes + clinton_votes)
</pre>
</div>

<p>
It's a bit more clear here that the skew of trumpism distribution is
weighted a bit on the left, though the mean is quite close to what's
expected, at about 48% of Trump+Clinton votes. The skewness of
distribution is expected, and in line with my original hypothesis;
more liberal states can expect more discent from the socially charged
Republican minority, while very Trump states have little inspiration
for outcry. In general, it would seem the most divided states see the
most traffic, with less divided having Democratic prominance. The mean
in preserved by what seems to be in states that Trump won by a
relatively small margin.
</p>
<div class="org-src-container">
<pre class="src src-org">Mean trumpism: 48.42 Trump voters seem to show -1.17% representation
on CL politics vs General Election results.
</pre>
</div>

<p>
An alternative representation that may make this skew a bit more apparent:
</p>

<div class="figure">
<p><img src="./img/py26878eDX.png" alt="py26878eDX.png" />
</p>
</div>
</div>
<div id="outline-container-orgb2cb42a" class="outline-4">
<h4 id="orgb2cb42a">Usage vs Trumpism</h4>
<div class="outline-text-4" id="text-orgb2cb42a">
<p>
We can see the correlations between patronage, population, and usage,
here. We of course expect correlation between patronage and population
to be quite high: states with more people generally have more
posts. However we see W Below, positive correlation is pictured by
redness, while negative is pictures by blueness. Darkness visualizes
closeness.
</p>

<div class="figure">
<p><img src="./img/py2241F8fd.png" alt="py2241F8fd.png" />
</p>
</div>

<p>
Note the correlation between trumpism and usage. Also, the correlation
between patronage and usage coincides with how you'd expect boards
with the least diversity to be disproportionately unfrequented. Boards
with few posts become ghost towns. Here are the pearson correlation
numbers behinds the colors:
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="all" frame="all">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-right">patronage</th>
<th scope="col" class="org-right">usage</th>
<th scope="col" class="org-right">trumpism</th>
<th scope="col" class="org-right">population</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">patronage</td>
<td class="org-right">1</td>
<td class="org-right">0.336</td>
<td class="org-right">-0.363</td>
<td class="org-right">0.895</td>
</tr>

<tr>
<td class="org-left">usage</td>
<td class="org-right">0.336</td>
<td class="org-right">1</td>
<td class="org-right">-0.302</td>
<td class="org-right">-0.008</td>
</tr>

<tr>
<td class="org-left">trumpism</td>
<td class="org-right">-0.363</td>
<td class="org-right">-0.302</td>
<td class="org-right">1</td>
<td class="org-right">-0.344</td>
</tr>

<tr>
<td class="org-left">population</td>
<td class="org-right">0.895</td>
<td class="org-right">-0.008</td>
<td class="org-right">-0.344</td>
<td class="org-right">1</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<div id="outline-container-org8078842" class="outline-2">
<h2 id="org8078842">Text Qualities</h2>
<div class="outline-text-2" id="text-org8078842">
<p>
Text usage is interesting to consider, but difficult to evaluate
semantically. While sampling encourages some compelling thoughts about
the data, proving any derivative ideas is a bit difficult. The
following is an effort to support the introduction of this blog post.
</p>
</div>
<div class="outline-text-2" id="text-org8078842">
<p>
Popular English words are excluded from the analysis. Words like
"the", "re", "and", etc., don't contribute interestingly. Popular
words were grabbed from <a href="http://www.world-english.org/english500.htm">http://www.world-english.org/english500.htm</a>,
and a couple were added as needed (e.g., "re" appears all the time).
</p>
</div>
<div id="outline-container-org8c5fa22" class="outline-3">
<h3 id="org8c5fa22">Liberals vs Conservatives</h3>
<div class="outline-text-3" id="text-org8c5fa22">
<p>
Investigating the discrepency between democrat/republican word usage,
we see the some discrepencies in the most used common words. Grab some words
</p>
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">lib_words</span> = words(df=post_politics[post_politics.trumpism &lt; .45],
<span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span> no_pop=<span class="org-constant">True</span>).rename(<span class="org-string">"libs"</span>)
<span class="org-variable-name">conserv_words</span> = words(df=post_politics[post_politics.trumpism &gt; .55],
<span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span> no_pop=<span class="org-constant">True</span>).rename(<span class="org-string">"conservs"</span>)
</pre>
</div>

<table border="2" cellspacing="0" cellpadding="6" rules="all" frame="all">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-right">counts</th>
<th scope="col" class="org-right">dem/rep ratio</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">thought</td>
<td class="org-right">393</td>
<td class="org-right">22.27</td>
</tr>

<tr>
<td class="org-left">2017</td>
<td class="org-right">230</td>
<td class="org-right">9</td>
</tr>

<tr>
<td class="org-left">must</td>
<td class="org-right">142</td>
<td class="org-right">8</td>
</tr>

<tr>
<td class="org-left">11</td>
<td class="org-right">128</td>
<td class="org-right">7.45</td>
</tr>

<tr>
<td class="org-left">usa</td>
<td class="org-right">276</td>
<td class="org-right">6.81</td>
</tr>
</tbody>
</table>
</div>
<div class="outline-text-3" id="text-org8c5fa22">
<p>
We find that "against", "how", and "won" have extreme preference for
"liberal" states. The reasons are in fact not obvious. Some random
sampling of such posts reveals possibly surprisingly pro-Trump
sentiment:
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="all" frame="all">


<colgroup>
<col  class="org-right" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-right">&#xa0;</th>
<th scope="col" class="org-left">title</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">12992</td>
<td class="org-left">RufRydrRADIO=&gt;LATEST Podcast=&gt;"WE WON'T GET 'TOOLED' AGAIN"=&gt;POP SONG</td>
</tr>

<tr>
<td class="org-right">28062</td>
<td class="org-left">Thought for the Day</td>
</tr>

<tr>
<td class="org-right">27429</td>
<td class="org-left">Thought for the Day</td>
</tr>

<tr>
<td class="org-right">6571</td>
<td class="org-left">I WONT SHED ONE TEAR !    you pukes made me suffer for 8 years !</td>
</tr>

<tr>
<td class="org-right">28026</td>
<td class="org-left">Thought for the Day</td>
</tr>
</tbody>
</table>

<p>
Looking at the general word sentiment, we see clearly that
disproportionately Trump and Obama are discussed. Note that "hillary"
and "clinton" are surprisingly not mentioned as much as you might
think. "Clinton", in fact, is mentioned less freqeuntly than
"Donald". It may be that a month after the election, "hillary" talk
has already begun to significantly subside. It's impossible to know
for sure, as CL does not hold on to their posts for longer than a
week.
</p>

<div class="figure">
<p><img src="./img/py31406ImT.png" alt="py31406ImT.png" />
</p>
</div>
</div>

<div id="outline-container-org78bd01f" class="outline-4">
<h4 id="org78bd01f">Unicode</h4>
<div class="outline-text-4" id="text-org78bd01f">
<p>
I was curious about non-ascii usage, and so I used to following code
to catch them.
</p>
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">check_ascii</span>(post):
<span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-doc">"""</span>
<span class="org-doc"><span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span></span><span class="org-doc">   Determines whether a title is encodable as ascii</span>
<span class="org-doc"><span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span></span><span class="org-doc">   """</span>
<span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-keyword">try</span>:
<span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   post.encode(<span class="org-string">'ascii'</span>)
<span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-keyword">return</span> <span class="org-constant">True</span>
<span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-keyword">except</span> <span class="org-type">UnicodeError</span>:
<span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-keyword">return</span> <span class="org-constant">False</span>

<span class="org-variable-name">ascii_posts</span> = usa[usa.title.<span class="org-builtin">apply</span>(check_ascii)]
<span class="org-variable-name">nonascii_posts</span> = usa[~usa.title.<span class="org-builtin">apply</span>(check_ascii)]
<span class="org-variable-name">distinct_states</span> = nonascii_posts[<span class="org-string">"state"</span>].unique()
</pre>
</div>

<p>
The number of posts containing non-ascii characters was surprisingly small:
</p>
<div class="org-src-container">
<pre class="src src-org">219 of 38,324 total posts were non-ascii (0.57%), confined to 22 states.
</pre>
</div>
<p>
However, influence for these posts can be seen by looking at the main
outlier, Pennsylvania:
</p>
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">pennsylvania</span> = nonascii_posts[nonascii_posts[<span class="org-string">"state"</span>] == <span class="org-string">"Pennsylvania"</span>]
pennsylvania.groupby(<span class="org-string">"region"</span>).count()
<span class="org-variable-name">penn_lenn</span> = <span class="org-builtin">float</span>(<span class="org-builtin">len</span>(pennsylvania.title))

<span class="org-variable-name">post_uniqueness</span> = (penn_lenn-pennsylvania.title.nunique())/penn_lenn * 100
</pre>
</div>

<p>
We can use a SequenceMatcher to test the similarity of the strings in
the pool:
</p>
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> itertools
<span class="org-keyword">from</span> difflib <span class="org-keyword">import</span> SequenceMatcher

<span class="org-keyword">def</span> <span class="org-function-name">avg_similarity</span>(posts):
<span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span> <span class="org-keyword">def</span> <span class="org-function-name">similarity</span>(a, b):
<span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-keyword">return</span> SequenceMatcher(<span class="org-constant">None</span>, a, b).ratio()

<span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span> <span class="org-variable-name">sim_sum</span> = 0
<span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span> <span class="org-variable-name">title_product</span> = itertools.product(posts.title, posts.title)
<span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span> <span class="org-keyword">for</span> title_pair <span class="org-keyword">in</span> title_product:
<span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-variable-name">sim_sum</span> += similarity(*title_pair)

<span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span> <span class="org-variable-name">avg_sim</span> = sim_sum/(<span class="org-builtin">len</span>(posts)**2)
<span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span> <span class="org-keyword">return</span> avg_sim
</pre>
</div>

<p>
We then can run this over all non-ascii posts to get an idea of how
much silliness is going on with these posts:
</p>
<div class="org-src-container">
<pre class="src src-org">The average similarity of all non-ascii posts is 0.19, while that 
of only those in Pennsylvania is 0.38. The average for all posts in
all regions is 0.19.
</pre>
</div>
<p>
It would therefore seem that a single Trump memester, making use of a
handful of unicode symbols, is responsible for this chaos in
Pennsylvania. I suspect that these crazy unicode posts are mostly
done by a very small set of people in general, though there is no
good way to tell, as CL remains completely anonymous.
</p>
</div>
</div>
<div id="outline-container-orgc8436fe" class="outline-4">
<h4 id="orgc8436fe">"trumps"</h4>
<div class="outline-text-4" id="text-orgc8436fe">
<p>
The more pro-Trump your state, the less likely you are to use "TRUMP"
over "Trump". Below is a visual depicting this ratio, by states in
order of trumpism. We can see that states on the right of the graph
tend to have a low ratio of upper to proper. This isn't too
surprising, as you expect, in leiu of social resistance, for pro-Trump
regions to settle for respectful praise over excitement and hysteria,
a general tone which is supported by some cursory sampling of these
regions.
</p>

<div class="figure">
<p><img src="./img/py6320cup.png" alt="py6320cup.png" />
</p>
</div>

<p>
Looking at the distribution of "trump" posts across trumpism looks
much the same as the distribution of all posts across trumpism:
</p>

<div class="figure">
<p><img src=".img/py268781zz.png" alt="py268781zz.png" />
</p>
</div>


<p>
However, Democratic states seem to have relatively strong preferance
for using "TRUMP" versus "Trump". Below's graph depicts this skew,
which is made more noticible by the considerable left-shift of the
mean:
</p>

<div class="figure">
<p><img src="./img/py26878b0D.png" alt="py26878b0D.png" />
</p>
</div>

<p>
It isn't clear why there seems to be preference for capitalization of
"TRUMP" among Dem states; are mostly angry and disparaging,
supportive, or a bit of both? Some random sampling of particularly
liberal states might provide some clues:
</p>
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">liberal_sample</span> = trumps_trumpism[trumps_trumpism.trumpism &lt; .45].sample(5)
</pre>
</div>

<p>
Selecting states that are espectially anti-trump:
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="all" frame="all">


<colgroup>
<col  class="org-right" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-right">&#xa0;</th>
<th scope="col" class="org-left"><b>trump</b></th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">2470</td>
<td class="org-left">Trump win shatters Obama legacy</td>
</tr>

<tr>
<td class="org-right">14765</td>
<td class="org-left">Trump to bring private security staff to White House</td>
</tr>

<tr>
<td class="org-right">27604</td>
<td class="org-left">Trump Is Right about our Sevice Men!</td>
</tr>

<tr>
<td class="org-right">10989</td>
<td class="org-left">Trumps approach is actually more honest</td>
</tr>

<tr>
<td class="org-right">13495</td>
<td class="org-left">Love Trumps Hate Wall</td>
</tr>
</tbody>
</table>
<p>
Politically liberal states composing the above sampling:
Illinois, Massachusetts, New York, California.
</p>
</div>
</div>

<div id="outline-container-org947b418" class="outline-4">
<h4 id="org947b418">"liberals" vs "conservatives"</h4>
<div class="outline-text-4" id="text-org947b418">
</div><ul class="org-ul"><li><a id="orgf147441"></a><b>Usage</b><br  /><div class="outline-text-5" id="text-orgf147441">
<p>
"liberal" is used far more often than "conservative". The
pluralizations, respectively, are comparitively not quite as
distinguished. This is expected, for previously mentioned reasons;
pluralizations may still be used as a means to negatively generalize.
</p>
<div class="org-src-container">
<pre class="src src-org">liberal/conservative: 18.07
liberals/conservatives: 5.16
liberal(s)/conservative(s): 10.14
</pre>
</div>
</div></li>

<li><a id="orgf0cca2b"></a><b>Pluralization</b><br  /><div class="outline-text-5" id="text-orgf0cca2b">
<p>
The singular version of "conservative" is used a bit more than half as
much as the pluralization. By contrast, the singular version of
"liberal" is used more than twice as much as the pluralization. I
suspect this is because "liberal" is a perjorative in common
nomenclature, while "conservative" doesn't really hold the same weight
as an insult:
</p>
<div class="org-src-container">
<pre class="src src-org"><span class="org-bold">*singular/plural*</span>
'conservative': 0.628
'liberal': 2.198
</pre>
</div>
</div></li>

<li><a id="orgbb0311e"></a><b>Capitalization</b><br  /><div class="outline-text-5" id="text-orgbb0311e">
<p>
We here see that, among democrats, "liberal" is capitalized at a rate
13x greater than the rate of capitalization of "conservative". We also
see that lowecase usage preference is completely neglible.
</p>
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">lib_cap</span> = eval_strs(<span class="org-string">"trump"</span>).<span class="org-builtin">sum</span>(numeric_only=<span class="org-constant">True</span>)
<span class="org-variable-name">conserv_cap</span> = eval_strs(<span class="org-string">"liberal"</span>).<span class="org-builtin">sum</span>(numeric_only=<span class="org-constant">True</span>)

<span class="org-variable-name">lib_con_cap_rat</span> = (lib_cap/conserv_cap).rename(
<span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-string">"Dem/Rep capital rates for 'trump'"</span>)
</pre>
</div>

<table border="2" cellspacing="0" cellpadding="6" rules="all" frame="all">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-right">Dem/Rep capital rates for 'trump'</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">proper</td>
<td class="org-right">10.5951</td>
</tr>

<tr>
<td class="org-left">uppercase</td>
<td class="org-right">13.4286</td>
</tr>

<tr>
<td class="org-left">lower</td>
<td class="org-right">1.07721</td>
</tr>
</tbody>
</table>
</div></li></ul>
</div>

<div id="outline-container-orgd126865" class="outline-4">
<h4 id="orgd126865">Semantics</h4>
<div class="outline-text-4" id="text-orgd126865">
<p>
I figured that a natural way to go about proving my hypothesis
outlined in this blog's introduction would be semantic analysis. I
quickly decided that this was, with it's present implementation, at
least, not the way to go about it. The following code will run
semantic analysis using the popular NLTK package. The results are
dubious.
</p>
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">from</span> textblob <span class="org-keyword">import</span> TextBlob

<span class="org-keyword">def</span> <span class="org-function-name">semants</span>(text):
<span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-variable-name">blob</span> = TextBlob(text)
<span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-variable-name">ss</span> = 0
<span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-keyword">for</span> sentence <span class="org-keyword">in</span> blob.sentences:
<span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-variable-name">ss</span> += sentence.sentiment.polarity
<span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-keyword">return</span> <span class="org-builtin">float</span>(ss)/<span class="org-builtin">len</span>(blob.sentences)

<span class="org-comment-delimiter"># </span><span class="org-comment">package does not like non-ascii encodings</span>
<span class="org-variable-name">trumps_ascii</span> = trumps[trumps[<span class="org-string">"*trump*"</span>].<span class="org-builtin">apply</span>(check_ascii)]


<span class="org-variable-name">usa_sentiment</span> = post_politics.join(ascii_posts.title.<span class="org-builtin">apply</span>(
<span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   semants).rename(<span class="org-string">"sentiment"</span>))
<span class="org-variable-name">trumps_sentiment</span> = usa_sentiment.<span class="org-builtin">filter</span>(trumps_ascii.index, axis=0)
</pre>
</div>

<p>
However, the results, and general output of the semantic analyzer,
were quite unconvincing, even if only interpreted as a binary measure:
</p>
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">zero_sents</span> = <span class="org-builtin">len</span>(usa_sentiment[usa_sentiment.sentiment == 0])
<span class="org-keyword">print</span>((<span class="org-string">'Number of posts with 0 sentiment: {0:,} '</span> + 
<span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>  <span class="org-string">'({1:.2f}%).'</span>).<span class="org-builtin">format</span>(zero_sents, 
<span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span><span class="org-builtin">float</span>(zero_sents)/<span class="org-builtin">len</span>(usa_sentiment)*100))
</pre>
</div>

<div class="org-src-container">
<pre class="src src-org">Number of posts with 0 sentiment: 25,632 (66.88%).
</pre>
</div>
</div>
</div>
</div>
</div>

<div id="outline-container-orgc942a2a" class="outline-2">
<h2 id="orgc942a2a">Conclusion</h2>
<div class="outline-text-2" id="text-orgc942a2a">
<p>
The distribution posts and the favor of those posts across the
politics sections is somewhat surprising. I suspect that this is
evidence of cultural normalization in the face of
resistance+anonimity: faceless, nameless interaction coupled with
outspokenness against relatively strict local social norms. This has
proven more difficult to prove than I initially suspected. While any
amount of ransom sampling of the posts allows me to be confident in
this theory, convincing proof would most likely involve a tedious,
exhausive effort.
</p>
</div>
</div>

<div id="outline-container-org9e7c35b" class="outline-2">
<h2 id="org9e7c35b">Notes about this document</h2>
<div class="outline-text-2" id="text-org9e7c35b">
<p>
This document is, in its original form, an emacs org-mode
organizational markup document that supports interactive programming
and exporting quite thoroughly. It exports to a variety of formats
(html, latex, markdown, etc), and in this case, was exported directly
to html. It's quite powerful, and allows me to tailor what headers are
exported, what code is exported, what code results, to what
interpreter the code talks, how it's formated, etc. The original
document, if viewed in org-mode in emacs, is quite a bit larger,
containing all of the code used for the project, most of which is not
shown in markdown exports. Therefore, if you view this document on
github, you will see a truncated version much like the version you are
likely viewing now. You can view on github, a .ipynb and a .py export
are available for the complete code of the document. Obviously, they
won't include the organization and commentary. You can look at the raw
contents of the .org file if curious (github will export primitively
to html by default for display), or check out this <a href="http://kozikow.com/2016/05/21/very-powerful-data-analysis-environment-org-mode-with-ob-ipython/comment-page-1/#comment-240">blog on interactive
python programming in emacs org-mode</a>.
</p>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Dodge Coates</p>
<p class="date">Created: 2017-01-09 Mon 05:13</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
