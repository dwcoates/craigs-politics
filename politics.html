<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2017-01-06 Fri 22:58 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Scraping the Bottom of the Barrel</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Dodge Coates" />
<style type="text/css">
  html{font-family:sans-serif;
       -ms-text-size-adjust:100%;
       -webkit-text-size-adjust:100%}
  body{margin:0}
  article,aside,details,figcaption,figure,footer,header,main,menu,nav,section,summary{display:block}
  audio,canvas,progress,video{display:inline-block}
  audio:not([controls]){display:none;
                        height:0}
  progress{vertical-align:baseline}
  [hidden],template{display:none}
  a{background-color:transparent;
    -webkit-text-decoration-skip:objects}
  a:active,a:hover{outline-width:0}
  abbr[title]{border-bottom:none;
              text-decoration:underline;
              text-decoration:underline dotted}
  b,strong{font-weight:inherit;
           font-weight:bolder}
  dfn{font-style:italic}
  h1{font-size:2em;
     margin:.67em 0}
  mark{background-color:#ff0;
       color:#000}
  small{font-size:80%}
  sub,sup{font-size:75%;
          line-height:0;
          position:relative;
          vertical-align:baseline}
  sub{bottom:-.25em}
  sup{top:-.5em}
  img{border-style:none}
  svg:not(:root){overflow:hidden}
  code,kbd,pre,samp{font-family:monospace;
                    font-size:1em}
  figure{margin:1em 40px}
  hr{box-sizing:content-box;
     height:0;
     overflow:visible}
  button,input,select,textarea{font:inherit;
                               margin:0}
  optgroup{font-weight:700}
  button,input{overflow:visible}
  button,select{text-transform:none}
  [type=reset],[type=submit],button,html [type=button]{-webkit-appearance:button}
  [type=button]::-moz-focus-inner,[type=reset]::-moz-focus-inner,[type=submit]::-moz-focus-inner,button::-moz-focus-inner{border-style:none;
                                                                                                                          padding:0}
  [type=button]:-moz-focusring,[type=reset]:-moz-focusring,[type=submit]:-moz-focusring,button:-moz-focusring{outline:1px dotted ButtonText}
  fieldset{border:1px solid silver;
           margin:0 2px;
           padding:.35em .625em .75em}
  legend{box-sizing:border-box;
         color:inherit;
         display:table;
         max-width:100%;
         padding:0;
         white-space:normal}
  textarea{overflow:auto}
  [type=checkbox],[type=radio]{box-sizing:border-box;
                               padding:0}
  [type=number]::-webkit-inner-spin-button,[type=number]::-webkit-outer-spin-button{height:auto}
  [type=search]{-webkit-appearance:textfield;
                outline-offset:-2px}
  [type=search]::-webkit-search-cancel-button,[type=search]::-webkit-search-decoration{-webkit-appearance:none}
  ::-webkit-input-placeholder{color:inherit;
                              opacity:.54}
  ::-webkit-file-upload-button{-webkit-appearance:button;
                               font:inherit}
  body{width:95%;
       margin:2%;
       font:normal normal normal 17px/1.6em Helvetica,sans-serif;
       color:#333}
  @media (min-width:769px){body{width:700px;
                                margin-left:5vw}
  }
  .title{margin:auto;
         color:#000}
  .subtitle,.title{text-align:center}
  .subtitle{font-size:medium;
            font-weight:700}
  .abstract{margin:auto;
            width:80%;
            font-style:italic}
  .abstract p:last-of-type:before{content:"    ";
                                  white-space:pre}
  .status{font-size:90%;
          margin:2em auto}
  [class^=section-number-]{margin-right:.5em}
  #footnotes{font-size:90%}
  .footpara{display:inline;
            margin:.2em auto}
  .footdef{margin-bottom:1em}
  .footdef sup{padding-right:.5em}
  a{color:#527d9a;
    text-decoration:none}
  a:hover{color:#035;
          border-bottom:1px dotted}
  figure{padding:0;
         margin:0;
         text-align:center}
  img{max-width:100%;
      vertical-align:middle}
  @media (min-width:769px){img{max-width:85vw;
                               margin:auto}
  }
  .MathJax_Display{margin:0!important;
                   width:90%!important}
  h1,h2,h3,h4,h5,h6{color:#a5573e;
                    line-height:1.6em;
                    font-family:Georgia,serif}
  h4,h5,h6{font-size:1em}
  dt{font-weight:700}
  table{margin:auto;
        border-top:2px solid;
        border-collapse:collapse}
  table,thead{border-bottom:2px solid}
  table td+td,table th+th{border-left:1px solid gray}
  table tr{border-top:1px solid #d3d3d3}
  td,th{padding:5px 10px;
        vertical-align:middle}
  caption.t-above{caption-side:top}
  caption.t-bottom{caption-side:bottom}
  th.org-center,th.org-left,th.org-right{text-align:center}
  td.org-right{text-align:right}
  td.org-left{text-align:left}
  td.org-center{text-align:center}
  code{padding:2px 5px;
       margin:auto 1px;
       border:1px solid #ddd;
       border-radius:3px;
       background-clip:padding-box;
       color:#333;
       font-size:80%}
  blockquote{margin:1em 2em;
             padding-left:1em;
             border-left:3px solid #ccc}
  kbd{background-color:#f7f7f7;
      font-size:80%;
      margin:0 .1em;
      padding:.1em .6em}
  .todo{color:red}
  .done,.todo{font-family:Lucida Console,monospace}
  .done{color:green}
  .priority{color:orange}
  .priority,.tag{font-family:Lucida Console,monospace}
  .tag{background-color:#eee;
       font-size:80%;
       font-weight:400;
       padding:2px}
  .timestamp{color:#bebebe}
  .timestamp-kwd{color:#5f9ea0}
  .org-right{margin-left:auto;
             margin-right:0;
             text-align:right}
  .org-left{margin-left:0;
            margin-right:auto;
            text-align:left}
  .org-center{margin-left:auto;
              margin-right:auto;
              text-align:center}
  .underline{text-decoration:underline}
  #postamble p,#preamble p{font-size:90%;
                           margin:.2em}
  p.verse{margin-left:3%}
  pre{border:1px solid #ccc;

      box-shadow:3px 3px 3px #eee;
      font-family:Lucida Console,monospace;
      margin:1.2em;
      padding:8pt}
  pre.src{overflow:auto;
          padding-top:1.2em;
          position:relative;
          font-size:80%}
  pre.src:before{background-color:#fff;
                 border:1px solid #000;
                 display:none;
                 padding:3px;
                 position:absolute;
                 right:10px;
                 top:.6em}
  pre.src:hover:before{display:inline}
  pre.src-sh:before{content:'sh'}
  pre.src-bash:before{content:'bash'}
  pre.src-emacs-lisp:before{content:'Emacs Lisp'}
  pre.src-R:before{content:'R'}
  pre.src-org:before{content:'Org'}
  pre.src-c+:before{content:'C++'}
  pre.src-c:before{content:'C'}
  pre.src-html:before{content:'HTML'}
  pre.example{overflow:auto;
              padding-top:1.2em;
              position:relative;
              font-size:80%}
  .inlinetask{background:#ffc;
              border:2px solid gray;
              margin:10px;
              padding:10px}
  #org-div-home-and-up{font-size:70%;
                       text-align:right;
                       white-space:nowrap}
  .linenr{font-size:smaller}
  .code-highlighted{background-color:#ff0}
  #bibliography{font-size:90%}
  #bibliography table{width:100%}
  .creator{display:block}@media (min-width:769px){.creator{display:inline;
                                                           float:right}}</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<h1 class="title">Scraping the Bottom of the Barrel</h1>

<div id="outline-container-org3918cb1" class="outline-2">
<h2 id="org3918cb1"><span class="section-number-2">1</span> Introduction</h2>
<div class="outline-text-2" id="text-1">
<p>
For my web scraping project, I've chosen to extract some of the
politics data from craigslist.org. My original ambition, though it
proved difficult to affirm, was to prove a small, non-existant, or
negative correllation of pro-trump chatter to expected
conservatism. That is, I suspected that, somewhat counter-intuitively,
the politics sections of more conservative states would a
disproportionately less likely source of pro-trump posts. My basis for
this suspicion was my general observation that less regulated areas
for discussion on the internet tend to be very attractive to those
members of a publically socially disparaged minority. Recognizing
that, among other clues, Trump supporters in largely pro-Clinton
geographic areas are disparaged for their support in amounts
disproportionate to their surprisingly high representation, it
followed that I could expect a surprising amount of pro-Trump (mostly
trollish) chatter in mostly liberal places (e.g., New York City). The
positive sentiment aspect of that hypothesis proved to be difficult to
convincingly affirm. More generally, also I sought to analyze the
trends of politcs discussion of craiglist, mostly in the area of text
usage (capitalization, word frequency, etc) vs political leaning.
</p>


<div class="figure">
<p><img src="./img/Trump_cloud_proper.png" alt="Trump_cloud_proper.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-orga30878d" class="outline-2">
<h2 id="orga30878d"><span class="section-number-2">2</span> Methodology</h2>
<div class="outline-text-2" id="text-2">
<p>
To extract data from craigslist, I used the Python Scrapy package,
which was probably overkill. Originally, I intended to collect post
bodies as well as the titles, however this would require about 100
times as many request, too many for me to reponsibly exectute in a
reasonable amount of time. I resigned to limiting myself to titles,
which involved about 500 requests, spread over 5 hours, to obtain
roughly 40,000 posts titles/times. For each of these titles, there is
a corresponding state and region, with some regions additionally
divided into subregions (the New York City region, for example,
consists of Brooklyn, Queens, Manhattan, etc). Each post, its time and
its geographical origin are represented with a single row in a 40k row
Pandas DataFrame, <code>usa</code>. Data corruption was not an issue, as the CL
layout is quite uniform, though I did need to take into account data
redundancy (e.g., occaisionally "regions" are also "subregions" of
sibling regions). To make use of the extracted post title data, I
employed the 2010 U.S. census, which is available from
<a href="http://www.census.gov/">http://www.census.gov/</a>, as well as the 2016 election results data,
which I scraped from <a href="http://uselectionatlas.org/">http://uselectionatlas.org/</a> using a BeautifulSoup
extraction script.
</p>
</div>
</div>
<div id="outline-container-orgc900958" class="outline-2">
<h2 id="orgc900958"><span class="section-number-2">3</span> Preparing data</h2>
<div class="outline-text-2" id="text-3">
</div><div id="outline-container-orga1aeb78" class="outline-3">
<h3 id="orga1aeb78"><span class="section-number-3">3.1</span> <span class="todo TODO">TODO</span> Craigcrawler</h3>
</div>
<div id="outline-container-org838f1f8" class="outline-3">
<h3 id="org838f1f8"><span class="section-number-3">3.2</span> Grab CL Data</h3>
<div class="outline-text-3" id="text-3-2">
<p>
Data is read from file that craigcrawler built
</p>
<div class="org-src-container">
<pre class="src src-ipython">usa_raw = pd.read_csv("data/us.csv", index_col=0)
</pre>
</div>

<p>
Data synopsis:
</p>
<p>

</p>

<p>
38,692 total posts exctracted from 416 regions over 52 state. The most popular
state was Alabama, and the most popular region was, surprisingly, SF bay area.
</p>
</div>
<div id="outline-container-orgd10ac60" class="outline-4">
<h4 id="orgd10ac60"><span class="section-number-4">3.2.1</span> <code>usa</code> Sample</h4>
<div class="outline-text-4" id="text-3-2-1">
<p>
Sample of posts in the <code>usa</code> DataFrame before preprocessing, which is
the DF for storing all CL politics posts:
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="all" frame="all">


<colgroup>
<col  class="org-right" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-right">&#xa0;</th>
<th scope="col" class="org-left">title</th>
<th scope="col" class="org-left">date</th>
<th scope="col" class="org-left">state</th>
<th scope="col" class="org-left">region</th>
<th scope="col" class="org-right">subregion</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">4760</td>
<td class="org-left">Stupid Ass, Steroid-Eating Cop in Hamilton County</td>
<td class="org-left">2016-12-23 09:47</td>
<td class="org-left">Tennessee</td>
<td class="org-left">chattanooga, TN</td>
<td class="org-right">nan</td>
</tr>

<tr>
<td class="org-right">31229</td>
<td class="org-left">Some Things I have Learned Over The Years</td>
<td class="org-left">2016-11-29 17:31</td>
<td class="org-left">Colorado</td>
<td class="org-left">denver, CO</td>
<td class="org-right">nan</td>
</tr>

<tr>
<td class="org-right">35537</td>
<td class="org-left">re-Trump eyeing an accomplished executive to head State</td>
<td class="org-left">2016-12-17 09:14</td>
<td class="org-left">Texas</td>
<td class="org-left">victoria, TX</td>
<td class="org-right">nan</td>
</tr>

<tr>
<td class="org-right">26045</td>
<td class="org-left">Demand The Electorial College Reverse This Sham of An Election &#x2026;.</td>
<td class="org-left">2016-11-30 12:01</td>
<td class="org-left">Ohio</td>
<td class="org-left">akron / canton</td>
<td class="org-right">nan</td>
</tr>

<tr>
<td class="org-right">36102</td>
<td class="org-left">House Bill 375</td>
<td class="org-left">2016-12-27 04:59</td>
<td class="org-left">Texas</td>
<td class="org-left">lubbock, TX</td>
<td class="org-right">nan</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>

<div id="outline-container-orgb96556d" class="outline-3">
<h3 id="orgb96556d"><span class="section-number-3">3.3</span> U.S. Census 2010</h3>
<div class="outline-text-3" id="text-3-3">
</div><div id="outline-container-org74e47b7" class="outline-4">
<h4 id="org74e47b7"><span class="section-number-4">3.3.1</span> Geo Keys</h4>
<div class="outline-text-4" id="text-3-3-1">
<div class="org-src-container">
<pre class="src src-ipython"># Keys for geography stuff. Table is an index table.
# These keys are used as index for census table.
GEO_NAME = "GEO.display-label"
GEO_KEY = "GEO.id"

state_keys = pd.read_csv("data/census/DEC_10_DP_G001_with_ann.csv")[1:].set_index(GEO_KEY)

state_keys = state_keys.filter([GEO_NAME])[:52]
state_keys = state_keys[state_keys[GEO_NAME]!= "Puerto Rico"]
</pre>
</div>
</div>
</div>

<div id="outline-container-org8d77378" class="outline-4">
<h4 id="org8d77378"><span class="section-number-4">3.3.2</span> Census Data</h4>
<div class="outline-text-4" id="text-3-3-2">
<p>
Census data is collected from U.S. Census Bureau for 2010 census. Here's a sample.
</p>
<div class="org-src-container">
<pre class="src src-ipython">census_all = pd.read_csv("data/census/DEC_10_DP_DPDP1_with_ann.csv")[1:].set_index(GEO_KEY)
</pre>
</div>

<table border="2" cellspacing="0" cellpadding="6" rules="all" frame="all">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">state</th>
<th scope="col" class="org-right">population</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Vermont</td>
<td class="org-right">625741</td>
</tr>

<tr>
<td class="org-left">Kentucky</td>
<td class="org-right">4.33937e+06</td>
</tr>

<tr>
<td class="org-left">Nevada</td>
<td class="org-right">2.70055e+06</td>
</tr>

<tr>
<td class="org-left">Indiana</td>
<td class="org-right">6.4838e+06</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div id="outline-container-org5fef5c6" class="outline-3">
<h3 id="org5fef5c6"><span class="section-number-3">3.4</span> U.S. 2016 Election</h3>
<div class="outline-text-3" id="text-3-4">
<p>
The 2016 Election results will be useful. They are grabbed from a really nice site, <a href="http://uselectionatlas.org/RESULTS/data.php?year=2016&amp;datatype=national&amp;def=1&amp;f=1&amp;off=0&amp;elect=0">uselectionsatlas.org</a>.
</p>
<div class="org-src-container">
<pre class="src src-ipython">import requests
from scrapy import Selector

atlas_url = ("http://uselectionatlas.org/RESULTS/data.php?year" +
             "=2016&amp;datatype=national&amp;def=1&amp;f=1&amp;off=0&amp;elect=0")
atlas_source = requests.get(atlas_url).text
select = Selector(text=atlas_source).xpath('//*[@id="datatable"]/tbody/tr')

convert = lambda s: int(s.replace(',', ''))
vote_names = map(str, select.xpath('td[3]/a/text()').extract())
# Correct name for DC
vote_names[8] = "District of Columbia"
clinton_votes = map(convert, select.xpath('td[17]/text()').extract())
trump_votes = map(convert, select.xpath('td[18]/text()').extract())

gen_votes = pd.DataFrame({"clinton": clinton_votes, "trump": trump_votes},
                         index=vote_names)

# Dub a states Rebublican vote rate "trumpism"
trump_favor = pd.DataFrame(gen_votes["trump"]/gen_votes.sum(axis=1),
                           columns=["trumpism"],
                           index=vote_names)
voting = gen_votes.join(trump_favor).sort_values("trumpism", ascending=False)
voting = voting.drop("District of Columbia")
</pre>
</div>

<p>
Sample of voting table:
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="all" frame="all">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-right">clinton</th>
<th scope="col" class="org-right">trump</th>
<th scope="col" class="org-right">trumpism</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Wyoming</td>
<td class="org-right">55973</td>
<td class="org-right">174419</td>
<td class="org-right">0.757</td>
</tr>

<tr>
<td class="org-left">West Virginia</td>
<td class="org-right">188794</td>
<td class="org-right">489371</td>
<td class="org-right">0.722</td>
</tr>

<tr>
<td class="org-left">North Dakota</td>
<td class="org-right">93758</td>
<td class="org-right">216794</td>
<td class="org-right">0.698</td>
</tr>

<tr>
<td class="org-left"><b>SPACE</b></td>
<td class="org-right">------</td>
<td class="org-right">------</td>
<td class="org-right">------</td>
</tr>

<tr>
<td class="org-left">Hawaii</td>
<td class="org-right">266891</td>
<td class="org-right">128847</td>
<td class="org-right">0.326</td>
</tr>

<tr>
<td class="org-left">California</td>
<td class="org-right">8753788</td>
<td class="org-right">4483810</td>
<td class="org-right">0.339</td>
</tr>

<tr>
<td class="org-left">Vermont</td>
<td class="org-right">178573</td>
<td class="org-right">95369</td>
<td class="org-right">0.348</td>
</tr>
</tbody>
</table>
</div>
</div>

<div id="outline-container-org64c8fab" class="outline-3">
<h3 id="org64c8fab"><span class="section-number-3">3.5</span> Preprocess Data</h3>
<div class="outline-text-3" id="text-3-5">
<p>
Some preprocessing to check data for corruption and unexpected results
</p>
<div class="org-src-container">
<pre class="src src-ipython">print "Data tests... \n\nAssertions Passed\n\n"

# Confirm all expected regions and states present
assert len(usa_raw["state"].unique()) == 52 # expected number of states (D.C., Territories)
assert len(usa_raw["region"].unique()) == 416  # expected number of regions

# Confirm that there are no posts without regions/states. Not all CL
# regions have subregions, so it's okay for null subregions.
assert len(usa_raw[usa_raw["state"].isnull()].index) == 0
assert len(usa_raw[usa_raw["region"].isnull()].index) == 0

# Find regions/subregions for which there are no posts
postless_regions = usa_raw[usa_raw["title"].isnull()]
postless_regions_times = usa_raw[usa_raw["date"].isnull()]

# Not actually a good test, but good enough
assert len(postless_regions) == len(postless_regions_times)
</pre>
</div>

<p>
58 regions/subregions over 32 states without any posts.
</p>

<p>
Drop unneeded data.
</p>
<div class="org-src-container">
<pre class="src src-ipython"># Drop empty regions.
usa = usa_raw.dropna(subset=["title", "date"], how="any", axis=0)
assert len(postless_regions) == len(usa_raw)-len(usa)

# Get rid of territories (Guam, Puerto Rico).
usa = usa[usa["state"] != "Territories"]
# Get rid of "District of Columbia"
usa = usa[usa["state"] != "District of Columbia"]
</pre>
</div>

<p>
Confirm Census Data
</p>
<div class="org-src-container">
<pre class="src src-ipython">assert set(usa.state.unique()) == set(census.index) and len(usa.state.unique() == len(census.index))
</pre>
</div>

<p>
Confirm Election Data
</p>
<div class="org-src-container">
<pre class="src src-ipython">assert set(usa.state.unique()) == set(voting.index) and len(usa.state.unique() == len(voting.index))
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org0df85e8" class="outline-2">
<h2 id="org0df85e8"><span class="section-number-2">4</span> State Usage</h2>
<div class="outline-text-2" id="text-4">
<p>
Although the post data has attached a fairly fine-grain geographical
description, I found the CL regions in general to not line up well
with any census bureau categories. Moreover, even in the lucky event
of such name correspondence, the division of regions was at least
questionable. For example, by far the datasets most prominent "state"
outliers, District of Columbia, has a census population of about 600k,
yet a practical metropolitan area population in the several millions,
a disparity that gross skews its contributions to state-wide
statistics. Therefore, regions and subregions were largely found to be
unmanageably tedious to consider seriously in any analysis. States,
however, having relatively little variation between practical
occupancy and census population, and have indisputable borders,
barring District of Columbia, are ideal for inspection.
</p>
</div>
<div id="outline-container-orgb734659" class="outline-3">
<h3 id="orgb734659"><span class="section-number-3">4.1</span> Terms</h3>
<div class="outline-text-3" id="text-4-1">
<ol class="org-ol">
<li><b>Patronage</b>
Patronage is the raw number of posts on a politics board.</li>
<li><b>Usage</b>
Usage is my measure for a states proportional interest in the
politics board. It is simply the normalized ratio of patronage and
state population.</li>
<li><b>Trumpism</b>
Trumpism is the name for a states republican vote percentage in the
general election. It is used as a rough measure of how pro-Trump
rate of a given state, and is a column in the <code>voting</code> DataFrame,
which is comprised of scraped data on the 2016 General Election
results.</li>
</ol>
</div>
</div>
<div id="outline-container-orga3375a9" class="outline-3">
<h3 id="orga3375a9"><span class="section-number-3">4.2</span> Organize Data</h3>
<div class="outline-text-3" id="text-4-2">
<p>
The <code>state_usage</code> table is the census table concatenated with patronage usage.
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="all" frame="all">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">state</th>
<th scope="col" class="org-right">patronage</th>
<th scope="col" class="org-right">population</th>
<th scope="col" class="org-right">usage</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">New Mexico</td>
<td class="org-right">428</td>
<td class="org-right">2.05918e+06</td>
<td class="org-right">0.490914</td>
</tr>

<tr>
<td class="org-left">Illinois</td>
<td class="org-right">965</td>
<td class="org-right">1.28306e+07</td>
<td class="org-right">0.128363</td>
</tr>

<tr>
<td class="org-left">Missouri</td>
<td class="org-right">619</td>
<td class="org-right">5.98893e+06</td>
<td class="org-right">0.205299</td>
</tr>
</tbody>
</table>
</div>
<div id="outline-container-orgb03bc90" class="outline-4">
<h4 id="orgb03bc90"><span class="section-number-4">4.2.1</span> <code>states</code> Sample</h4>
<div class="outline-text-4" id="text-4-2-1">
<p>
Joining <code>state_usage</code> with voting gives us a decent top down view of
state political tendencies on CL.
</p>
<div class="org-src-container">
<pre class="src src-ipython">states = state_usage.join(voting, how="left").sort_values("usage")
</pre>
</div>

<table border="2" cellspacing="0" cellpadding="6" rules="all" frame="all">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">state</th>
<th scope="col" class="org-right">patronage</th>
<th scope="col" class="org-right">population</th>
<th scope="col" class="org-right">usage</th>
<th scope="col" class="org-right">clinton</th>
<th scope="col" class="org-right">trump</th>
<th scope="col" class="org-right">trumpism</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Ohio</td>
<td class="org-right">1401</td>
<td class="org-right">1.15365e+07</td>
<td class="org-right">0.254726</td>
<td class="org-right">2.39416e+06</td>
<td class="org-right">2.84100e+06</td>
<td class="org-right">0.542677</td>
</tr>

<tr>
<td class="org-left">Virginia</td>
<td class="org-right">555</td>
<td class="org-right">8.00102e+06</td>
<td class="org-right">0.112388</td>
<td class="org-right">1.98147e+06</td>
<td class="org-right">1.76944e+06</td>
<td class="org-right">0.471736</td>
</tr>

<tr>
<td class="org-left">North Dakota</td>
<td class="org-right">19</td>
<td class="org-right">672591</td>
<td class="org-right">0</td>
<td class="org-right">93758</td>
<td class="org-right">216794</td>
<td class="org-right">0.698092</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div id="outline-container-org3c27d0c" class="outline-3">
<h3 id="org3c27d0c"><span class="section-number-3">4.3</span> Outliers</h3>
<div class="outline-text-3" id="text-4-3">
<p>
There are two major outlying states in the dataset: Colorodo and
District of Columbia.
</p>
</div>
<div id="outline-container-orgb6322bd" class="outline-4">
<h4 id="orgb6322bd"><span class="section-number-4">4.3.1</span> Colorodo</h4>
<div class="outline-text-4" id="text-4-3-1">
<p>
We can see from the following that Colorado is an extreme outlier,
being the fifth most popular state, yet the 23rd most populous.
</p>

<div class="figure">
<p><img src="./img/py6320WCb.png" alt="py6320WCb.png" />
</p>
</div>

<p>
Denver, as a region, is also especially large. Despite having a
population of 650,000 people (and a metropolitcan area of 3 million),
Denver sees a patronage of 1187.
</p>
<div class="org-src-container">
<pre class="src src-ipython">print(len(usa[usa.region == "denver, CO"]))
</pre>
</div>
<p>
1187
</p>
<p>
By comparison, the "new york city" region, which is expansive enough
as to include metropolitan area subregions like "new jersey", "long island",
"fairfield", etc, has fewer posts, at 1006.
</p>
<p>
1006 posts in NYC spread over manhattan, brooklyn, queens, bronx, staten island, new jersey, long island, westchester, and fairfield. This is ~6.5% the usage rate of Denver
</p>

<p>
This is a remarkably popular region, clearly. I suspect that it has to
do with the region granularity CL mostly likely arbitrarily assigned
to the state. They might want to consider providing mode regions to
the state of Colorado.
</p>
</div>
</div>
<div id="outline-container-org7a8b689" class="outline-4">
<h4 id="org7a8b689"><span class="section-number-4">4.3.2</span> District of Columbia</h4>
<div class="outline-text-4" id="text-4-3-2">
<p>
While I found Colorado to be an inexplicable anamoly, it was also
justifiably accurate. District of Columbia, having a Republican voting
rate of ~4% and the usage similar to that of Colorado, coupled with
it's unclear geographic distinction and population, meant its results
were too extreme and variable to consider in analysis. Besides, it's
not even a real state&#x2026;
</p>
</div>
</div>
</div>

<div id="outline-container-org65eb99d" class="outline-3">
<h3 id="org65eb99d"><span class="section-number-3">4.4</span> Patronage</h3>
<div class="outline-text-3" id="text-4-4">

<div class="figure">
<p><img src="./img/py6320oYD.png" alt="py6320oYD.png" />
</p>
</div>

<p>
We can get a feel for the usage distribution by taking a look at the
following sample from the state<sub>usage</sub> table:
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="all" frame="all">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-right">patronage</th>
<th scope="col" class="org-right">population</th>
<th scope="col" class="org-right">usage</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Colorado</td>
<td class="org-right">1982</td>
<td class="org-right">5029196</td>
<td class="org-right">1.0</td>
</tr>

<tr>
<td class="org-left">Hawaii</td>
<td class="org-right">445</td>
<td class="org-right">1360301</td>
<td class="org-right">0.817</td>
</tr>

<tr>
<td class="org-left">Montana</td>
<td class="org-right">286</td>
<td class="org-right">989415</td>
<td class="org-right">0.713</td>
</tr>

<tr>
<td class="org-left">Oregon</td>
<td class="org-right">1094</td>
<td class="org-right">3831074</td>
<td class="org-right">0.703</td>
</tr>

<tr>
<td class="org-left">Nevada</td>
<td class="org-right">770</td>
<td class="org-right">2700551</td>
<td class="org-right">0.702</td>
</tr>

<tr>
<td class="org-left"><b>SPACE</b></td>
<td class="org-right">------</td>
<td class="org-right">------</td>
<td class="org-right">------</td>
</tr>

<tr>
<td class="org-left">North Dakota</td>
<td class="org-right">19</td>
<td class="org-right">672591</td>
<td class="org-right">0.0</td>
</tr>

<tr>
<td class="org-left">Vermont</td>
<td class="org-right">18</td>
<td class="org-right">625741</td>
<td class="org-right">0.001</td>
</tr>

<tr>
<td class="org-left">Kansas</td>
<td class="org-right">106</td>
<td class="org-right">2853118</td>
<td class="org-right">0.024</td>
</tr>

<tr>
<td class="org-left">Wyoming</td>
<td class="org-right">22</td>
<td class="org-right">563626</td>
<td class="org-right">0.029</td>
</tr>

<tr>
<td class="org-left">New Jersey</td>
<td class="org-right">400</td>
<td class="org-right">8791894</td>
<td class="org-right">0.047</td>
</tr>
</tbody>
</table>

<p>
Seemingly some correlation between low population and low usage is
evident from this table. However, the states for which the politics
board is most popular are also fairly small. This correlation is
explored more by some political investigation. However, first outliers
must be determined and possibly removed from the data.
</p>
</div>
</div>
<div id="outline-container-orga67af48" class="outline-3">
<h3 id="orga67af48"><span class="section-number-3">4.5</span> Usage</h3>
<div class="outline-text-3" id="text-4-5">
<p>
<img src="./img/py6320LXp.png" alt="py6320LXp.png" />
These are the PDF estimations for normalized patronage, population,
usage. They are estimations, so they extend beyond 0 and 1 on the
graph. Usage distribution is the ratio distribution of patronage and
population.
</p>
<div class="org-src-container">
<pre class="src src-ipython">norm_usage = (state_usage - state_usage.min()) / (state_usage.max() - state_usage.min())
norm_usage.plot(kind="density", title="Normalized PDF estimations", sharey=True)
</pre>
</div>

<div class="figure">
<p><img src="./img/py6320jfT.png" alt="py6320jfT.png" />
</p>
</div>

<p>
Here we can see illustrated what's been already hinted at: the states
with the most and least usage are generally less populated and less
patronaged, and, of course, there is a tight correlation between
patronage and population.
</p>

<div class="figure">
<p><img src="./img/py6320Yhv.png" alt="py6320Yhv.png" />
</p>
</div>
</div>
</div>
<div id="outline-container-org3b74332" class="outline-3">
<h3 id="org3b74332"><span class="section-number-3">4.6</span> Politics</h3>
<div class="outline-text-3" id="text-4-6">
</div><div id="outline-container-org3b56c78" class="outline-4">
<h4 id="org3b56c78"><span class="section-number-4">4.6.1</span> Posts over Trumpism</h4>
<div class="outline-text-4" id="text-4-6-1">

<div class="figure">
<p><img src="./img/py22415X0p.png" alt="py22415X0p.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-org6200696" class="outline-4">
<h4 id="org6200696"><span class="section-number-4">4.6.2</span> States/Usage</h4>
<div class="outline-text-4" id="text-4-6-2">
<p>
Note the correlation between trumpism and usage. Also, the correlation
between patronage and usage coincides with how you'd expect boards
with the least diversity to be disproportionately unfrequented. Boards
with few posts become ghost towns.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="all" frame="all">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-right">patronage</th>
<th scope="col" class="org-right">usage</th>
<th scope="col" class="org-right">trumpism</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">patronage</td>
<td class="org-right">1.0</td>
<td class="org-right">0.336</td>
<td class="org-right">-0.363</td>
</tr>

<tr>
<td class="org-left">usage</td>
<td class="org-right">0.336</td>
<td class="org-right">1.0</td>
<td class="org-right">-0.302</td>
</tr>

<tr>
<td class="org-left">trumpism</td>
<td class="org-right">-0.363</td>
<td class="org-right">-0.302</td>
<td class="org-right">1.0</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>

<div id="outline-container-org5239db7" class="outline-3">
<h3 id="org5239db7"><span class="section-number-3">4.7</span> Correlations</h3>
<div class="outline-text-3" id="text-4-7">
</div><div id="outline-container-org7e9b13e" class="outline-4">
<h4 id="org7e9b13e"><span class="section-number-4">4.7.1</span> Distributions</h4>
<div class="outline-text-4" id="text-4-7-1">
<p>
We can see the correlations between patronage, population, and usage,
here. We of course expect correlation between patronage and
population: states with more people generally have more posts.
</p>

<div class="figure">
<p><img src="./img/py224159fd.png" alt="py224159fd.png" />
</p>
</div>

<p>
We can see that usage and population correlate somewhat. In more
concrete numerical terms, using the pearson correlation coefficient:
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="all" frame="all">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-right">patronage</th>
<th scope="col" class="org-right">population</th>
<th scope="col" class="org-right">usage</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">patronage</td>
<td class="org-right">1</td>
<td class="org-right">0.895182</td>
<td class="org-right">0.336453</td>
</tr>

<tr>
<td class="org-left">population</td>
<td class="org-right">0.895182</td>
<td class="org-right">1</td>
<td class="org-right">-0.00831774</td>
</tr>

<tr>
<td class="org-left">usage</td>
<td class="org-right">0.336453</td>
<td class="org-right">-0.00831774</td>
<td class="org-right">1</td>
</tr>
</tbody>
</table>
<p>
Below, we can see that usage has less variance than patronage and
population, which we should expect. Perhaps it is somewhat more than
expected, however. We expect (perhaps naively) for usage to coincide
with population/patronage closely.
</p>
<div class="org-src-container">
<pre class="src src-ipython">norm_usage = (state_usage - state_usage.min()) / (state_usage.max() - state_usage.min())
norm_usage.plot(kind="density", title="Normalized PDF estimations", sharey=True)
</pre>
</div>

<div class="figure">
<p><img src="./img/py6320cwT.png" alt="py6320cwT.png" />
</p>
</div>
<p>
Mean/median of normalized state usage metrics:
                mean    median
patronage   0.197488  0.091557
population  0.152608  0.105552
usage       0.264764  0.203740
</p>
</div>
</div>

<div id="outline-container-org693a27c" class="outline-4">
<h4 id="org693a27c"><span class="section-number-4">4.7.2</span> Usage per state</h4>
<div class="outline-text-4" id="text-4-7-2">
<p>
The distribution of usage among states seems reasonable:
</p>

<div class="figure">
<p><img src="./img/py22415jSF.png" alt="py22415jSF.png" />
</p>
</div>

<p>
#+END<sub>SRC</sub>
</p>
</div>
</div>
<div id="outline-container-orgb6951df" class="outline-4">
<h4 id="orgb6951df"><span class="section-number-4">4.7.3</span> Politics</h4>
<div class="outline-text-4" id="text-4-7-3">

<div class="figure">
<p><img src="./img/py22415k-v.png" alt="py22415k-v.png" />
</p>
</div>
</div>
</div>
</div>
</div>

<div id="outline-container-org039a45c" class="outline-2">
<h2 id="org039a45c"><span class="section-number-2">5</span> Text Qualities</h2>
<div class="outline-text-2" id="text-5">
<p>
Text usage is interesting to consider, but difficult to evaluate
semantically. While sampling provides some surprising ideas about the
data, proving any derivative ideas is a bit difficult. The following
is and effort to support the introduction of this blog post.
</p>
</div>
<div id="outline-container-orgeb8da4f" class="outline-3">
<h3 id="orgeb8da4f"><span class="section-number-3">5.1</span> Words</h3>
<div class="outline-text-3" id="text-5-1">
<p>
pop<sub>english</sub><sub>words</sub> is a list of the most popular words in
English. Grabbed from <a href="http://www.world-english.org/english500.htm">http://www.world-english.org/english500.htm</a>.
Probably don't care about stupid common words.
</p>
</div>
</div>
<div id="outline-container-org0504fea" class="outline-3">
<h3 id="org0504fea"><span class="section-number-3">5.2</span> Substrings</h3>
<div class="outline-text-3" id="text-5-2">
<p>
Find substrings in posts
</p>
</div>
</div>
<div id="outline-container-orgd777b3e" class="outline-3">
<h3 id="orgd777b3e"><span class="section-number-3">5.3</span> Analysis</h3>
<div class="outline-text-3" id="text-5-3">
<p>
Investigating the discrepency between democrat/republican word usage,
we see the some discrepencies in the most used common words. Grab some words
</p>
<div class="org-src-container">
<pre class="src src-ipython">lib_words = words(df=post_politics[post_politics.trumpism &lt; .45],
                  no_pop=True).rename("libs")
conserv_words = words(df=post_politics[post_politics.trumpism &gt; .55],
                      no_pop=True).rename("conservs")
</pre>
</div>

<table border="2" cellspacing="0" cellpadding="6" rules="all" frame="all">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-right">counts</th>
<th scope="col" class="org-right">dem/rep ratio</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">thought</td>
<td class="org-right">393</td>
<td class="org-right">22.27</td>
</tr>

<tr>
<td class="org-left">2017</td>
<td class="org-right">230</td>
<td class="org-right">9</td>
</tr>

<tr>
<td class="org-left">must</td>
<td class="org-right">142</td>
<td class="org-right">8</td>
</tr>

<tr>
<td class="org-left">11</td>
<td class="org-right">128</td>
<td class="org-right">7.45</td>
</tr>

<tr>
<td class="org-left">usa</td>
<td class="org-right">276</td>
<td class="org-right">6.81</td>
</tr>
</tbody>
</table>
<p>
We find that "against", "how", and "won" have extreme preference for
"liberal" states. The reasons are in fact not obvious. Some random
sampling of such posts reveals possibly surprisingly pro-Trump
sentiment:
</p>
<div class="org-src-container">
<pre class="src src-ipython">print(pd.concat([find_strs("thought"),
                 find_strs("usa"),
                 find_strs("won")]).sample(10))
</pre>
</div>
<p>
27254                                                      Thought for the Day
17057                                         Out of work actors?  No wonder!!
29942        Ten thousand dollar bet: right wing troll cannot define communism
16343                                                   HERE IS A BOLD THOUGHT
33805       RE Proof of massive Democrats voter fraud thousands voter fraud ..
34090                                  Trump won the legal popular vote too ..
17981              WE WON !!!!!!!  Clinton's- rapists pedophiles  for JAIL !!!
2594                                                    One Has to Wonder?????
27404                                                      Thought for the Day
1936     good thing only a small %, got such a butt hurt and wont accept Trump
dtype: object
</p>

<p>
Looking at the general word sentiment, we see clearly has vastly disproportionately PEOTUS Trump and President Obama are discussed.
</p>

<div class="figure">
<p><img src="./img/py31406ImT.png" alt="py31406ImT.png" />
</p>
</div>


<div class="figure">
<p><img src="./img/py314068Os.png" alt="py314068Os.png" />
</p>
</div>
</div>
<div id="outline-container-org6ec0673" class="outline-4">
<h4 id="org6ec0673"><span class="section-number-4">5.3.1</span> "trumps"</h4>
<div class="outline-text-4" id="text-5-3-1">
</div><ol class="org-ol"><li><a id="org9be1857"></a>Patronage<br  /></li>
<li><a id="org5d025c2"></a>Politics<br  /><div class="outline-text-5" id="text-5-3-1-2">
<p>
The more pro-Trump your state, the less likely you are to use "Trump" over "TRUMP"
</p>

<div class="figure">
<p><img src="./img/py6320cup.png" alt="py6320cup.png" />
</p>
</div>

<p>
Selecting states that are espectially anti-trump:
28660     Boycott CNN and MSNBC cooper, mathews, lemon, maddow, etc etc etc!
12880                                                  Love Trumps Hate Wall
12949                                     Not in Chico, you owe me an answer
3972             Boycott  Gunston Hall * Cruelty and Killing of Animals !!!!
28061                                                    Thought for the Day
29067                                         "Pedoleaks : Crime of Century"
28184                            The story of Russian Hackers by Mike Tracey
11884    Alert Now Mysterious Vibration detected around World source unknown
28806                      CHER said she would move to jupiter if TRUMP won!
3912                                              Liberal Dems In The Toilet
Name: title, dtype: object
Politically liberal states composing the above sampling:
['California', 'Hawaii', 'Maryland', 'Massachusetts', 'New York', 'Vermont']
</p>
</div></li></ol>
</div>

<div id="outline-container-org2558af3" class="outline-4">
<h4 id="org2558af3"><span class="section-number-4">5.3.2</span> Unicode</h4>
<div class="outline-text-4" id="text-5-3-2">
<p>
I was curious about non-ascii usage, and so I used to following code to catch them.
</p>
<div class="org-src-container">
<pre class="src src-ipython">def check_ascii(post):
    """
    Determines whether a title is encodable as ascii
    """
    try:
        post.encode('ascii')
        return True
    except UnicodeError:
        return False

ascii_posts = usa[usa.title.apply(check_ascii)]
nonascii_posts = usa[~usa.title.apply(check_ascii)]
distinct_states = nonascii_posts["state"].unique()
</pre>
</div>
<p>
219 of 38,324 total posts were non-ascii (0.57%), confined to 22 states.
</p>
<p>
However, influence for these posts can be seen by looking at the main outlier, Pennsylvania:
</p>
<div class="org-src-container">
<pre class="src src-ipython">pennsylvania = nonascii_posts[nonascii_posts["state"] == "Pennsylvania"]
pennsylvania.groupby("region").count()
penn_lenn = float(len(pennsylvania.title))

post_uniqueness = (penn_lenn-pennsylvania.title.nunique())/penn_lenn * 100

print("{:.2f}% of non-ascii posts are completely unique.".format(post_uniqueness))
</pre>
</div>

<p>
58.93% of non-ascii posts are completely unique.
</p>

<p>
We can use a SequenceMatcher to test the similarity of the strings in the pool:
</p>
<div class="org-src-container">
<pre class="src src-ipython">import itertools
from difflib import SequenceMatcher

def avg_similarity(posts):
  def similarity(a, b):
    return SequenceMatcher(None, a, b).ratio()

  sim_sum = 0
  title_product = itertools.product(posts.title, posts.title)
  for title_pair in title_product:
    sim_sum += similarity(*title_pair)

  avg_sim = sim_sum/(len(posts)**2)
  return avg_sim
</pre>
</div>

<p>
We then can run this over all non-ascii posts to get an idea of how
much silliness is going on with these posts:
</p>
<p>
The average similarity of all non-ascii posts is 0.19 while that 
of only those in Pennsylvania is 0.38. The average for all posts in
all regions is 0.19.
</p>

<p>
It would seem that a single Trump memester is responsible for this
chaos in Pennsylvania. I suspect that these crazy unicode posts are
mostly done by a very small set of people in general, though there is
no good way to tell:
#+END<sub>SRC</sub>*** Politics <code>[0/2]</code>
</p>
</div>
<ol class="org-ol"><li><a id="orgfbc606d"></a><span class="todo TODO">TODO</span> Diversity of words vs trumpism<br  /></li>
<li><a id="org68a64d2"></a>"liberals" vs "conservatives"<br  /><ol class="org-ol"><li><a id="org0f8b779"></a>Pluralization<br  /><div class="outline-text-6" id="text-5-3-2-2-1">
<p>
The singular version of "conservative" is used a bit more than half as
much as the pluralization. By contrast, the singular version of
"liberal" is used more than twice as much as the pluralization. I
suspect this is because "liberal" is a perjorative in common
nomenclature, while "conservative" doesn't really hold the same weight
as an insult:
</p>
<p>
singular/plural:
'conservative': 0.628
'liberal': 2.198
</p>
</div></li>
<li><a id="orgc3bb5f7"></a>Usage<br  /><div class="outline-text-6" id="text-5-3-2-2-2">
<p>
"liberal" is used far more often than "conservative". The
pluralizations, respectively, are comparitively not quite as
distinguished. This is expected, for previously mentioned reasons;
pluralizations may still be used as a means to negatively generalize.
</p>
<p>
liberal/conservative: 18.07
liberals/conservatives: 5.16
liberal(s)/conservative(s): 10.14
</p>
</div></li>
<li><a id="org719cd92"></a>Capitalization<br  /><div class="outline-text-6" id="text-5-3-2-2-3">
<p>
We here see that, among democrats, "liberal" is capitalized at a rate
13x greater than the rate of capitalization of "conservative". We also
see that lowecase usage preference is completely neglible.
</p>
<div class="org-src-container">
<pre class="src src-ipython">lib_cap = eval_strs("trump").sum(numeric_only=True)
conserv_cap = eval_strs("liberal").sum(numeric_only=True)

lib_con_cap_rat = (lib_cap/conserv_cap).rename("liberal/conservative cap rates for 'trump'")
</pre>
</div>

<p>
Dem/Rep capitalization ratio for 'trump':
proper       10.595062
uppercase    13.428571
lower         1.077206
</p>
</div></li></ol></li>

<li><a id="org17e2ace"></a><span class="todo TODO">TODO</span> "trump" vs "clinton" vs "obama"<br  /><ol class="org-ol"><li><a id="orgd324fe9"></a>"trump" usage / total usage<br  /><div class="outline-text-6" id="text-5-3-2-3-1">
<div class="org-src-container">
<pre class="src src-ipython"></pre>
</div>
</div></li>

<li><a id="orgd8ca0a0"></a>"trump" usage / trumpism<br  /></li>
<li><a id="orgb43b51a"></a>upcase usage / trumpism<br  /></li>
<li><a id="org3d7a490"></a>trumpism<br  /><div class="outline-text-6" id="text-5-3-2-3-4">
<div class="org-src-container">
<pre class="src src-ipython">trump_posts = usa.join(voting, on="state").join(find_strs("trump"),
                                                how="inner")
</pre>
</div>
</div></li></ol></li>

<li><a id="org0560266"></a>Semantics<br  /><div class="outline-text-5" id="text-5-3-2-4">
<p>
I figured that a natural way to go about proving my hypothesis
outlined in this blog's introduction would be semantic analysis. I
quickly decided that this was, with it's present implementation, at
least, not the way to go about it. The following code will run
semantic analysis using the popular NLTK package. The results are
dubious.
</p>
<div class="org-src-container">
<pre class="src src-ipython"># from textblob import TextBlob

# def semants(text):
#     blob = TextBlob(text)
#     ss = 0
#     for sentence in blob.sentences:
#         ss += sentence.sentiment.polarity
#     return float(ss)/len(blob.sentences)

# # package does not like non-ascii encodings
# semantics = ascii_posts.title.map(lambda x: semants(x)).rename("semants")
# semant = eval_strs("trump", df=ascii_posts).join(pd.DataFrame(semantics))
# sems_usa = ascii_posts.join(usa, how="inner")
# trumps_semantics = sems_usa.groupby("state").mean().join(voting,
#                                                          how="inner").sort_values(
#                                                              "semants").corr()
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">#trumps_semantics
</pre>
</div>
</div></li></ol>
</div>
</div>
</div>

<div id="outline-container-orge1ef0fd" class="outline-2">
<h2 id="orge1ef0fd"><span class="section-number-2">6</span> Conclusion</h2>
<div class="outline-text-2" id="text-6">
<p>
The distribution posts and the favor of those posts across the
politics sections is somewhat surprising. I suspect that this is
evidence of cultural normalization in the face of
resistance+anonimity: faceless, nameless interaction coupled with
outspokenness against relatively strict local social norms. This has
proven more difficult to prove than I initially suspected. While any
amount of ransom sampling of the posts allows me to be confident in
this theory, convincing proof would most likely involve a tedious,
exhausive effort.
</p>
</div>
</div>

<div id="outline-container-orga4fc70b" class="outline-2">
<h2 id="orga4fc70b"><span class="section-number-2">7</span> Notes about this document</h2>
<div class="outline-text-2" id="text-7">
<p>
This document is, in its original form, an emacs org-mode
organizational markup document that supports interactive programming
and exporting quite thoroughly. It exports to a variety of formats
(html, latex, markdown, etc). It's quite powerful, and allows me to
tailor what headers are exported, what code is exported, what code
results, etc. The original document, if viewed in org-mode in emacs,
is quite a bit larger, containing all of the code used for the
project, most of which is not shown in markdown exports. Therefore, if
you view this document on github, you will see a truncated version
much like the version you are likely viewing now. You can view on
github a .ipynb and a .py export for the complete code of the
document. Obviously, they won't include the organization and
commentary. You can look at the raw contents of the .org file if
curious (github will export primitively to html by default for
display), or check out this <a href="http://kozikow.com/2016/05/21/very-powerful-data-analysis-environment-org-mode-with-ob-ipython/comment-page-1/#comment-240">blog on interactive python programming in
emacs org-mode</a>.
</p>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Dodge Coates</p>
<p class="date">Created: 2017-01-06 Fri 22:58</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
