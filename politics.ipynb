{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},

            "source": [
                "<h1 align=\"center\"><font color=\"0066FF\" size=110>Scraping the Bottom of the Barrel</font></h1>\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  %matplotlib inline\n",
                "  import numpy as np\n",
                "  import scipy\n",
                "  from scipy import stats\n",
                "  import matplotlib as mpln\n",
                "  import matplotlib.pyplot as plt\n",
                "  import matplotlib.cm as cm\n",
                "  import pandas as pd\n",
                "\n",
                "  from tabulate import tabulate\n",
                "\n",
                "  import pprint as pp\n",
                "  import pickle\n",
                "  import re\n",
                "\n",
                "  pd.options.display.max_colwidth = 1000\n",
                "\n",
                "  def print_df(df, headers=\"keys\", rnd=100, dis_parse=False):\n",
                "      \"\"\"\n",
                "      Pretty print DataFrame in an org table. Org tables are good.\n",
                "      They also export nicely.\n",
                "      \"\"\"\n",
                "      print(tabulate(df.round(rnd),\n",
                "                     tablefmt=\"orgtbl\",\n",
                "                     headers=headers,\n",
                "                     disable_numparse=dis_parse))\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "usa_raw = pd.read_csv(\"data/us.csv\", index_col=0)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "post_count_total_raw = len(usa_raw)\n",
                "post_count_by_state_raw = usa_raw.groupby(\"state\").count()[\"title\"].sort_values(ascending=False)\n",
                "post_count_by_region_raw = usa_raw.groupby(\"region\").count()[\"title\"].sort_values(ascending=False)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  print (\"{0:,} total posts exctracted from {1:} regions over {2} \"+\n",
                "         \"states. The most \\nfrequented state was '{3}', and the most \" +\n",
                "         \"frequented region was,\\nsurprisingly, '{4}'.\").format(post_count_total_raw,                                                          \n",
                "                                                               len(post_count_by_region_raw),\n",
                "                                                               len(post_count_by_state_raw),\n",
                "                                                               post_count_by_state_raw.index[0],\n",
                "                                                               post_count_by_region_raw.index[0],)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "80,222 total posts exctracted from 416 regions over 52 states. The most \n",
                "frequented state was 'Florida', and the most frequented region was,\n",
                "surprisingly, 'south florida'.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "# This can fail because tabulate can't handle unicode.\n",
                "# There's only about a 2.5% chance if fails on a given execution, though.\n",
                "print_df(usa_raw.sample(3), rnd=3)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "# Keys for geography stuff. Table is an index table.\n",
                "# These keys are used as index for census table.\n",
                "GEO_NAME = \"GEO.display-label\"\n",
                "GEO_KEY = \"GEO.id\"\n",
                "\n",
                "state_keys = pd.read_csv(\"data/census/DEC_10_DP_G001_with_ann.csv\")[1:].set_index(GEO_KEY)\n",
                "\n",
                "state_keys = state_keys.filter([GEO_NAME])[:52]\n",
                "state_keys = state_keys[state_keys[GEO_NAME]!= \"Puerto Rico\"]\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  # keys for the census data. Only really care about two of them (there are hundreds):\n",
                "  TOT_NUM_ID = \"HD01_S001\" # total number key\n",
                "  TOT_PER_ID = \"HD02_S001\" # total percent key\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  census_states = census_all.filter([TOT_NUM_ID]).join(state_keys, how=\"right\")\n",
                "  census_states.columns = [\"population\", \"state\"]\n",
                "  census_states.set_index(\"state\", inplace=True)\n",
                "\n",
                "  def correct_stat(s):\n",
                "      \"\"\"\n",
                "      Some states have extra information for population.\n",
                "      Example: 25145561(r48514), should be 25145561.\n",
                "      \"\"\"\n",
                "      loc = s.find(\"(\")\n",
                "      return int(s[:loc] if loc > 0 else s)\n",
                "\n",
                "  census_states.population = census_states.population.apply(correct_stat)\n",
                "\n",
                "  census = census_states.drop(\"District of Columbia\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "print_df(census.sample(4), rnd=3)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "| state          |  population |\n",
                "|----------------+-------------|\n",
                "| North Carolina | 9.53548e+06 |\n",
                "| Washington     | 6.72454e+06 |\n",
                "| Hawaii         |  1.3603e+06 |\n",
                "| North Dakota   |      672591 |\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  import requests\n",
                "  from scrapy import Selector\n",
                "\n",
                "  atlas_url = (\"http://uselectionatlas.org/RESULTS/data.php?year\" +\n",
                "               \"=2016&datatype=national&def=1&f=1&off=0&elect=0\")\n",
                "  atlas_source = requests.get(atlas_url).text\n",
                "  select = Selector(text=atlas_source).xpath('//*[@id=\"datatable\"]/tbody/tr')\n",
                "\n",
                "  convert = lambda s: int(s.replace(',', ''))\n",
                "  vote_names = map(str, select.xpath('td[3]/a/text()').extract())\n",
                "  # Correct name for DC\n",
                "  vote_names[8] = \"District of Columbia\"\n",
                "  clinton_votes = map(convert, select.xpath('td[17]/text()').extract())\n",
                "  trump_votes = map(convert, select.xpath('td[18]/text()').extract())\n",
                "\n",
                "  gen_votes = pd.DataFrame({\"clinton\": clinton_votes, \"trump\": trump_votes},\n",
                "                           index=vote_names)\n",
                "\n",
                "  # Dub a states Rebublican vote rate \"trumpism\"\n",
                "  trump_favor = pd.DataFrame(gen_votes[\"trump\"]/gen_votes.sum(axis=1),\n",
                "                             columns=[\"trumpism\"],\n",
                "                             index=vote_names)\n",
                "  voting = gen_votes.join(trump_favor).sort_values(\"trumpism\", ascending=False)\n",
                "  voting = voting.drop(\"District of Columbia\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  # for pretty printing\n",
                "  voting_space = pd.DataFrame([[\"------\", \"------\", \"------\"]],index=[\"*SPACE*\"],\n",
                "                              columns=voting.columns)\n",
                "  print_df(pd.concat([voting[:3].round(3), voting_space, voting[-3:].round(3).sort_values(\"trumpism\")]),\n",
                "           rnd=3)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "|               | clinton |   trump | trumpism |\n",
                "|---------------+---------+---------+----------|\n",
                "| Wyoming       |   55973 |  174419 |    0.757 |\n",
                "| West Virginia |  188794 |  489371 |    0.722 |\n",
                "| North Dakota  |   93758 |  216794 |    0.698 |\n",
                "| *SPACE*         |  ------ |  ------ |   ------ |\n",
                "| Hawaii        |  266891 |  128847 |    0.326 |\n",
                "| California    | 8753788 | 4483810 |    0.339 |\n",
                "| Vermont       |  178573 |   95369 |    0.348 |\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  print \"Data tests... \\n\\nAssertions Passed\\n\\n\"\n",
                "\n",
                "  # Confirm all expected regions and states present\n",
                "  assert len(usa_raw[\"state\"].unique()) == 52 # expected number of states (D.C., Territories)\n",
                "  assert len(usa_raw[\"region\"].unique()) == 416  # expected number of regions\n",
                "\n",
                "  # Confirm that there are no posts without regions/states. Not all CL\n",
                "  # regions have subregions, so it's okay for null subregions.\n",
                "  assert len(usa_raw[usa_raw[\"state\"].isnull()].index) == 0\n",
                "  assert len(usa_raw[usa_raw[\"region\"].isnull()].index) == 0\n",
                "\n",
                "  # Find regions/subregions for which there are no posts\n",
                "  postless_regions = usa_raw[usa_raw[\"title\"].isnull()]\n",
                "  postless_regions_times = usa_raw[usa_raw[\"date\"].isnull()]\n",
                "\n",
                "  # Not actually a good test, but good enough\n",
                "  assert len(postless_regions) == len(postless_regions_times)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  print((\"{0:,} regions/subregions over {1} states without \" +\n",
                "         \"any posts.\").format(len(postless_regions), postless_regions[\"state\"].nunique()))\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "68 regions/subregions over 35 states without any posts.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "# Confirm census data\n",
                "assert set(usa.state.unique()) == set(census.index) and len(usa.state.unique() == len(census.index))\n",
                "\n",
                "print \"Census data complete\"\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "# Confirm election data\n",
                "assert set(usa.state.unique()) == set(voting.index) and len(usa.state.unique() == len(voting.index))\n",
                "\n",
                "print \"Voting data complete\"\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  patronage = pd.DataFrame(usa.groupby('state').size(), columns=[\"patronage\"]).sort_values(\n",
                "      \"patronage\",ascending=False)\n",
                "\n",
                "  print(\"Top ten most frequented states:\\n\")\n",
                "  print_df(patronage[:10])\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  cl_by_state = patronage.join(census, how=\"inner\")\n",
                "  usage = cl_by_state.apply(\n",
                "      lambda df: df[\"patronage\"] / float(df[\"population\"]), axis=1)\n",
                "\n",
                "  # Weight for max = 1.000\n",
                "  usage_weighted = (usage - usage.min())/(usage.max() - usage.min())\n",
                "  weighted_usage = pd.DataFrame((usage_weighted),\n",
                "                                 columns=[\"usage\"])\n",
                "  state_usage = pd.concat([cl_by_state, weighted_usage],\n",
                "                          axis=1).sort_values(\"usage\",\n",
                "                                              ascending=False)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  # Just some printing\n",
                "\n",
                "  # Useful for displaying several splices of a dataframe as a concatenation\n",
                "  state_usage_space = pd.DataFrame([[\"------\", \"------\", \"------\"]],index=[\"*SPACE*\"],\n",
                "                                   columns=state_usage.columns)\n",
                "\n",
                "  print_df(state_usage.sample(3))\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  states = state_usage.join(voting, how=\"left\").sort_values(\"usage\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  print(tabulate(states.sample(3), tablefmt=\"orgtbl\", headers=\"keys\"))\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "top_five = state_usage.sort_values(\"patronage\")[-5:][::-1]\n",
                "fig = plt.figure() # Create matplotlib figure\n",
                "\n",
                "ax = fig.add_subplot(111) # Create matplotlib axes\n",
                "ax2 = ax.twinx() # Create another axes that shares the same x-axis as ax.\n",
                "\n",
                "width = 0.2\n",
                "\n",
                "top_five.patronage.plot(kind='bar', color='#992255', ax=ax, width=width, position=1)\n",
                "top_five.population.plot(kind='bar', color='#CC7733', ax=ax2, width=width, position=0)\n",
                "\n",
                "ax.set_ylabel('Patronage')\n",
                "ax2.set_ylabel('Population')\n",
                "\n",
                "plt.show()\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "print(\"Patronage of Denver, Colorado: {}\".format(len(usa[usa.region == \"denver, CO\"])))\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "Patronage of Denver, Colorado: 1988\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  # From census bureau, to the nearest 1000 people\n",
                "  pop_denver_proper = 649000.0 \n",
                "  pop_denver_metro = 2814000.0\n",
                "  pop_nyc_proper = 8550000.0  \n",
                "  pop_nyc_metro = 20200000.0\n",
                "\n",
                "  # Enumerate the NYC subregions. More than you might think.\n",
                "  nyc_subregions = usa.groupby(\"region\").get_group(\n",
                "      \"new york city\").subregion.unique().tolist()\n",
                "  num_nyc_posts = len(usa[usa.region == \"new york city\"])\n",
                "  num_denver_posts = len(usa[usa.region == \"denver, CO\"])\n",
                "\n",
                "  den_nyc_rat_prop =  (num_denver_posts/pop_denver_proper) /     \\\n",
                "                      (num_nyc_posts/pop_nyc_proper)\n",
                "\n",
                "  den_nyc_rat_metro =  (num_denver_posts/pop_denver_metro)/     \\\n",
                "                       (num_nyc_posts/pop_nyc_metro)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "2016 posts in NYC spread over:\n",
                "manhattan,\n",
                "brooklyn,\n",
                "queens,\n",
                "bronx,\n",
                "staten island,\n",
                "new jersey,\n",
                "long island,\n",
                "westchester,\n",
                "and fairfield.\n",
                "\n",
                "Considering city propers, we can say that Denver has ~13.0x the usage rate\n",
                "of New York City. Adjusting for census estimates for metropolitan areas, it\n",
                "would seem that Denver's usage is ~7.1x that of NYC's.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "# The range of fifty states (one to fifty, duh)\n",
                "x = np.arange(len(state_usage))\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  print_df(pd.concat([state_usage[:5].round(3),\n",
                "                       state_usage_space,\n",
                "                       state_usage[-5:].sort_values(\"usage\").round(3)]))\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "ax = plt.subplot(111)\n",
                "ax.spines[\"top\"].set_visible(False)\n",
                "ax.spines[\"right\"].set_visible(False)\n",
                "\n",
                "ax.get_xaxis().tick_bottom()\n",
                "ax.get_yaxis().tick_left()\n",
                "\n",
                "plt.xlabel(\"Usage\", fontsize=12)\n",
                "plt.ylabel(\"States\", fontsize=12)\n",
                "\n",
                "plt.suptitle('Usage Distribution for CL politics board', fontsize=14)\n",
                "\n",
                "plt.hist(state_usage.usage,\n",
                "         color=\"#661111\", bins=17)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  # Plot normalized state usage measures\n",
                "  state_usage_min_zero = state_usage - state_usage.min()\n",
                "  state_usage_range = state_usage.max() - state_usage.min()\n",
                "  norm_usage = state_usage_min_zero / state_usage_range\n",
                "\n",
                "  norm_usage.plot(kind=\"density\", \n",
                "                  title=\"Normalized PDF estimations\",\n",
                "                  sharey=True)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  stats = pd.DataFrame({\"mean\": norm_usage.mean(),\n",
                "                        \"median\": norm_usage.median()})\n",
                "  print(\"Mean/median of normalized state usage metrics:\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  print_df(stats)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "colors = cm.YlOrRd(state_usage.usage)\n",
                "\n",
                "ax.spines[\"top\"].set_visible(False)\n",
                "ax.spines[\"right\"].set_visible(False)\n",
                "\n",
                "ax.get_xaxis().tick_bottom()\n",
                "ax.get_yaxis().tick_left()\n",
                "\n",
                "plt.ylabel(\"Patronage\", fontsize=12)\n",
                "plt.xlabel(\"Population\", fontsize=12)\n",
                "\n",
                "plt.suptitle('Patronage vs Population, heatmapped by Usage', fontsize=12)\n",
                "\n",
                "\n",
                "plt.scatter(state_usage.population, state_usage.patronage, color=colors)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  post_politics = usa.join(states.trumpism, how=\"outer\", on=\"state\")\n",
                "  post_politics.trumpism.plot(kind=\"hist\", bins=20, color=[\"#FF9911\"], \n",
                "                              title=\"Distribution of posts by politics\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  avg_post_trumpism = post_politics.trumpism.mean()\n",
                "  trump_votes = voting.trump.sum()\n",
                "  clinton_votes = voting.clinton.sum()\n",
                "  national_trumpism = trump_votes/float((trump_votes + clinton_votes))\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  # Some printing\n",
                "  print((\"Mean trumpism: {:.2f} Trump voters seem to show \" + \n",
                "         \"{:+.2f}% representation\\non CL politics vs General \" + \n",
                "         \"Election results.\").format(\n",
                "             (avg_post_trumpism*100), \n",
                "             (avg_post_trumpism/national_trumpism)*100-100))\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "Mean trumpism: 48.64 Trump voters seem to show -0.71% representation\n",
                "on CL politics vs General Election results.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  post_trumpism_tot = post_politics.trumpism.plot(\n",
                "      kind=\"density\", \n",
                "      title=\"PDF estimation of Trumpism w/ mean\",\n",
                "      sharey=True)\n",
                "  plt.axvline(post_politics.trumpism.mean(), color='r', linestyle='dashed', linewidth=.5)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  corr = states.filter([\"patronage\", \"usage\", \"trumpism\", \"population\"]).corr()\n",
                "  fig, ax = plt.subplots(figsize=(4, 4))\n",
                "  ax.matshow(corr, cmap=plt.cm.seismic)\n",
                "  plt.xticks(range(len(corr.columns)), corr.columns);\n",
                "  plt.yticks(range(len(corr.columns)), corr.columns);\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "print_df(corr, rnd=3)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  pop_english_words = [\"the\", \"re\", \"a\", \"s\",\n",
                "                       \"t\", \"i\", \"of\", \"to\",\n",
                "                       \"and\", \"and\", \"in\", \"is\",\n",
                "                       \"it\", \"you\", \"that\", \"he\",\n",
                "                       \"was\", \"for\", \"on\", \"are\",\n",
                "                       \"with\", \"as\", \"I\", \"his\",\n",
                "                       \"they\", \"be\", \"at\", \"one\",\n",
                "                       \"have\", \"this\", \"from\", \"or\",\n",
                "                       \"had\", \"by\", \"hot\", \"but\",\n",
                "                       \"some\", \"what\", \"there\", \"we\",\n",
                "                       \"can\", \"out\", \"other\", \"were\",\n",
                "                       \"all\", \"your\", \"shit\", \"when\",\n",
                "                       \"up\", \"use\", \"word\", \"how\",\n",
                "                       \"said\", \"an\", \"each\", \"she\",\n",
                "                       \"which\", \"do\", \"their\", \"time\",\n",
                "                       \"if\", \"will\", \"way\", \"about\", \"thought\"\n",
                "                       \"many\", \"fuck\", \"then\", \"them\",\n",
                "                       \"would\", \"write\", \"like\", \"so\",\n",
                "                       \"these\", \"her\", \"long\", \"make\",\n",
                "                       \"thing\", \"see\", \"him\", \"two\",\n",
                "                       \"has\", \"look\", \"more\", \"day\",\n",
                "                       \"could\", \"go\", \"come\", \"did\",\n",
                "                       \"my\", \"sound\", \"no\", \"most\",\n",
                "                       \"number\", \"who\", \"over\", \"know\",\n",
                "                       \"water\", \"than\", \"call\", \"first\",\n",
                "                       \"people\", \"may\", \"down\", \"side\",\n",
                "                       \"been\", \"now\", \"find\"]\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  from collections import Counter\n",
                "\n",
                "  def post_words(df, no_pop=False):\n",
                "      wds = re.findall(r'\\w+', df.title.apply(lambda x: x + \" \").sum())\n",
                "      if no_pop:\n",
                "          # pop_english_words is a list of the most popular (and boring) English\n",
                "          # words. E.g., \"and\", \"to\", \"the\", etc.\n",
                "          wds = [word for word in wds if word.lower() not in pop_english_words]\n",
                "      return  wds\n",
                "\n",
                "  def words(df=usa, no_pop=False):\n",
                "      # word counts across all posts\n",
                "      wds = post_words(df, no_pop)\n",
                "      word_counts = Counter([word.lower() for word in wds])\n",
                "      wd_counts = zip(*[[word, count] for word, count in word_counts.iteritems()])\n",
                "      corpus = pd.Series(wd_counts[1], index=wd_counts[0]).rename(\"counts\")\n",
                "\n",
                "      return corpus.sort_values(ascending=False)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  #\n",
                "  # Find substrings in posts\n",
                "  #\n",
                "\n",
                "  def find_strs(substr, df=usa):\n",
                "      \"\"\"\n",
                "      Get all titles from usa that have substr in their post title. Add some data on capitalization.\n",
                "      \"\"\"\n",
                "\n",
                "      find = lambda s: (1 if re.search(substr, s, re.IGNORECASE) else np.nan)\n",
                "\n",
                "      return df.title[df.title.map(find) == 1].rename(\"*\" + substr + \"*\", inplace=True)\n",
                "\n",
                "  def categ_strs(findings):\n",
                "      \"\"\"\n",
                "      Return a list of\n",
                "      \"\"\"\n",
                "      s = findings.name[1:-1]\n",
                "      find = lambda sub, string: (1 if re.search(sub, string) else np.nan)\n",
                "\n",
                "      proper = findings.apply(lambda x: find(s[0].upper() + s[1:].lower(), x)).rename(\"proper\")\n",
                "      cap = findings.apply(lambda x: find(s.upper(), x)).rename(\"uppercase\")\n",
                "      low = findings.apply(lambda x: find(s.lower(), x)).rename(\"lower\")\n",
                "\n",
                "      return pd.concat([proper, cap, low], axis=1)\n",
                "\n",
                "  def eval_strs(string, df=usa):\n",
                "      findings = find_strs(string, df)\n",
                "      return categ_strs(findings).join(findings)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "def check_ascii(post):\n",
                "    \"\"\"\n",
                "    Determines whether a title is encodable as ascii\n",
                "    \"\"\"\n",
                "    try:\n",
                "        post.encode('ascii')\n",
                "        return True\n",
                "    except UnicodeError:\n",
                "        return False\n",
                "ascii_posts = usa[usa.title.apply(check_ascii)]\n",
                "nonascii_posts = usa[~usa.title.apply(check_ascii)]\n",
                "distinct_states = nonascii_posts[\"state\"].unique()\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "print (\"{0:,} of {1:,} total posts were non-ascii ({2:.2f}%), confined to {3} \"\n",
                "       + \"states.\").format(len(nonascii_posts),\n",
                "                       len(usa),\n",
                "                       len(nonascii_posts)/float(len(usa)) * 100,\n",
                "                       len(distinct_states))\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "392 of 79,462 total posts were non-ascii (0.49%), confined to 26 states.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  pennsylvania = nonascii_posts[nonascii_posts[\"state\"] == \"Pennsylvania\"]\n",
                "  pennsylvania.groupby(\"region\").count()\n",
                "  penn_lenn = float(len(pennsylvania.title))\n",
                "  post_uniqueness = (penn_lenn-pennsylvania.title.nunique())/penn_lenn * 100\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  import itertools\n",
                "  from difflib import SequenceMatcher\n",
                "  def avg_similarity(posts):\n",
                "    def similarity(a, b):\n",
                "      return SequenceMatcher(None, a, b).ratio()\n",
                "    sim_sum = 0\n",
                "    title_product = itertools.product(posts.title, posts.title)\n",
                "    for title_pair in title_product:\n",
                "      sim_sum += similarity(*title_pair)\n",
                "    avg_sim = sim_sum/(len(posts)**2)\n",
                "    return avg_sim\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "    print((\"The average similarity of all non-ascii posts is \" +\n",
                "           \"{:.2f}, while that \\nof only those in Pennsylvania is \" +\n",
                "           \"{:.2f}. The average for all posts in\\nall regions is \" +\n",
                "           \"{:.2f}.\")).format(avg_similarity(nonascii_posts),\n",
                "                              avg_similarity(pennsylvania),\n",
                "                              avg_similarity(usa.sample(200)))\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "The average similarity of all non-ascii posts is 0.19, while that \n",
                "of only those in Pennsylvania is 0.37. The average for all posts in\n",
                "all regions is 0.18.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  # Grab some words\n",
                "  lib_words = words(df=post_politics[post_politics.trumpism < .45],\n",
                "                    no_pop=True).rename(\"libs\")\n",
                "  conserv_words = words(df=post_politics[post_politics.trumpism > .55],\n",
                "                        no_pop=True).rename(\"conservs\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  # THIS IS BROKEN AND BAD. Placeholder code\n",
                "  rat = lambda df: df.libs/df.conservs\n",
                "  ratio = pd.DataFrame().join([lib_words[lib_words >= 10],\n",
                "                               conserv_words[conserv_words >= 10]],\n",
                "                              how=\"outer\").apply(rat, axis=1).dropna()\n",
                "  ratio = ratio.rename(\"dem/rep ratio\")\n",
                "\n",
                "  lib_con_ratio = pd.DataFrame(posts_corpus).join(ratio.sort_values(ascending=False),\n",
                "                                                  how=\"inner\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "            counts  dem/rep ratio\n",
                "thought        595      19.080000\n",
                "tv             231      14.545455\n",
                "global         339      11.583333\n",
                "world          596      10.941176\n",
                "top            166       9.600000\n",
                "wake           198       9.090909\n",
                "government     350       8.550000\n",
                "dnc            133       8.400000\n",
                "life           255       8.375000\n",
                "york           166       8.250000\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  print_df(pd.DataFrame(pd.concat([find_strs(\"tax\"),\n",
                "                                   find_strs(\"speech\"),\n",
                "                                   find_strs(\"russian\")]).rename(\n",
                "                                       \"title\")).sample(5), \n",
                "           rnd=3)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "p = posts_corpus[:25].sort_values(ascending=True)\n",
                "\n",
                "ax = p.plot(kind=\"bar\", color=\"#662200\", grid=True)\n",
                "\n",
                "ax.spines[\"top\"].set_visible(False)\n",
                "ax.spines[\"right\"].set_visible(False)\n",
                "\n",
                "ax.get_xaxis().tick_bottom()\n",
                "ax.get_yaxis().tick_left()\n",
                "\n",
                "plt.ylabel(\"Occurences\", fontsize=12)\n",
                "\n",
                "plt.suptitle('Word usages', fontsize=14)\n",
                "\n",
                "ax.spines[\"top\"].set_visible(False)\n",
                "ax.spines[\"right\"].set_visible(False)\n",
                "\n",
                "ax.get_xaxis().tick_bottom()\n",
                "ax.get_yaxis().tick_left()\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                " # Splitting a series into chunks such that values.sum() = val (or as close\n",
                " # as possible, greedily) so we can wee how the diversity of words is\n",
                " # distributed:\n",
                " def splicer(ss, val):\n",
                "   indices = ss.index.tolist()\n",
                "   if len(indices) <= 1:\n",
                "     return pd.Series(ss[index[0]], index=[[indices[0]]])\n",
                "   left = [ss.index[0]]\n",
                "   right = ss.index[1:].tolist()\n",
                "   s = ss[left[0]]\n",
                "   while s < val and len(right) > 0:\n",
                "     i = right.pop(0)\n",
                "     left.append(i)\n",
                "     s += ss[i]\n",
                "   return [ss.filter(left)] + (splicer(ss.filter(right), val) if len(right) > 0 else [])\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                " chunks = splicer(posts_corpus, posts_corpus.iloc[0])\n",
                " ax = plt.subplot()\n",
                " \n",
                " ax.spines[\"top\"].set_visible(False)\n",
                " ax.spines[\"right\"].set_visible(False)\n",
                "\n",
                " ax.get_xaxis().tick_bottom()\n",
                " ax.get_yaxis().tick_left()\n",
                "\n",
                " plt.ylabel(\"\", fontsize=12)\n",
                " plt.suptitle('', fontsize=14)\n",
                "\n",
                " ax.spines[\"top\"].set_visible(False)\n",
                " ax.spines[\"right\"].set_visible(False)\n",
                "\n",
                " ax.get_xaxis().tick_bottom()\n",
                " ax.get_yaxis().tick_left()\n",
                "\n",
                " plt.bar(np.arange(0, len(chunks)), np.array([len(c) for c in chunks]))\n",
                " \n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "trumps = eval_strs(\"trump\").join(usa.state, how=\"inner\")\n",
                "trumps_by_state = trumps.groupby(\"state\").count().join(states).drop([\"clinton\", \"trump\"], axis=1)\n",
                "up_over_trumps = (trumps_by_state.uppercase/trumps_by_state[\"*trump*\"]).rename(\"uppercase usage\")\n",
                "prop_over_trumps = (trumps_by_state.proper/trumps_by_state[\"*trump*\"]).rename(\"propercase usage\")\n",
                "trumps_over_pat = (trumps_by_state[\"*trump*\"]/trumps_by_state.patronage).rename(\"trumps usage\")\n",
                "trumps_by_state = trumps_by_state.join([prop_over_trumps, up_over_trumps, trumps_over_pat], how=\"outer\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  trumps_vs_trumpism = trumps_by_state.sort_values(\n",
                "      \"trumpism\", ascending=True).filter([\"propercase usage\",\n",
                "                          \"uppercase usage\"])\n",
                "\n",
                "  trumps_vs_trumpism.plot(kind=\"bar\", stacked=True, figsize=(10, 5))\n",
                "\n",
                "  ax = plt.subplot()\n",
                "\n",
                "  ax.spines[\"top\"].set_visible(False)\n",
                "  ax.spines[\"right\"].set_visible(False)\n",
                "\n",
                "  ax.get_xaxis().tick_bottom()\n",
                "  ax.get_yaxis().tick_left()\n",
                "\n",
                "  plt.xlabel(\"States, in order of trumpism\")\n",
                "\n",
                "  ax.spines[\"top\"].set_visible(False)\n",
                "  ax.spines[\"right\"].set_visible(False)\n",
                "\n",
                "  ax.get_xaxis().tick_bottom()\n",
                "  ax.get_yaxis().tick_left()\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "    post_politics.trumpism.plot(kind=\"density\", linewidth=0.8)\n",
                "\n",
                "    ax = plt.subplot()\n",
                "\n",
                "    ax.spines[\"top\"].set_visible(False)\n",
                "    ax.spines[\"right\"].set_visible(False)\n",
                "\n",
                "    ax.get_xaxis().tick_bottom()\n",
                "    ax.get_yaxis().tick_left()\n",
                "\n",
                "    plt.ylabel(\"Occurences\", fontsize=12)\n",
                "\n",
                "    ax.spines[\"top\"].set_visible(False)\n",
                "    ax.spines[\"right\"].set_visible(False)\n",
                "\n",
                "    ax.get_xaxis().tick_bottom()\n",
                "    ax.get_yaxis().tick_left()\n",
                "\n",
                "    trumps_trumpism = trumps.join(post_politics.trumpism)\n",
                "\n",
                "    trumps_trumpism.trumpism.plot(kind=\"density\", \n",
                "                                  title=\"PDF of trumpism for \"  +  \n",
                "                                  \"posts containing 'Trump'\",\n",
                "                                  linewidth=2)\n",
                "    plt.axvline(trumps_trumpism.trumpism.mean(), color='r',\n",
                "                linestyle='dashed', linewidth=.5)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  cap_trumps = trumps_trumpism[trumps_trumpism.uppercase > 0]\n",
                "\n",
                "  ax = plt.subplot()\n",
                "\n",
                "  ax.spines[\"top\"].set_visible(False)\n",
                "  ax.spines[\"right\"].set_visible(False)\n",
                "\n",
                "  ax.get_xaxis().tick_bottom()\n",
                "  ax.get_yaxis().tick_left()\n",
                "\n",
                "  ax.spines[\"top\"].set_visible(False)\n",
                "  ax.spines[\"right\"].set_visible(False)\n",
                "\n",
                "  ax.get_xaxis().tick_bottom()\n",
                "  ax.get_yaxis().tick_left()\n",
                "\n",
                "  cap_trumps.trumpism.plot(kind=\"density\", \n",
                "                           title=\"PDF of trumpism for posts \" \\\n",
                "                           \"containing 'TRUMP'\",\n",
                "                           color='blue', linewidth=1.5)\n",
                "  plt.axvline(cap_trumps.trumpism.mean(), color='r',\n",
                "              linestyle='dashed', linewidth=.5)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  liberal_sample = trumps_trumpism[trumps_trumpism.trumpism < .45].sample(5)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  print(\"Selecting states that are espectially \" \\\n",
                "        \"anti-trump:\\n\")\n",
                "  print_df(pd.DataFrame(liberal_sample[\"*trump*\"]))\n",
                "\n",
                "  print(\"Politically liberal states composing \" +\n",
                "        \"the above sampling:\\n{}.\".format(\n",
                "             \", \".join(\"{}\".format(r) for r in liberal_sample.state.unique())))\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  liberal = float(posts_corpus[\"liberal\"])\n",
                "  liberal_p = float(posts_corpus[\"liberals\"])\n",
                "  conserv = float(posts_corpus[\"conservative\"])\n",
                "  conserv_p = float(posts_corpus[\"conservatives\"])\n",
                "\n",
                "  print (\"liberal/conservative: {0:.2f}\\n\" +\n",
                "         \"liberals/conservatives: {1:.2f}\\n\" +\n",
                "         \"liberal(s)/conservative(s): {2:.2f}\" +\n",
                "         \"\") .format(liberal/conserv,\n",
                "                     liberal_p/conserv_p,\n",
                "                     (liberal+liberal_p)/(conserv+conserv_p))\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  print(\"*singular/plural*\\n\" +\n",
                "        \"'conservative': {0:.3f}\\n\" +\n",
                "        \"'liberal': \" +\n",
                "        \"{1:.3f}\").format(posts_corpus[\"conservative\"]/float(posts_corpus[\"conservatives\"]),\n",
                "                          posts_corpus[\"liberal\"]/float(posts_corpus[\"liberals\"]))\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                ",*singular/plural*\n",
                "'conservative': 1.495\n",
                "'liberal': 1.789\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  libs = eval_strs(\"liberal\").sum(numeric_only=True)\n",
                "  conservs = eval_strs(\"conservative\").sum(numeric_only=True)\n",
                "\n",
                "  lib_con_rates = (libs/libs.sum()) / (conservs/conservs.sum())\n",
                "  lib_con_rates.rename(\"'liberal'/'conservative' usage\", inplace=True)\n",
                "\n",
                "  lib_con_cap_rat = pd.DataFrame([(libs/conserv).rename(\n",
                "      \"# 'liberal' per 'conservative'\"), lib_con_rates])\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  from textblob import TextBlob\n",
                "\n",
                "  def semants(text):\n",
                "      blob = TextBlob(text)\n",
                "      ss = 0\n",
                "      for sentence in blob.sentences:\n",
                "          ss += sentence.sentiment.polarity\n",
                "      return float(ss)/len(blob.sentences)\n",
                "\n",
                "  # package does not like non-ascii encodings\n",
                "  trumps_ascii = trumps[trumps[\"*trump*\"].apply(check_ascii)]\n",
                "\n",
                "\n",
                "  usa_sentiment = post_politics.join(ascii_posts.title.apply(\n",
                "      semants).rename(\"sentiment\"))\n",
                "  trumps_sentiment = usa_sentiment.filter(trumps_ascii.index, axis=0)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  zero_sents = len(usa_sentiment[usa_sentiment.sentiment == 0])\n",
                "  print(('Number of posts with 0 sentiment: {0:,} ' + \n",
                "         '({1:.2f}%).').format(zero_sents, \n",
                "                               float(zero_sents)/len(usa_sentiment)*100))\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "Number of posts with 0 sentiment: 52,774 (66.41%).\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  from os import path\n",
                "  from PIL import Image\n",
                "\n",
                "  from wordcloud import WordCloud\n",
                "\n",
                "  d = path.dirname(\".\")\n",
                "\n",
                "  plt.figure(num=None, figsize=(10, 8))\n",
                "\n",
                "  trump_mask = np.array(Image.open(path.join(d, \"img/Trump_silhouette.png\")))\n",
                "\n",
                "  wc = WordCloud(background_color=\"white\", max_words=2000, mask=trump_mask)\n",
                "\n",
                "  wc.generate(posts_sum)\n",
                "\n",
                "  wc.to_file(path.join(d, \"img/Trump_test.png\"))\n",
                "\n",
                "  plt.imshow(wc)\n",
                "  plt.axis(\"off\")\n",
                "  plt.figure()\n",
                "  plt.imshow(trump_mask, cmap=plt.cm.gray)\n",
                "  plt.axis(\"off\")\n",
                "\n",
                "  plt.show()\n",
                "\n",
                "\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 2",
            "language": "python",
            "name": "python2"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 2
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython2",
            "version": "2.7.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
