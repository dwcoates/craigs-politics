{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mtl\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "\n",
    "import pprint as pp\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "pd.options.display.max_colwidth = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "38,692 total posts exctracted from 416 regions over 52 state. The most popular\n",
      "state was California, and the most popular region was, surprisingly, denver, CO.\n"
     ]
    }
   ],
   "source": [
    "# read us data collected by craigcrawler\n",
    "usa_raw = pd.read_csv(\"data/us.csv\", index_col=0)\n",
    "post_count_total_raw = len(usa_raw)\n",
    "post_count_by_state_raw = usa_raw.groupby(\"state\").count()[\"title\"].sort_values(ascending=False)\n",
    "post_count_by_region_raw = usa_raw.groupby(\"region\").count()[\"title\"].sort_values(ascending=False)\n",
    "\n",
    "print (\"\\n{0:,} total posts exctracted from {3:,} regions over {4} \"+ \n",
    "       \"state. The most popular\\nstate was {1}, and the most \" + \n",
    "       \"popular region was, surprisingly, {2}.\").format(post_count_total_raw,\n",
    "                                                        post_count_by_state_raw.index[0],\n",
    "                                                        post_count_by_region_raw.index[0],\n",
    "                                                        len(post_count_by_region_raw),\n",
    "                                                        len(post_count_by_state_raw))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 675,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some preprocessing to check data corrupted files\n",
    "assert len(usa_raw[\"state\"].unique()) == 52\n",
    "len(usa_raw[\"region\"].unique()) == 416\n",
    "\n",
    "len(usa_raw[\"subregion\"].unique()) + len(usa_raw[\"region\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# How responsible is trump for my corrupted data?\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of census keys, if curious:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "        GEO.id GEO.id2 GEO.display-label        HD01_S001 HD02_S001 HD01_S002  \\\n1  0400000US01      01           Alabama  4779736(r38235)     100.0    304957   \n2  0400000US02      02            Alaska   710231(r38823)     100.0     53996   \n3  0400000US04      04           Arizona          6392017     100.0    455715   \n4  0400000US05      05          Arkansas  2915918(r39193)     100.0    197689   \n5  0400000US06      06        California         37253956     100.0   2531333   \n\n  HD02_S002 HD01_S003 HD02_S003 HD01_S004    ...    HD01_S182 HD02_S182  \\\n1       6.4    308229       6.4    319655    ...      3311304    ( X )    \n2       7.6     50887       7.2     50816    ...       448438    ( X )    \n3       7.1    453680       7.1    448664    ...      4134117    ( X )    \n4       6.8    196877       6.8    197559    ...      1929218    ( X )    \n5       6.8   2505839       6.7   2590930    ...     20742929    ( X )    \n\n  HD01_S183 HD02_S183 HD01_S184 HD02_S184 HD01_S185 HD02_S185 HD01_S186  \\\n1      2.52    ( X )     571202      30.3   1352616    ( X )       2.37   \n2      2.76    ( X )      95293      36.9    235441    ( X )       2.47   \n3      2.63    ( X )     809303      34.0   2118516    ( X )       2.62   \n4      2.51    ( X )     378928      33.0    907769    ( X )       2.40   \n5      2.95    ( X )    5542127      44.1  15691211    ( X )       2.83   \n\n  HD02_S186  \n1    ( X )   \n2    ( X )   \n3    ( X )   \n4    ( X )   \n5    ( X )   \n\n[5 rows x 375 columns]"
      ]
     },
     "execution_count": 667,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# US census data for 2010 from the census bureau\n",
    "#\n",
    "\n",
    "# Census data is not labelled exactly as my data is. Some states are named a little differently,\n",
    "# and regions are almost never named similarly. These have to be resolved.\n",
    "\n",
    "census = pd.read_csv(\"data/census/DEC_10_DP_DPDP1_with_ann.csv\")[1:]\n",
    "# keys for the census data. Only really care about two of them (there are hundreds):\n",
    "TOT_NUM_ID = \"HD01_S001\" # total number key\n",
    "TOT_PER_ID = \"HD02_S001\" # total percent key\n",
    "\n",
    "# Keys for geography stuff. Table is an index table.\n",
    "# These keys are used as index for census table.\n",
    "census_keys = pd.read_csv(\"data/census/DEC_10_DP_G001_with_ann.csv\")[1:]\n",
    "GEO_KEY = \"GEO.display-label\"\n",
    "GEO_ID = \"GEO.id\"\n",
    "# keys used to reference states in census\n",
    "census_states_keys = dict(zip(list(census_keys[GEO_KEY]), list(census_keys[GEO_ID][:52])))\n",
    "\n",
    "print \"Sample of census keys, if curious:\"\n",
    "zip(list(census_states_keys), list(census_states_keys.values()))[:5]\n",
    "\n",
    "census[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Puerto Rico'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-641-a6309db01c1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmisnamed_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcensus_states_keys\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mpost_count_by_state_raw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmisnamed_states\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/dodge/.local/anaconda2/lib/python2.7/site-packages/pandas/core/series.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/dodge/.local/anaconda2/lib/python2.7/site-packages/pandas/indexes/base.pyc\u001b[0m in \u001b[0;36mget_value\u001b[1;34m(self, series, key)\u001b[0m\n\u001b[0;32m   1992\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1993\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1994\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1995\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pragma: no cover\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1996\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0me1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Puerto Rico'"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Standardizing census and cl data names This may have to be limited\n",
    "# to states names. Regions will be, at the very least, a huge pain.\n",
    "# Most likely will be unresolvable.\n",
    "#\n",
    "misnamed_states = []\n",
    "for name in census_states_keys:\n",
    "    if post_count_by_state_raw[name] < 0: misnamed_states.append(state)\n",
    "\n",
    "# Standarize top city names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "199 of 38,981 total posts were non-ascii (0.51%), confined to 23 states.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# ascii vs. unicode\n",
    "#\n",
    "\n",
    "def check_ascii(post):\n",
    "    \"\"\"\n",
    "    Determines whether a title is properly encoded.\n",
    "    \"\"\"\n",
    "    title = post[\"title\"]\n",
    "    try:\n",
    "        title.encode('ascii')\n",
    "        return True\n",
    "    except UnicodeError:\n",
    "        return False\n",
    "\n",
    "ascii_titles_tv = usa_raw.apply(check_ascii, axis=1)\n",
    "nonascii_posts = usa_raw[~ascii_titles_tv]\n",
    "\n",
    "distinct_states = nonascii_posts[\"state\"].unique()\n",
    "print (\"\\n{0:,} of {1:,} total posts were non-ascii ({2:.2f}%), confined to {3} \"\n",
    "       + \"states.\").format(len(nonascii_posts),\n",
    "                       total_posts_raw,\n",
    "                       len(nonascii_posts)/float(total_posts_raw) * 100,\n",
    "                       len(distinct_states))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top ten most popular unicode states:\n",
      "state\n",
      "Pennsylvania    18\n",
      "Maryland         8\n",
      "New York         7\n",
      "California       6\n",
      "Arizona          5\n",
      "Florida          5\n",
      "Washington       4\n",
      "Texas            4\n",
      "Colorado         3\n",
      "Connecticut      3\n",
      "Name: title, dtype: int64\n",
      "ðŸ™ŠðŸ™‰The ZOMBIES are comingðŸ™ŠðŸ™‰\n",
      "\n",
      "A single Trump memester seems to be responsible for the chaos in Pennsylvania.\n",
      "I suspect that these crazy unicode posts are mostly done by a very small\n",
      "set of people, though there is no way to tell.\n",
      "\n",
      "Random sample of 5 non-ascii Pennsylvania posts\n",
      "18505                     ðŸ™ŠðŸ™‰The ZOMBIES are comingðŸ™ŠðŸ™‰\n",
      "18514    ðŸ‘‘HAPPY NEW YEARSðŸ‘‘ America ðŸ‘‘ DONALD J.TRUMPðŸ‘‘\n",
      "18515    ðŸŽ€HAPPY NEW YEARðŸŽ€ AMERICA ðŸ‘‘ DONALD J. TRUMPðŸ‘‘\n",
      "18530              ðŸ’¥DONALD J. TRUMPðŸ’¥[Need a Tissue Anyone]\n",
      "18540                                     ðŸ—½Keep on CryingðŸ—½\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# ascii vs. unicode\n",
    "#\n",
    "\n",
    "nonascii_states_count = nonascii_posts.groupby(\n",
    "    \"state\").title.nunique().sort_values(ascending=False)\n",
    "print \"\\nTop ten most popular unicode states:\"\n",
    "print nonascii_states_count[:10]\n",
    "\n",
    "pennsylvania = nonascii_posts[nonascii_posts[\"state\"] == \"Pennsylvania\"]\n",
    "print pennsylvania[\"title\"].tolist()[0]\n",
    "\n",
    "print(\"\\nA single Trump memester seems to be responsible for the chaos \" +\n",
    "      \"in Pennsylvania.\\n\" + \"I suspect that these crazy unicode posts \" +\n",
    "      \"are mostly done by a very small\\nset of people, though there is \" +\n",
    "      \"no way to tell.\")\n",
    "print \"\\nRandom sample of 5 non-ascii Pennsylvania posts\"\n",
    "print pennsylvania[\"title\"][:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top ten most popular states\n",
      "state\n",
      "California      3801\n",
      "Florida         3585\n",
      "Texas           3155\n",
      "New York        2358\n",
      "Colorado        2066\n",
      "Pennsylvania    1881\n",
      "Washington      1400\n",
      "Ohio            1396\n",
      "Arizona         1381\n",
      "Michigan        1359\n",
      "Name: title, dtype: int64\n",
      "\n",
      "Top ten most popular regions\n",
      "region\n",
      "denver, CO               1271\n",
      "new york city            1023\n",
      "seattle-tacoma            883\n",
      "pittsburgh, PA            801\n",
      "phoenix, AZ               790\n",
      "south florida             750\n",
      "los angeles               728\n",
      "minneapolis / st paul     700\n",
      "dallas / fort worth       681\n",
      "SF bay area               679\n",
      "Name: title, dtype: int64\n",
      "\n",
      "\n",
      "7 regions in Colorado\n"
     ]
    }
   ],
   "source": [
    "state_patronage = usa.groupby('state').count()[\"title\"].sort_values(ascending=False)\n",
    "region_patronage = usa.groupby('region').count()[\"title\"].sort_values(ascending=False)\n",
    "\n",
    "#\n",
    "# INCLUDE NORMALIZED FIGURES (posts per capita)\n",
    "#\n",
    "\n",
    "print \"\\nTop ten most popular states\"\n",
    "print state_patronage[:10]\n",
    "\n",
    "# Denver is about 10 times as populous as nyc, for example\n",
    "print \"\\nTop ten most popular regions\"\n",
    "print region_patronage[:10]\n",
    "\n",
    "print \"\\n\\n{0} regions in Colorado\".format(usa[usa['state'] == \"Colorado\"][\"region\"].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def occurance(s, word):\n",
    "    \"\"\"\n",
    "    Return number of occurances of word in s. Include properties.\n",
    "    \"\"\"\n",
    "    # completely lowercase\n",
    "    # first letter capitalized\n",
    "    # partially capitalized\n",
    "    # completely capitalized\n",
    "\n",
    "occurance(\"this IS a senTence\")\n",
    "\n",
    "def capitalization(s, word):\n",
    "    \"\"\"\n",
    "    Return qualities of capitization of word in s\n",
    "    \"\"\"\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "name": "politics.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
