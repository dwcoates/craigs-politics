{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},

            "source": [
                "<h1 align=\"center\"><font color=\"0066FF\" size=110>A look into Craigslist politics</font></h1>\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "%matplotlib inline\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import scipy\n",
                "from scipy import stats\n",
                "import matplotlib as mpln\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "import pprint as pp\n",
                "import pickle\n",
                "import re\n",
                "\n",
                "pd.options.display.max_colwidth = 1000\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "# read us data collected by craigcrawler \n",
                "usa_raw = pd.read_csv(\"data/us.csv\", index_col=0)\n",
                "post_count_total_raw = len(usa_raw)\n",
                "post_count_by_state_raw = usa_raw.groupby(\"state\").count()[\"title\"]#.sort_values(ascending=False)\n",
                "post_count_by_region_raw = usa_raw.groupby(\"region\").count()[\"title\"]#.sort_values(ascending=False)\n",
                "\n",
                "print (\"\\n{0:,} total posts exctracted from {3:,} regions over {4} \"+ \n",
                "       \"state. The most popular\\nstate was {1}, and the most \" + \n",
                "       \"popular region was, surprisingly, {2}.\").format(post_count_total_raw,\n",
                "                                                        post_count_by_state_raw.index[0],\n",
                "                                                        post_count_by_region_raw.index[0],\n",
                "                                                        len(post_count_by_region_raw),\n",
                "                                                        len(post_count_by_state_raw))\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "# Keys for geography stuff. Table is an index table.\n",
                "# These keys are used as index for census table.\n",
                "GEO_NAME = \"GEO.display-label\"\n",
                "GEO_KEY = \"GEO.id\"\n",
                "state_keys = pd.read_csv(\"data/census/DEC_10_DP_G001_with_ann.csv\")[1:].set_index(GEO_KEY)\n",
                "\n",
                "state_keys = state_keys.filter([GEO_NAME])[:52]\n",
                "state_keys = state_keys[state_keys[GEO_NAME]!= \"Puerto Rico\"]\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  # keys for the census data. Only really care about two of them (there are hundreds):\n",
                "  TOT_NUM_ID = \"HD01_S001\" # total number key\n",
                "  TOT_PER_ID = \"HD02_S001\" # total percent key\n",
                "\n",
                "  census = pd.read_csv(\"data/census/DEC_10_DP_DPDP1_with_ann.csv\")[1:].set_index(GEO_KEY)\n",
                "\n",
                "  census = census.filter([TOT_NUM_ID])\n",
                "  census = census.join(state_keys, how=\"right\")\n",
                "  census.columns = [\"population\", \"state\"]\n",
                "  census.set_index(\"state\", inplace=True)\n",
                "    \n",
                "  def correct_stat(s):\n",
                "      \"\"\"\n",
                "      Some states have extra information for population. \n",
                "      Example: 25145561(r48514)\n",
                "      \"\"\"\n",
                "      loc = s.find(\"(\")\n",
                "      return int(s[:loc] if loc > 0 else s)\n",
                "\n",
                "  census.population = census.population.apply(correct_stat)\n",
                "  \n",
                "  census = census.drop(\"District of Columbia\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  import requests\n",
                "  from scrapy import Selector\n",
                "\n",
                "  atlas_url = \"http://uselectionatlas.org/RESULTS/data.php?year=2016&datatype=national&def=1&f=1&off=0&elect=0\"\n",
                "  atlas_source = requests.get(atlas_url).text\n",
                "  select = Selector(text=atlas_source).xpath('//*[@id=\"datatable\"]/tbody/tr')\n",
                "\n",
                "  convert = lambda s: int(s.replace(',', ''))\n",
                "  vote_names = map(str, select.xpath('td[3]/a/text()').extract())\n",
                "  # Correct name for DC\n",
                "  vote_names[8] = \"District of Columbia\"\n",
                "  clinton_votes = map(convert, select.xpath('td[17]/text()').extract())\n",
                "  trump_votes = map(convert, select.xpath('td[18]/text()').extract())\n",
                "\n",
                "  gen_votes = pd.DataFrame({\"clinton\": clinton_votes, \"trump\": trump_votes}, index=vote_names)\n",
                "\n",
                "  trump_favor = pd.DataFrame(gen_votes[\"trump\"]/gen_votes.sum(axis=1), columns=[\"trumpism\"], index=vote_names)  \n",
                "  voting = gen_votes.join(trump_favor).sort_values(\"trumpism\", ascending=False)  \n",
                "  voting = voting.drop(\"District of Columbia\")\n",
                "\n",
                "  # for pretty printing\n",
                "  voting_space = pd.DataFrame([[\"------\", \"------\", \"------\"]],index=[\"*SPACE*\"], columns=voting.columns) \n",
                "  print(pd.concat([voting[:5], voting_space, voting[-5:].sort_values(\"trumpism\")]))\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  print \"Data tests... \\n\\nAssertions Passed\\n\\n\"\n",
                "\n",
                "  # Confirm all expected regions and states present\n",
                "  assert len(usa_raw[\"state\"].unique()) == 52 # expected number of states\n",
                "  assert len(usa_raw[\"region\"].unique()) == 416  # expected number of regions\n",
                " \n",
                "  # Confirm that there are no posts without regions/states. Not all CL \n",
                "  # regions have subregions, so it's okay for null subregions.\n",
                "  assert len(usa_raw[usa_raw[\"state\"].isnull()].index) == 0\n",
                "  assert len(usa_raw[usa_raw[\"region\"].isnull()].index) == 0\n",
                "\n",
                "  # Find regions/subregions for which there are no posts\n",
                "  postless_regions = usa_raw[usa_raw[\"title\"].isnull()]  \n",
                "  postless_regions_times = usa_raw[usa_raw[\"date\"].isnull()]\n",
                "\n",
                "  # not actually an effective test, but good enough\n",
                "  assert len(postless_regions) == len(postless_regions_times)\n",
                "\n",
                "  print((\"{0:,} regions/subregions over {1} states without \" + \n",
                "         \"any posts.\").format(len(postless_regions), postless_regions[\"state\"].nunique()))  \n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "# Drop empty regions.\n",
                "usa = usa_raw.dropna(subset=[\"title\", \"date\"], how=\"any\", axis=0)\n",
                "assert len(postless_regions) == len(usa_raw)-len(usa)\n",
                "\n",
                "# Get rid of territories (Guam, Puerto Rico)\n",
                "usa = usa[usa[\"state\"] != \"Territories\"]\n",
                "usa = usa[usa[\"state\"] != \"District of Columbia\"]\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "assert set(usa.state.unique()) == set(census.index) and len(usa.state.unique() == len(census.index))\n",
                "\n",
                "print \"Census data complete\"\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "assert set(usa.state.unique()) == set(voting.index) and len(usa.state.unique() == len(voting.index))\n",
                "\n",
                "print \"Voting data complete\"\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  patronage = pd.DataFrame(usa.groupby('state').size(), columns=[\"patronage\"]).sort_values(\n",
                "      \"patronage\",ascending=False)\n",
                "\n",
                "  print \"Top ten most frequented states:\\n{}\".format(patronage[:10])\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "    cl_by_state = patronage.join(census, how=\"inner\")\n",
                "    usage = cl_by_state.apply(\n",
                "        lambda df: df[\"patronage\"] / float(df[\"population\"]), axis=1)\n",
                "\n",
                "    # Weight for max = 1.000\n",
                "    usage_weighted = (usage - usage.min())/(usage.max() - usage.min())\n",
                "    weighted_usage = pd.DataFrame((usage_weighted),\n",
                "                                   columns=[\"usage\"])\n",
                "\n",
                "    state_usage = pd.concat([cl_by_state, weighted_usage],\n",
                "                            axis=1).sort_values(\"usage\",\n",
                "                                                ascending=False)\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  state_usage_space = pd.DataFrame([[\"------\", \"------\", \"------\"]],index=[\"*SPACE*\"],\n",
                "                                   columns=state_usage.columns)\n",
                "\n",
                "  print(pd.concat([state_usage[:5], state_usage_space, state_usage[-5:].sort_values(\"usage\")]))\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "x = np.arange(len(pat))\n",
                "\n",
                "plt.bar(x, pat.population)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "pat = state_usage.sort_values(\"patronage\", ascending=True)\n",
                "x = np.arange(len(pat))\n",
                "\n",
                "ax = plt.subplot(111)  \n",
                "ax.spines[\"top\"].set_visible(False)  \n",
                "ax.spines[\"right\"].set_visible(False)  \n",
                "    \n",
                "ax.get_xaxis().tick_bottom()  \n",
                "ax.get_yaxis().tick_left()  \n",
                "\n",
                "plt.xlabel(\"Usage\", fontsize=16)  \n",
                "plt.ylabel(\"States\", fontsize=16)      \n",
                "\n",
                "plt.hist(states.usage,\n",
                "         color=\"#3F5D7D\", bins=15)  \n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "norm_usage = (state_usage - state_usage.min()) / (state_usage.max() - state_usage.min())\n",
                "norm_usage.plot(kind=\"density\", title=\"Normalized PDF estimations\", sharey=True)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "plt.plot(x, state_usage.population.sort_values().values)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "# Getting rid of California\n",
                "p1 = state_usage.sort_values(\"population\", ascending=False)[5:]\n",
                "\n",
                "plt.bar(p1[\"population\"], p1[\"usage\"])\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "post_politics = usa.join(voting, on=\"state\").join(find_strs(\"trump\"), how=\"inner\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "print(states.filter([\"patronage\", \"usage\", \"normalized\", \"trumpism\"]).corr())\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "pop_english_words = [\"the\", \"re\", \"a\", \"s\", \"t\", \"i\", \"of\", \"to\", \"and\", \"and\", \"in\", \"is\", \"it\", \"you\", \"that\", \"he\", \"was\", \"for\", \"on\", \"are\", \"with\", \"as\", \"I\", \"his\", \"they\", \"be\", \"at\", \"one\", \"have\", \"this\", \"from\", \"or\", \"had\", \"by\", \"hot\", \"but\", \"some\", \"what\", \"there\", \"we\", \"can\", \"out\", \"other\", \"were\", \"all\", \"your\", \"shit\", \"when\", \"up\", \"use\", \"word\", \"how\", \"said\", \"an\", \"each\", \"she\", \"which\", \"do\", \"their\", \"time\", \"if\", \"will\", \"way\", \"about\", \"many\", \"fuck\", \"then\", \"them\", \"would\", \"write\", \"like\", \"so\", \"these\", \"her\", \"long\", \"make\", \"thing\", \"see\", \"him\", \"two\", \"has\", \"look\", \"more\", \"day\", \"could\", \"go\", \"come\", \"did\", \"my\", \"sound\", \"no\", \"most\", \"number\", \"who\", \"over\", \"know\", \"water\", \"than\", \"call\", \"first\", \"people\", \"may\", \"down\", \"side\", \"been\", \"now\", \"find\"]\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "posts_corpus = words(df=usa, no_pop=True)\n",
                "\n",
                "usa_words_full = post_words(df=usa)\n",
                "usa_words = post_words(df=usa, no_pop=True)\n",
                "\n",
                "posts_sum = \" \".join([word for word in usa_words_full if word.lower() not in pop_english_words])\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  def find_strs(substr, df=usa):\n",
                "      \"\"\"\n",
                "      Get all titles from usa that have substr in their post title. Add some data on capitalization.\n",
                "      \"\"\"\n",
                "      \n",
                "      find = lambda s: (1 if re.search(substr, s, re.IGNORECASE) else np.nan)\n",
                "\n",
                "      return df.title[df.title.map(find) == 1].rename(\"*\" + substr + \"*\", inplace=True)\n",
                "\n",
                "  def categ_strs(findings):\n",
                "      \"\"\"\n",
                "      Return a list of \n",
                "      \"\"\"\n",
                "      s = findings.name[1:-1]\n",
                "      find = lambda sub, string: (1 if re.search(sub, string) else np.nan)\n",
                "\n",
                "      proper = findings.apply(lambda x: find(s[0].upper() + s[1:].lower(), x)).rename(\"proper\")\n",
                "      cap = findings.apply(lambda x: find(s.upper(), x)).rename(\"uppercase\")\n",
                "      low = findings.apply(lambda x: find(s.lower(), x)).rename(\"lower\")\n",
                "\n",
                "      return pd.concat([proper, cap, low], axis=1)\n",
                "\n",
                "  def eval_strs(string, df=usa):\n",
                "      findings = find_strs(string, df)\n",
                "      return categ_strs(findings).join(findings)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  lib_words = words(df=post_politics[post_politics.trumpism < .45], no_pop=True).rename(\"libs\")\n",
                "  conserv_words = words(df=post_politics[post_politics.trumpism > .55], no_pop=True).rename(\"conservs\")  \n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "# number of words\n",
                "# percentage distinct\n",
                "usa_words\n",
                "\n",
                "#by demographic\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "\n",
                "  rat = lambda df: df.libs/df.conservs\n",
                "  ratio = pd.DataFrame().join([lib_words[lib_words >= 10], conserv_words[conserv_words >= 10]],\n",
                "                                      how=\"outer\").apply(rat, axis=1).dropna()\n",
                "  ratio = ratio.rename(\"dem/rep ratio\")\n",
                "  lib_con_ratio = pd.DataFrame(posts_corpus).join(ratio.sort_values(ascending=False), how=\"inner\")\n",
                "  lib_con_ratio.sort(\"dem/rep ratio\", ascending=False, inplace=True)\n",
                "  print(lib_con_ratio[:10])\n",
                "  #lib_con_ratio = posts_corpus.join(lib_con_ratio.sort_values(ascending=False), on=\"words\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "trumps = eval_strs(\"trump\").join(usa.state, how=\"inner\")\n",
                "trumps_by_state = trumps.groupby(\"state\").count().join(states).drop([\"clinton\", \"trump\"], axis=1)\n",
                "up_over_trumps = (trumps_by_state.uppercase/trumps_by_state[\"*trump*\"]).rename(\"uppercase usage\")\n",
                "prop_over_trumps = (trumps_by_state.proper/trumps_by_state[\"*trump*\"]).rename(\"propercase usage\")\n",
                "trumps_over_pat = (trumps_by_state[\"*trump*\"]/trumps_by_state.patronage).rename(\"trumps usage\")\n",
                "trumps_by_state = trumps_by_state.join([prop_over_trumps, up_over_trumps, trumps_over_pat], how=\"outer\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "trumps_vs_trumpism = trumps_by_state.filter([\"trumpism\", \"propercase usage\", \"uppercase usage\", \"trumps usage\"]).sort_values(\"trumps usage\", ascending=True)[1:]\n",
                "\n",
                "pd.DataFrame.hist(trumps_vs_trumpism, bins=50)\n",
                "#plt.hist([prop_over_cap.trumpism, prop_over_cap[\"\"]], bins=30)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "trump_posts = usa.join(voting, on=\"state\").join(find_strs(\"trump\"), how=\"outer\")\n",
                "\n",
                "print \"Selecting states that are espectially anti-trump:\\n{0}\".format(t[t.trumpism < .4].title.sample(10))\n",
                "\n",
                "print \"\\nPolitically liberal states composing the above sampling:\\n{0}\".format(t[t.trumpism < .4].groupby(\"state\").sum().index.tolist())\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "# not really ever used\n",
                "trump_words = [\"liberals\",\n",
                "               \"conservatives\",\n",
                "               \"centipede\",\n",
                "               \"cuck\",\n",
                "               \"maga\",\n",
                "               \"regressive left\",\n",
                "               \"shillary\",\n",
                "               \"sjw\",\n",
                "               \"triggered\"]\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "def check_ascii(post):\n",
                "    \"\"\"\n",
                "    Determines whether a title is encodable as ascii\n",
                "    \"\"\"\n",
                "    try:\n",
                "        post.encode('ascii')\n",
                "        return True\n",
                "    except UnicodeError:\n",
                "        return False\n",
                "\n",
                "ascii_titles_tv = usa.title.apply(check_ascii)\n",
                "ascii_posts = usa[ascii_titles_tv]\n",
                "nonascii_posts = usa[~ascii_titles_tv]\n",
                "\n",
                "distinct_states = nonascii_posts[\"state\"].unique()\n",
                "print (\"{0:,} of {1:,} total posts were non-ascii ({2:.2f}%), confined to {3} \"\n",
                "       + \"states.\").format(len(nonascii_posts),\n",
                "                       len(usa),\n",
                "                       len(nonascii_posts)/float(len(usa)) * 100,\n",
                "                       len(distinct_states))\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "nonascii_states_count = nonascii_posts.groupby(\n",
                "    \"state\").title.nunique().sort_values(ascending=False)\n",
                "print \"\\nTop ten most popular unicode states:\"\n",
                "print nonascii_states_count[:10]\n",
                "\n",
                "pennsylvania = nonascii_posts[nonascii_posts[\"state\"] == \"Pennsylvania\"]\n",
                "print pennsylvania[\"title\"].tolist()[0]\n",
                "\n",
                "print(\"\\nA single Trump memester seems to be responsible for the chaos \" +\n",
                "      \"in Pennsylvania.\\n\" + \"I suspect that these crazy unicode posts \" +\n",
                "      \"are mostly done by a very small\\nset of people, though there is \" +\n",
                "      \"no way to tell.\")\n",
                "print \"\\nRandom sample of 5 non-ascii Pennsylvania posts\"\n",
                "print pennsylvania[\"title\"][:5]\n",
                "\n",
                "pennsylvania.groupby(\"region\").count()\n",
                "\n",
                "post_uniqueness = pennsylvania.title.nunique()/float(len(pennsylvania.title))\n",
                "\n",
                "post_uniqueness\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "print \"{0} regions in Colorado\".format(usa[usa['state'] == \"Colorado\"][\"region\"].nunique())\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "print(len(usa.groupby(\"state\")[\"title\"].agg(sum)[\"Kansas\"]))\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "  from textblob import TextBlob\n",
                "\n",
                "  def semants(text):\n",
                "      blob = TextBlob(text)\n",
                "      ss = 0\n",
                "      for sentence in blob.sentences:\n",
                "          ss += sentence.sentiment.polarity\n",
                "\n",
                "      return float(ss)/len(blob.sentences)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": 1,

            "source": [
                "total_semants = usa.join(semantics, how=\"outer\").groupby(\"state\").mean().join(voting).sort_values(\"semants\").corr()\n",
                "\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 2",
            "language": "python",
            "name": "python2"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 2
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython2",
            "version": "2.7.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
