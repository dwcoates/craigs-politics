#+TITLE: Babel Powered Jupyter Notebook
#+OPTIONS: toc:nil


# <h1 align="center"><font color="0066FF" size=110%>Simple Notebook</font></h1>


* Introduction
#+BEGIN_SRC ipython :session :file  :exports both :tangle ./politics.py
%matplotlib inline
import numpy as np
import pandas as pd
import scipy
from scipy import stats
import matplotlib as mpln
import matplotlib.pyplot as plt

import pprint as pp
import pickle
import re

pd.options.display.max_colwidth = 1000
#+END_SRC

#+RESULTS:
* Preparing Data
** Grab CL Data
#+BEGIN_SRC ipython :session :file  :exports both  :tangle ./politics.py
# read us data collected by craigcrawler 
usa_raw = pd.read_csv("data/us.csv", index_col=0)
post_count_total_raw = len(usa_raw)
post_count_by_state_raw = usa_raw.groupby("state").count()["title"]#.sort_values(ascending=False)
post_count_by_region_raw = usa_raw.groupby("region").count()["title"]#.sort_values(ascending=False)

print ("\n{0:,} total posts exctracted from {3:,} regions over {4} "+ 
       "state. The most popular\nstate was {1}, and the most " + 
       "popular region was, surprisingly, {2}.").format(post_count_total_raw,
                                                        post_count_by_state_raw.index[0],
                                                        post_count_by_region_raw.index[0],
                                                        len(post_count_by_region_raw),
                                                        len(post_count_by_state_raw))

#+END_SRC
#+RESULTS:
** U.S. Census 2010
- US census data for 2010 from the census bureau
- Census data is not labelled exactly as my data is. Some states are named a little differently,
and regions are almost never named similarly. These have to be resolved.
*** Geo Keys
#+BEGIN_SRC ipython :session :file  :exports both :tangle ./politics.py
# Keys for geography stuff. Table is an index table.
# These keys are used as index for census table.
GEO_NAME = "GEO.display-label"
GEO_KEY = "GEO.id"
state_keys = pd.read_csv("data/census/DEC_10_DP_G001_with_ann.csv")[1:].set_index(GEO_KEY)

state_keys = state_keys.filter([GEO_NAME])[:52]
state_keys = state_keys[state_keys[GEO_NAME]!= "Puerto Rico"]
#+END_SRC

#+RESULTS:

*** Census Data
#+BEGIN_SRC ipython :session :file  :exports both :tangle ./politics.py
  # keys for the census data. Only really care about two of them (there are hundreds):
  TOT_NUM_ID = "HD01_S001" # total number key
  TOT_PER_ID = "HD02_S001" # total percent key

  census = pd.read_csv("data/census/DEC_10_DP_DPDP1_with_ann.csv")[1:].set_index(GEO_KEY)

  census = census.filter([TOT_NUM_ID])
  census = census.join(state_keys, how="right")
  census.columns = ["population", "state"]
  census.set_index("state", inplace=True)
    
  def correct_stat(s):
      """
      Some states have extra information for population. 
      Example: 25145561(r48514)
      """
      loc = s.find("(")
      return int(s[:loc] if loc > 0 else s)

  census.population = census.population.apply(correct_stat)
  
  census = census.drop("District of Columbia")
#+END_SRC

#+RESULTS:

[5 rows x 375 columns]
** U.S. 2016 Election
#+BEGIN_SRC ipython :session :file  :exports both :tangle ./politics.py
  import requests
  from scrapy import Selector

  atlas_url = "http://uselectionatlas.org/RESULTS/data.php?year=2016&datatype=national&def=1&f=1&off=0&elect=0"
  atlas_source = requests.get(atlas_url).text
  select = Selector(text=atlas_source).xpath('//*[@id="datatable"]/tbody/tr')

  convert = lambda s: int(s.replace(',', ''))
  vote_names = map(str, select.xpath('td[3]/a/text()').extract())
  # Correct name for DC
  vote_names[8] = "District of Columbia"
  clinton_votes = map(convert, select.xpath('td[17]/text()').extract())
  trump_votes = map(convert, select.xpath('td[18]/text()').extract())

  gen_votes = pd.DataFrame({"clinton": clinton_votes, "trump": trump_votes}, index=vote_names)

  trump_favor = pd.DataFrame(gen_votes["trump"]/gen_votes.sum(axis=1), columns=["trumpism"], index=vote_names)  
  voting = gen_votes.join(trump_favor).sort_values("trumpism", ascending=False)  
  voting = voting.drop("District of Columbia")

  # for pretty printing
  voting_space = pd.DataFrame([["------", "------", "------"]],index=["*SPACE*"], columns=voting.columns) 
  pd.concat([voting[:5], voting_space, voting[-5:].sort_values("trumpism")])
#+END_SRC

#+RESULTS:
#+begin_example
               clinton    trump  trumpism
Wyoming          55973   174419  0.757053
West Virginia   188794   489371  0.721611
North Dakota     93758   216794  0.698092
Oklahoma        420375   949136  0.693047
Idaho           189765   409055  0.683102
*SPACE*         ------   ------    ------
Hawaii          266891   128847  0.325587
California     8753788  4483810  0.338718
Vermont         178573    95369  0.348136
Massachusetts  1995196  1090893  0.353487
Maryland       1677928   943169  0.359838
#+end_example

** Preprocess Data
Some preprocessing to check data corrupted files
#+BEGIN_SRC ipython :session :file  :exports both :tangle ./politics.py
  print "Data tests... \n\nAssertions Passed\n\n"

  # Confirm all expected regions and states present
  assert len(usa_raw["state"].unique()) == 52 # expected number of states
  assert len(usa_raw["region"].unique()) == 416  # expected number of regions
 
  # Confirm that there are no posts without regions/states. Not all CL 
  # regions have subregions, so it's okay for null subregions.
  assert len(usa_raw[usa_raw["state"].isnull()].index) == 0
  assert len(usa_raw[usa_raw["region"].isnull()].index) == 0

  # Find regions/subregions for which there are no posts
  postless_regions = usa_raw[usa_raw["title"].isnull()]  
  postless_regions_times = usa_raw[usa_raw["date"].isnull()]

  # not actually an effective test, but good enough
  assert len(postless_regions) == len(postless_regions_times)

  print(("{0:,} regions/subregions over {1} states without " + 
         "any posts.").format(len(postless_regions), postless_regions["state"].nunique()))  
#+END_SRC

#+RESULTS:

Drop unneeded data
#+BEGIN_SRC ipython :session :file  :exports both :tangle ./politics.py
# Drop empty regions.
usa = usa_raw.dropna(subset=["title", "date"], how="any", axis=0)
assert len(postless_regions) == len(usa_raw)-len(usa)

# Get rid of territories (Guam, Puerto Rico)
usa = usa[usa["state"] != "Territories"]
usa = usa[usa["state"] != "District of Columbia"]
#+END_SRC

#+RESULTS:

Confirm Census Data
#+BEGIN_SRC ipython :session :file  :exports both :tangle ./politics.py
assert set(usa.state.unique()) == set(census.index) and len(usa.state.unique() == len(census.index))

print "Census data complete"
#+END_SRC

#+RESULTS:

Confirm Election Data
#+BEGIN_SRC ipython :session :file  :exports both :tangle ./politics.py
assert set(usa.state.unique()) == set(voting.index) and len(usa.state.unique() == len(voting.index))

print "Voting data complete"
#+END_SRC
#+RESULTS:
* State Popularity
** Data
*** Grab Data
#+BEGIN_SRC ipython :session :file  :exports both :tangle ./politics.py
  patronage = pd.DataFrame(usa.groupby('state').size(), columns=["patronage"]).sort_values(
      "patronage",ascending=False)

  print "Top ten most frequented states:\n{}".format(patronage[:10])
#+END_SRC 
#+RESULTS:
State Usage table
#+BEGIN_SRC ipython :session :file  :exports both :tangle ./politics.py
    cl_by_state = patronage.join(census, how="inner")
    usage = cl_by_state.apply(
        lambda df: df["patronage"] / float(df["population"]), axis=1)

    # Weight for max = 1.000
    usage_weighted = (usage - usage.min())/(usage.max() - usage.min())
    weighted_usage = pd.DataFrame((usage_weighted),
                                   columns=["usage"])

    state_usage = pd.concat([cl_by_state, weighted_usage],
                            axis=1).sort_values("usage",
                                                ascending=False)

#+END_SRC

#+RESULTS:

Useful for displaying several splices of a dataframe as a concatenation
#+BEGIN_SRC ipython :session :file  :exports both
  state_usage_space = pd.DataFrame([["------", "------", "------"]],index=["*SPACE*"],
                                   columns=state_usage.columns)

  pd.concat([state_usage[:5], state_usage_space, state_usage[-5:].sort_values("usage")])
#+END_SRC
#+RESULTS:
#+begin_example
             patronage population popularity
Colorado          1982    5029196          1
Hawaii             445    1360301    0.83008
Montana            286     989415    0.73347
Oregon            1094    3831074   0.724589
Nevada             770    2700551   0.723491
*SPACE*         ------     ------     ------
North Dakota        19     672591  0.0716799
Vermont             18     625741  0.0729916
Kansas             106    2853118  0.0942716
Wyoming             22     563626  0.0990436
New Jersey         400    8791894   0.115444
#+end_example

** Analysis
*** Patronage

#+RESULTS:
[[file:/home/dodge/workspace/craig-politics/img/py6320LXp.png]]

#+BEGIN_SRC ipython :session :file ./img/py6320oYD.png :exports both :tangle ./politics.py
x = np.arange(len(pat))

plt.bar(x, pat.population)
#+END_SRC

#+RESULTS:
[[file:./img/py6320oYD.png]]

*** Usage
**** Distribution
#+BEGIN_SRC ipython :session :file ./img/py6320LXp.png :exports both :tangle ./politics.py
pat = state_usage.sort_values("patronage", ascending=True)
x = np.arange(len(pat))

ax = plt.subplot(111)  
ax.spines["top"].set_visible(False)  
ax.spines["right"].set_visible(False)  
    
ax.get_xaxis().tick_bottom()  
ax.get_yaxis().tick_left()  

plt.xlabel("Usage", fontsize=16)  
plt.ylabel("States", fontsize=16)      

plt.hist(states.usage
         color="#3F5D7D", bins=15)  
#+END_SRC

#+RESULTS:
[[file:/home/dodge/workspace/craig-politics/img/py6320LXp.png]]

#+BEGIN_SRC ipython :session :file ./img/py6320lr1.png :exports both :tangle ./politics.py
plt.bar(x, pat.sort_values("population").usage)

fig = plt.figure() # Create matplotlib figure

ax = fig.add_subplot(111) # Create matplotlib axes
ax2 = ax.twinx() # Create another axes that shares the same x-axis as ax.

width = 0.4

pat.population.plot(kind='bar', color='red', ax=ax, width=width, position=1)
pat.patronage.plot(kind='bar', color='blue', ax=ax2, width=width, position=0)

ax.set_ylabel('population')
ax2.set_ylabel('usage')

ax = pat.plot(kind="bar")
ax2 = ax.twinx()
for r in ax.patches[len(pat):]:
    r.set_transform(ax2.transData)
ax2.set_ylim(0, 2);



#+END_SRC
**** Normalized state usage distributions
#+BEGIN_SRC ipython :session :file ./img/py6320jfT.png :exports both
norm_usage = (state_usage - state_usage.min()) / (state_usage.max() - state_usage.min())
norm_usage.plot(kind="density", title="Normalized PDF estimations", sharey=True)
#+END_SRC

#+RESULTS:
[[file:/home/dodge/workspace/craig-politics/py6320jfT.png]]

#+BEGIN_SRC ipython :session :file ./img/py6320Yhv.png :exports both :tangle ./politics.py
plt.plot(x, state_usage.population.sort_values().values)
#+END_SRC
#+RESULTS:
[[file:/home/dodge/workspace/craig-politics/img/py6320Yhv.png]]
I expect population to relate to patronage linearly.
#+BEGIN_SRC ipython :session :file /home/dodge/workspace/craig-politics/py63201RB.png :exports both
# Getting rid of California
p1 = state_usage.sort_values("population", ascending=False)[5:]

plt.bar(p1["population"], p1["usage"])
#+END_SRC
 #+RESULTS:
 [[file:/home/dodge/workspace/craig-politics/py63201RB.png]]

*** Politics
Assign to each post their expected value for political leaning
#+BEGIN_SRC ipython :session :file ./craig-politics/py6320CcH.png :exports both
post_politics = usa.join(voting, on="state").join(find_strs("trump"), how="inner")
#+END_SRC
#+BEGIN_SRC ipython :session :file ./img/py6320PmN.png :exports both
states = state_usage.join(voting, how="left").sort_values("usage")[:50]
plt.hist([states.usage, states.trumpism], bins=30)
#+END_SRC
#+RESULTS:
[[file:./img/py6320PmN.png]]

Note the correlation between trumpism and usage
#+BEGIN_SRC ipython :session :file ./img/py6320k_K.png :exports both
print states.filter(["patronage", "usage", "normalized", "trumpism"]).corr()
#+END_SRC :tangle ./politics.py
* Text Qualities
** Data
*** Words
Most popular words in English. Grabbed from http://www.world-english.org/english500.htm
#+BEGIN_SRC ipython :session :file  :exports both
pop_english_words = ["the", "re", "a", "s", "t", "i", "of", "to", "and", "and", "in", "is", "it", "you", "that", "he", "was", "for", "on", "are", "with", "as", "I", "his", "they", "be", "at", "one", "have", "this", "from", "or", "had", "by", "hot", "but", "some", "what", "there", "we", "can", "out", "other", "were", "all", "your", "shit", "when", "up", "use", "word", "how", "said", "an", "each", "she", "which", "do", "their", "time", "if", "will", "way", "about", "many", "fuck", "then", "them", "would", "write", "like", "so", "these", "her", "long", "make", "thing", "see", "him", "two", "has", "look", "more", "day", "could", "go", "come", "did", "my", "sound", "no", "most", "number", "who", "over", "know", "water", "than", "call", "first", "people", "may", "down", "side", "been", "now", "find"]
#+END_SRC

#+RESULTS:

#+BEGIN_SRC ipython :session :file ./img/py63203mB.png :exports both
  def post_words(df, no_pop=False):
      words = re.findall(r'\w+', df.title.apply(lambda x: x + " ").sum())
      if no_pop:
          # pop_english_words is a list of the most popular (and boring) English
          # words. E.g., "and", "to", "the", etc.
          words = [word for word in words if word not in pop_english_words]
      return  words

  def words(df=usa, no_pop=False):
      # word counts across all posts
      words = post_words(df, no_pop)
      word_counts = Counter([word.lower() for word in words])
      wcs = zip(*[[word, count] for word, count in word_counts.iteritems()])

      corpus = pd.Series(wcs[1], index=wcs[0]).rename("counts")

      return corpus.sort_values(ascending=False)
#+END_SRC
Probably don't care about stupid common words 
#+BEGIN_SRC ipython :session :file ./img/py6320H0c.png :exports both
posts_corpus = words(df=usa, no_pop=True)

usa_words_full = post_words(df=usa)
usa_words = post_words(df=usa, no_pop=True)

posts_sum = " ".join([word for word in usa_words_full if word.lower() not in pop_english_words])
#+END_SRC
*** Substrings
Find substrings in posts
#+BEGIN_SRC ipython :session :file ./img/py6320WhL.png :exports both  :tangle ./politics.py
  def find_strs(substr, df=usa):
      """
      Get all titles from usa that have substr in their post title. Add some data on capitalization.
      """
      
      find = lambda s: (1 if re.search(substr, s, re.IGNORECASE) else np.nan)

      return df.title[df.title.map(find) == 1].rename("*" + substr + "*", inplace=True)

  def categ_strs(findings):
      """
      Return a list of 
      """
      s = findings.name[1:-1]
      find = lambda sub, string: (1 if re.search(sub, string) else np.nan)

      proper = findings.apply(lambda x: find(s[0].upper() + s[1:].lower(), x)).rename("proper")
      cap = findings.apply(lambda x: find(s.upper(), x)).rename("uppercase")
      low = findings.apply(lambda x: find(s.lower(), x)).rename("lower")

      return pd.concat([proper, cap, low], axis=1)

  def eval_strs(string, df=usa):
      findings = find_strs(string, df)
      return categ_strs(findings).join(findings)


#+END_SRC
** Analysis
*** General Language
#+BEGIN_SRC ipython :session :file  :exports both
  lib_words = words(df=post_politics[post_politics.trumpism < .45], no_pop=True).rename("libs")
  conserv_words = words(df=post_politics[post_politics.trumpism > .55], no_pop=True).rename("conservs")  
#+end_src
Words distribution
#+BEGIN_SRC ipython :session :file  :exports both
# number of words
# percentage distinct
usa_words

#by demographic
#+END_SRC
Ratio
#+BEGIN_SRC ipython :session :file  :exports both   
  rat = lambda df: df.libs/df.conservs
  ratio = pd.DataFrame().join([lib_words[lib_words >= 10], conserv_words[conserv_words >= 10]],
                                      how="outer").apply(rat, axis=1).dropna()
  ratio = ratio.rename("dem/rep ratio")
  lib_con_ratio = pd.DataFrame(posts_corpus).join(ratio.sort_values(ascending=False), how="inner")
  lib_con_ratio.sort("dem/rep ratio", ascending=False, inplace=True)
  lib_con_ratio[:10]
  #lib_con_ratio = posts_corpus.join(lib_con_ratio.sort_values(ascending=False), on="words")
#+END_SRC

#+RESULTS:
#+begin_example
           counts  dem/rep ratio
against       346       5.000000
won           320       4.461538
sign          262       3.363636
voted         223       2.375000
not           993       2.000000
get           480       1.615385
trump        5071       1.366133
america       784       1.363636
appoints       37       1.083333
president     654       1.075758
#+end_example

#+BEGIN_SRC ipython :session :file ./img/py6320I8X.png :exports both
l
#+END_SRC

#+RESULTS:

*** Trumps
**** Patronage
#+BEGIN_SRC ipython :session :file ./img/py6320Qlq.png :exports both :tangle ./politics.py
trumps = eval_strs("trump").join(usa.state, how="inner")
trumps_by_state = trumps.groupby("state").count().join(states).drop(["clinton", "trump"], axis=1)
up_over_trumps = (trumps_by_state.uppercase/trumps_by_state["*trump*"]).rename("uppercase usage")
prop_over_trumps = (trumps_by_state.proper/trumps_by_state["*trump*"]).rename("propercase usage")
trumps_over_pat = (trumps_by_state["*trump*"]/trumps_by_state.patronage).rename("trumps usage")
trumps_by_state = trumps_by_state.join([prop_over_trumps, up_over_trumps, trumps_over_pat], how="outer")
#+END_SRC
**** Politics
The more pro-Trump your state, the less likely you are to use "Trump" over "TRUMP"
#+BEGIN_SRC ipython :session :file ./img/py6320cup.png :exports both :tangle ./politics.py
trumps_vs_trumpism = trumps_by_state.filter(["trumpism", "propercase usage", "uppercase usage", "trumps usage"]).sort_values("trumps usage", ascending=True)[1:]

pd.DataFrame.hist(trumps_vs_trumpism, bins=50)
#plt.hist([prop_over_cap.trumpism, prop_over_cap[""]], bins=30)
#+END_SRC

#+RESULTS:
[[file:/home/dodge/workspace/craig-politics/img/py6320cup.png]]

#+BEGIN_SRC ipython :session :file ./img/py6320U3u.png :exports both
trump_posts = usa.join(voting, on="state").join(find_strs("trump"), how="outer")

print "Selecting states that are espectially anti-trump:\n{0}".format(t[t.trumpism < .4].title.sample(10))

print "\nPolitically liberal states composing the above sampling:\n{0}".format(t[t.trumpism < .4].groupby("state").sum().index.tolist())
#+END_SRC
**** Trump Language
#+BEGIN_SRC ipython :session :file ./img/py63202C2.png :exports both :tangle ./politics.py
trump_words = ["liberals",
               "conservatives",
               "centipede",
               "cuck",
               "maga",
               "regressive left",
               "shillary",
               "sjw",
               "triggered"]

#+END_SRC
**** word cloud
#+BEGIN_SRC ipython :session :file ./img/py6320RCC.png :exports both
from os import path
from PIL import Image

from wordcloud import WordCloud, STOPWORDS

d = path.dirname("/home/dodge/workspace/craig-politics/")

trump_mask = np.array(Image.open(path.join(d, "Trump_silhouette.png")))

stopwords = set(STOPWORDS)

wc = WordCloud(background_color="white", max_words=2000, mask=alice_mask,
               stopwords=stopwords)


# generate word cloud
wc.generate(posts_sum)

# save to file
wc.to_file(path.join(d, "Trump_test.png"))

# show
plt.imshow(wc)
plt.axis("off")
plt.figure()
plt.imshow(alice_mask, cmap=plt.cm.gray)
plt.axis("off")
plt.show()
#+END_SRC

#+RESULTS:
[[file:/home/dodge/workspace/craig-politics/py6320RCC.png]]

*** Unicode
ascii vs. unicode usage. 
#+BEGIN_SRC ipython :session :file  :exports both :tangle ./politics.py
def check_ascii(post):
    """
    Determines whether a title is encodable as ascii
    """
    try:
        post.encode('ascii')
        return True
    except UnicodeError:
        return False

ascii_titles_tv = usa.title.apply(check_ascii)
ascii_posts = usa[ascii_titles_tv]
nonascii_posts = usa[~ascii_titles_tv]

distinct_states = nonascii_posts["state"].unique()
print ("{0:,} of {1:,} total posts were non-ascii ({2:.2f}%), confined to {3} "
       + "states.").format(len(nonascii_posts),
                       len(usa),
                       len(nonascii_posts)/float(len(usa)) * 100,
                       len(distinct_states))
#+END_SRC

#+RESULTS:

**** Pennsylvania
Pennsylvania has was the preeminent outlier in non-ascii usage per-state
#+BEGIN_SRC ipython :session :file  :exports both :tangle ./politics.py
nonascii_states_count = nonascii_posts.groupby(
    "state").title.nunique().sort_values(ascending=False)
print "\nTop ten most popular unicode states:"
print nonascii_states_count[:10]

pennsylvania = nonascii_posts[nonascii_posts["state"] == "Pennsylvania"]
print pennsylvania["title"].tolist()[0]

print("\nA single Trump memester seems to be responsible for the chaos " +
      "in Pennsylvania.\n" + "I suspect that these crazy unicode posts " +
      "are mostly done by a very small\nset of people, though there is " +
      "no way to tell.")
print "\nRandom sample of 5 non-ascii Pennsylvania posts"
print pennsylvania["title"][:5]

pennsylvania.groupby("region").count()

post_uniqueness = pennsylvania.title.nunique()/float(len(pennsylvania.title))
#+END_SRC

#+RESULTS:
=                   title  date  state  subregion
region                                          
harrisburg, PA        11    11     11          0
lancaster, PA         11    11     11          0
philadelphia           1     1      1          0
pittsburgh, PA         1     1      1          0
reading, PA           10    10     10          0
state college, PA     11    11     11          0
york, PA              11    11     11          0
==<pandas.core.groupby.DataFrameGroupBy object at 0x7fa5c0d57250>
==<pandas.core.groupby.DataFrameGroupBy object at 0x7fa5f43f5050>
==Series([], dtype: int64)
==Empty DataFrame
Columns: [title, date, state, region]
Index: []
=* 
***** Colorado
#+BEGIN_SRC ipython :session :file  :exports both :tangle ./politics.py
print "\n\n{0} regions in Colorado".format(usa[usa['state'] == "Colorado"]["region"].nunique())
#+END_SRC

#+RESULTS:


#+BEGIN_SRC ipython :session :file ./img/py6320XN2.png :exports both :tangle ./politics.py
posts = usa.groupby("state")["title"].agg(sum)["Kansas"]
#+END_SRC
      
*** Semantics
#+BEGIN_SRC ipython :session :file ./img/py63201WL.png :exports both :tangle ./politics.py
  from textblob import TextBlob

  def semants(text):
      blob = TextBlob(text)
      ss = 0
      for sentence in blob.sentences:
          ss += sentence.sentiment.polarity

      return float(ss)/len(blob.sentences)
#+END_SRC
#+BEGIN_SRC ipython :session :file ./img/py63202Qe.png :exports both :tangle ./politics.py
semantics = ascii_posts.title.map(lambda x: semants(x)).rename("semants")
semant = eval_strs("trump", df=ascii_posts).join(pd.DataFrame(semantics))
sems_usa = semant.join(usa, how="inner")
trumps_semantics = sems_usa.groupby("state").mean().join(voting, how="inner").sort_values("semants").corr()
#+END_SRC
#+BEGIN_SRC ipython :session :file ./img/py6320Dbk.png :exports both :tangle ./politics.py
total_semants = usa.join(semantics, how="outer").groupby("state").mean().join(voting).sort_values("semants").corr()
#+END_SRC

* historgrams
#+BEGIN_SRC ipython :session :file ./img/py6320JLH.png :exports both
  pat = state_usage.sort_values("patronage", ascending=True)
  x = np.arange(len(pat))
    
    
  ax = plt.subplot(111)  
  ax.spines["top"].set_visible(False)  
  ax.spines["right"].set_visible(False)  
    
  ax.get_xaxis().tick_bottom()  
  ax.get_yaxis().tick_left()  
    
  plt.xticks(fontsize=14)  
  plt.yticks(range(5000, 30001, 5000), fontsize=14)  
    
  plt.xlabel("Patronage", fontsize=16)  
  plt.ylabel("Count", fontsize=16)  
    
  plt.text(1300, -5000, "Data source: www.ChessGames.com | "  
           "Author: Randy Olson (randalolson.com / @randal_olson)", fontsize=10)  
    
  # # Finally, save the figure as a PNG.  
  # # You can also save it as a PDF, JPEG, etc.  
  # # Just change the file extension in this call.  
  # # bbox_inches="tight" removes all the extra whitespace on the edges of your plot.  
#  plt.savefig("chess-elo-rating-distribution.png", bbox_inches="tight");  

  plt.hist(states.usage,  
           color="#3F5D7D", bins=100)  
#+END_SRC
