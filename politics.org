#+HTML_HEAD: <link href="/home/dodge/.emacs.d/leuven-theme.css" rel="stylesheet">
#+TITLE: *@@html:<font color = "C2492F">@@Scraping the Bottom of the Barrel@@html:</font>@@*

#+OPTIONS: toc:2 num:nil
#+TABLFM: $0;%0.3f


# <h1 align="center"><font color="0066FF" size=110%>Simple Notebook</font></h1>

* TODO stuff todo [9/14]                                           :noexport:
** DONE Corpus is broken. Including non-pop words
** DONE Make thesis more clear

** DONE Stop using the word "generally"
** DONE Consider hiding code for diagrams. It isnt interesting.
** TODO Make sure diagrams are properly detailed [0/1]
*** TODO The correlation diagram needs to say describe color value

** DONE Add a sample of the data for the introduction

** DONE Find next highest number of words equal to trump instances
** DONE Add small description of scraping process with sample code
** DONE Fix how D.C. is removed
in voting, and in preprocessing, and in census
** TODO Add sources for Denver/NYC population stuff
- how to do this?
** DONE Population vs Patronage graph
- should be a scatter plot, where the color of the dots is a greyscale of usage.
- That or a 2d histogram
** TODO Demonstrate trumpism by population vs trumpism by posts
- basically demonstrates liberal usage of craigslist politics
** TODO lib words vs conserv words needs a revamp
- see "THIS IS BROKEN AND BAD"
** TODO How can I weight the dems for trumpism distribution?
dems show up more in posts, but like, there are more of them. Wait,
not there aren't. They're about half of the country, right? Why am I
weighting again? Maybe just for good measure, but really, I can get
away with only a couple of points between them
** TODO Correlation matrix vis is broken?!
* Setup Code :noexport:

  General settings, packages and functions.

#+BEGIN_SRC ipython :session :exports results :results none :tangle ./politics.py
  %matplotlib inline
  import numpy as np
  import scipy
  from scipy import stats
  import matplotlib as mpln
  import matplotlib.pyplot as plt
  import matplotlib.cm as cm
  import pandas as pd

  from tabulate import tabulate

  import pprint as pp
  import pickle
  import re

  pd.options.display.max_colwidth = 1000

  def print_df(df, headers="keys", rnd=100, dis_parse=False):
      """
      Pretty print DataFrame in an org table. Org tables are good.
      They also export nicely.
      """
      print(tabulate(df.round(rnd),
                     tablefmt="orgtbl",
                     headers=headers,
                     disable_numparse=dis_parse))
#+END_SRC

* Introduction

  For my web scraping project, I've chosen to extract some of the political data
  from craigslist.org. My original ambition, though it proved difficult to
  affirm, was to show a small, non-existant, or negative correllation of
  pro-Trump chatter to expected conservatism. I suspected that in the /politics/
  section of Craigslist, more conservative states would be disproportionately less
  likely sources of pro-Trump posts.

  My basis for this suspicion was my general observation that less regulated
  areas for discussion on the Internet tend to be very attractive to those
  members of a socially disparaged minority. Recognizing that Trump supporters
  in largely pro-Clinton geographic areas are disparaged for their support in
  amounts disproportionate to their high representation, it followed that I
  could expect some amount of pro-Trump (mostly trollish) chatter in mostly
  liberal places (e.g., New York City). The positive sentiment proved to be
  difficult to convincingly affirm. 

  More generally, I sought to analyze the trends of discussion on Craiglist,
  mostly in areas of text usage (capitalization, word frequency, etc.) vs
  political leaning.

:RESULTS:
[[./img/Trump_cloud_proper.png]]
:END:

* Methodology

  To extract data from Craigslist, I used the Python ~scrapy~ package, which was
  probably overkill. Originally, I intended to collect post-bodies as well as the
  post-titles, however this would require about 100 times as many request, too many for
  me to reponsibly execute in a reasonable amount of time. Therefore, I limited
  the extraction to titles, which involved about 500 requests, spread over five hours,
  to obtain roughly 40,000 post titles/times. 

  For each of these titles, there is a corresponding state and region, with some
  regions additionally divided into subregions (the New York City region, for
  example, consists of Brooklyn, Queens, Manhattan, etc). Each post, its time
  and its geographical origin, are represented with a single row in a 40k row
  Pandas ~DataFrame~ (DF), ~usa~. 

  Data corruption was not an issue, as the CL layout is quite uniform, though I
  did need to take into account data redundancy (e.g., occasionally "regions"
  are also "subregions" of sibling regions), and was able to handle this by
  ensuring links were travelled to only once. To make use of the extracted post
  title data, I employed the 2010 U.S. census, which is available from
  http://www.census.gov, as well as the 2016 election results data, which I
  scraped from http://uselectionatlas.org/ using a BeautifulSoup extraction
  script.

* Preparing data
** TODO Craigcrawler :ignore:

   The [[https://github.com/dwcoates/craigs-politics/tree/master/craigcrawler][craigslist extractor]], written as a scrapy program, collected titles and
   dates, as well as the corresponding geographical designations. To keep it
   simple, these are all stored together as rows in a csv file.

** Grab CL Data  :ignore:

   Reading this data from craigcrawler's file:

#+BEGIN_SRC ipython :session :exports none :tangle ./politics.py
usa_raw = pd.read_csv("data/us.csv", index_col=0)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC ipython :session :file :exports none  :tangle ./politics.py
post_count_total_raw = len(usa_raw)
post_count_by_state_raw = usa_raw.groupby("state").count()["title"].sort_values(ascending=False)
post_count_by_region_raw = usa_raw.groupby("region").count()["title"].sort_values(ascending=False)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC ipython :session :file  :results output org :noweb yes :exports results  :tangle ./politics.py
  print ("{0:,} total posts exctracted from {1:} regions over {2} "+
         "states. The most \nfrequented state was '{3}', and the most " +
         "frequented region was,\nsurprisingly, '{4}'.").format(post_count_total_raw,                                                          
                                                               len(post_count_by_region_raw),
                                                               len(post_count_by_state_raw),
                                                               post_count_by_state_raw.index[0],
                                                               post_count_by_region_raw.index[0],)
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
80,222 total posts exctracted from 416 regions over 52 states. The most 
frequented state was 'Florida', and the most frequented region was,
surprisingly, 'south florida'.
#+END_SRC

*** ~usa~ Sample

    A sample of posts in the ~usa~ ~DataFrame~ before pre-processing, which is the DF for
    storing all CL politics posts:

#+BEGIN_SRC ipython :session :exports results :results output raw drawer :noweb yes :cache yes :tangle ./politics.py
# This can fail because tabulate can't handle unicode.
# There's only about a 2.5% chance if fails on a given execution, though.
print_df(usa_raw.sample(3), rnd=3)
#+END_SRC

#+RESULTS[b5ed096f86f54cc3f99d59f9291ac54746dc56d1]:
:RESULTS:
|       | title                 | date             | state    | region              | subregion |
|-------+-----------------------+------------------+----------+---------------------+-----------|
|   535 | Trump                 | 2016-12-22 23:06 | Delaware | delaware            |       nan |
| 37362 | Rubio Was Right       | 2016-12-20 22:06 | Texas    | dallas / fort worth | north DFW |
| 65107 | LOST DOG - reward!*** | 2017-01-23 23:12 | Kentucky | lexington, KY       |       nan |
:END:

** U.S. Census 2010
*** Geo Keys   :noexport:

#+BEGIN_SRC ipython :session :exports none :tangle ./politics.py :results none
# Keys for geography stuff. Table is an index table.
# These keys are used as index for census table.
GEO_NAME = "GEO.display-label"
GEO_KEY = "GEO.id"

state_keys = pd.read_csv("data/census/DEC_10_DP_G001_with_ann.csv")[1:].set_index(GEO_KEY)

state_keys = state_keys.filter([GEO_NAME])[:52]
state_keys = state_keys[state_keys[GEO_NAME]!= "Puerto Rico"]
#+END_SRC

*** Census Data

#+BEGIN_SRC ipython :session :exports none :tangle ./politics.py :results none
  # keys for the census data. Only really care about two of them (there are hundreds):
  TOT_NUM_ID = "HD01_S001" # total number key
  TOT_PER_ID = "HD02_S001" # total percent key
#+end_src

#+begin_src ipython :session :exports none :tangle ./politics.py :results none
  cd_file = "data/census/DEC_10_DP_DPDP1_with_ann.csv"
  census_all = pd.read_csv(cd_file)[1:].set_index(GEO_KEY)
#+end_src

#+begin_src ipython :session  :exports none :tangle ./politics.py
  census_states = census_all.filter([TOT_NUM_ID]).join(state_keys, how="right")
  census_states.columns = ["population", "state"]
  census_states.set_index("state", inplace=True)

  def correct_stat(s):
      """
      Some states have extra information for population.
      Example: 25145561(r48514), should be 25145561.
      """
      loc = s.find("(")
      return int(s[:loc] if loc > 0 else s)

  census_states.population = census_states.population.apply(correct_stat)

  census = census_states.drop("District of Columbia")
#+end_src

#+RESULTS:

Census data is collected from U.S. Census Bureau for [[http://www.census.gov/2010census/][2010 census]]. This will be
used mostly to guage the CL usage of individual states, which can then be
compared against political orientation, etc. Here's a sample:
#+begin_src ipython :session :results output raw drawer :noweb yes :exports results :tangle ./politics.py
print_df(census.sample(4), rnd=3)
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
| state          |  population |
|----------------+-------------|
| North Carolina | 9.53548e+06 |
| Washington     | 6.72454e+06 |
| Hawaii         |  1.3603e+06 |
| North Dakota   |      672591 |
#+END_SRC
** U.S. 2016 Election

   The 2016 Election results will be useful for comparing expected political
   orientation to CL usage trends. They are grabbed from a really nice site,
   [[http://uselectionatlas.org/RESULTS/data.php?year%3D2016&datatype%3Dnational&def%3D1&f%3D1&off%3D0&elect%3D0][uselectionsatlas.org]] using the following script:

#+BEGIN_SRC ipython :session :exports code :tangle ./politics.py
  import requests
  from scrapy import Selector

  atlas_url = ("http://uselectionatlas.org/RESULTS/data.php?year" +
               "=2016&datatype=national&def=1&f=1&off=0&elect=0")
  atlas_source = requests.get(atlas_url).text
  select = Selector(text=atlas_source).xpath('//*[@id="datatable"]/tbody/tr')

  convert = lambda s: int(s.replace(',', ''))
  vote_names = map(str, select.xpath('td[3]/a/text()').extract())
  # Correct name for DC
  vote_names[8] = "District of Columbia"
  clinton_votes = map(convert, select.xpath('td[17]/text()').extract())
  trump_votes = map(convert, select.xpath('td[18]/text()').extract())

  gen_votes = pd.DataFrame({"clinton": clinton_votes, "trump": trump_votes},
                           index=vote_names)

  # Dub a states Rebublican vote rate "trumpism"
  trump_favor = pd.DataFrame(gen_votes["trump"]/gen_votes.sum(axis=1),
                             columns=["trumpism"],
                             index=vote_names)
  voting = gen_votes.join(trump_favor).sort_values("trumpism", ascending=False)
  voting = voting.drop("District of Columbia")
#+end_src

#+RESULTS:

   Sample of ~voting~ DataFrame:

#+begin_src ipython :session :results output raw drawer :noweb yes :exports results :tangle ./politics.py
  # for pretty printing
  voting_space = pd.DataFrame([["------", "------", "------"]],index=["*SPACE*"],
                              columns=voting.columns)
  print_df(pd.concat([voting[:3].round(3), voting_space, voting[-3:].round(3).sort_values("trumpism")]),
           rnd=3)
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
|               | clinton |   trump | trumpism |
|---------------+---------+---------+----------|
| Wyoming       |   55973 |  174419 |    0.757 |
| West Virginia |  188794 |  489371 |    0.722 |
| North Dakota  |   93758 |  216794 |    0.698 |
| *SPACE*         |  ------ |  ------ |   ------ |
| Hawaii        |  266891 |  128847 |    0.326 |
| California    | 8753788 | 4483810 |    0.339 |
| Vermont       |  178573 |   95369 |    0.348 |
#+END_SRC

** Preprocess Data

   A small bit of preprocessing to check data for corruption and unexpected results
   was necessary. There was no missing data, and no corruption. I suspected that I
   might encounter some amount of redundancy, but the extractor was written to
   exclude duplicated links, and it happened to be the case that CL keys areas
   uniquely across highly related (sub)regions. For example, the "long island"
   /region/ and "long island, NY" /subregion/ (subregion of "new york city" region)
   seem like they might be the same, but are actually completely distinct.

#+BEGIN_SRC ipython :session :exports none :tangle ./politics.py
  print "Data tests... \n\nAssertions Passed\n\n"

  # Confirm all expected regions and states present
  assert len(usa_raw["state"].unique()) == 52 # expected number of states (D.C., Territories)
  assert len(usa_raw["region"].unique()) == 416  # expected number of regions

  # Confirm that there are no posts without regions/states. Not all CL
  # regions have subregions, so it's okay for null subregions.
  assert len(usa_raw[usa_raw["state"].isnull()].index) == 0
  assert len(usa_raw[usa_raw["region"].isnull()].index) == 0

  # Find regions/subregions for which there are no posts
  postless_regions = usa_raw[usa_raw["title"].isnull()]
  postless_regions_times = usa_raw[usa_raw["date"].isnull()]

  # Not actually a good test, but good enough
  assert len(postless_regions) == len(postless_regions_times)
#+end_src

#+RESULTS:

#+begin_src ipython :session :results output raw org :noweb yes :exports none :tangle ./politics.py
  print(("{0:,} regions/subregions over {1} states without " +
         "any posts.").format(len(postless_regions), postless_regions["state"].nunique()))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
68 regions/subregions over 35 states without any posts.
#+END_SRC

#+BEGIN_SRC ipython :session  :exports code :tangle ./politics.py
# Drop empty regions. Some regions are too small to have any posts.
usa = usa_raw.dropna(subset=["title", "date"], how="any", axis=0)
assert len(postless_regions) == len(usa_raw)-len(usa)

# Get rid of territories (Guam, Puerto Rico).
usa = usa[usa["state"] != "Territories"]
# Get rid of "District of Columbia"
usa = usa[usa["state"] != "District of Columbia"]
#+END_SRC
#+RESULTS:

#+BEGIN_SRC ipython :session  :exports none :tangle ./politics.py
# Confirm census data
assert set(usa.state.unique()) == set(census.index) and len(usa.state.unique() == len(census.index))

print "Census data complete"
#+end_src

#+RESULTS:

#+BEGIN_SRC ipython :session :exports none :tangle ./politics.py
# Confirm election data
assert set(usa.state.unique()) == set(voting.index) and len(usa.state.unique() == len(voting.index))

print "Voting data complete"
#+end_src

#+RESULTS:

* State Usage
** intro :ignore:

   Although the post data has attached a fairly fine-grain geographical
   description, I found the CL regions in general to not line up well with any
   census bureau categories. Moreover, even in the lucky event of such name
   correspondence, the division of regions was at least questionable. For example,
   by far the dataset's most prominent "state" outliers, District of Columbia, has
   a census population of about 600k, yet a practical metropolitan area population
   in the several millions, a disparity that grossly skews its contributions to
   state-wide political statistics. For this reason, regions and subregions were
   largely found to be unmanageably tedious to consider seriously in any
   analysis. States, however, having relatively little variation between practical
   occupancy and census population, and having indisputable borders, barring District
   of Columbia, are ideal for inspection.

** Terms
   1. *Patronage*
      Patronage is the raw number of posts on a politics board.
   2. *Usage*
      Usage is my measure for a states proportional interest in the
      politics board. It is simply the normalized ratio of patronage and
      state population.
   3. *Trumpism*
      Trumpism is the name for a state's Republican vote percentage in the
      General Election. It is used as a rough measure of how pro-Trump
      a given stat is, and is a column in the ~voting~ DataFrame,
      which is comprised of scraped data on the 2016 General Election
      results.

** Organize Data :ignore:

#+BEGIN_SRC ipython :session :results output raw drawer :noweb yes :exports none :tangle ./politics.py
  patronage = pd.DataFrame(usa.groupby('state').size(), columns=["patronage"]).sort_values(
      "patronage",ascending=False)

  print("Top ten most frequented states:\n")
  print_df(patronage[:10])
#+END_SRC

#+RESULTS:
:RESULTS:
Top ten most frequented states:

| state        |   patronage |
|--------------+-------------|
| Florida      |        7728 |
| California   |        7521 |
| Texas        |        6401 |
| New York     |        4713 |
| Pennsylvania |        3902 |
| Colorado     |        3425 |
| Arizona      |        2909 |
| Ohio         |        2857 |
| Washington   |        2711 |
| Oregon       |        2590 |
:END:

The ~state_usage~ table is the census table concatenated with patronage usage.

#+BEGIN_SRC ipython :session :exports none :tangle ./politics.py
  cl_by_state = patronage.join(census, how="inner")
  usage = cl_by_state.apply(
      lambda df: df["patronage"] / float(df["population"]), axis=1)

  # Weight for max = 1.000
  usage_weighted = (usage - usage.min())/(usage.max() - usage.min())
  weighted_usage = pd.DataFrame((usage_weighted),
                                 columns=["usage"])
  state_usage = pd.concat([cl_by_state, weighted_usage],
                          axis=1).sort_values("usage",
                                              ascending=False)
#+end_src

#+RESULTS:

#+BEGIN_SRC ipython :session :results output raw drawer :noweb yes :exports none :tangle ./politics.py
  # Just some printing

  # Useful for displaying several splices of a dataframe as a concatenation
  state_usage_space = pd.DataFrame([["------", "------", "------"]],index=["*SPACE*"],
                                   columns=state_usage.columns)

  print_df(state_usage.sample(3))
#+END_SRC
#+RESULTS:
#+BEGIN_SRC org
| state   | patronage |  population |    usage |
|---------+-----------+-------------+----------|
| Maine   |       200 | 1.32836e+06 | 0.135327 |
| Arizona |      2909 | 6.39202e+06 | 0.563611 |
| Ohio    |      2857 | 1.15365e+07 | 0.271865 |
#+END_SRC

*** ~states~ Sample

Joining ~state_usage~ with ~voting~ gives us a decent top-down view of state
political tendencies on CL:
#+BEGIN_SRC ipython :session :exports code :tangle ./politics.py
  states = state_usage.join(voting, how="left").sort_values("usage")
#+END_SRC

#+RESULTS:



#+BEGIN_SRC ipython :session :exports results :results output raw drawer :noweb yes :tangle ./politics.py
  print(tabulate(states.sample(3), tablefmt="orgtbl", headers="keys"))
#+END_SRC
#+RESULTS:
#+BEGIN_SRC org
| state      | patronage |  population |    usage |     clinton |       trump | trumpism |
|------------+-----------+-------------+----------+-------------+-------------+----------|
| Louisiana  |       666 | 4.53337e+06 | 0.130192 |      780154 | 1.17864e+06 | 0.601717 |
| Washington |      2711 | 6.72454e+06 | 0.490553 | 1.74272e+06 | 1.22175e+06 | 0.412131 |
| Indiana    |      1178 |  6.4838e+06 | 0.179095 | 1.03313e+06 | 1.55729e+06 | 0.601173 |
#+END_SRC
** Outliers

   There are two major outlying states in the dataset: /Colorodo/ and
   /District of Columbia/.

*** Colorado

    We can see from the following that Colorado is an extreme outlier,
    being the fifth most popular state, yet the 23rd most populous.

#+BEGIN_SRC ipython :session :file ./img/py6320WCb.png :exports results :tangle ./politics.py
top_five = state_usage.sort_values("patronage")[-5:][::-1]
fig = plt.figure() # Create matplotlib figure

ax = fig.add_subplot(111) # Create matplotlib axes
ax2 = ax.twinx() # Create another axes that shares the same x-axis as ax.

width = 0.2

top_five.patronage.plot(kind='bar', color='#992255', ax=ax, width=width, position=1)
top_five.population.plot(kind='bar', color='#CC7733', ax=ax2, width=width, position=0)

ax.set_ylabel('Patronage')
ax2.set_ylabel('Population')

plt.show()
#+END_SRC

#+RESULTS:
:RESULTS:
[[file:./img/py6320WCb.png]]
:END:

   With the normalized population/patronage ratio depicted above, we derive the
   /usage/ metric, for which the median is 0.203, and for which the state with the
   next highest popularity, Hawaii, is rated 0.816.

   Usage in the Denver region is also especially large. Despite having a population
   of 650,000 people (and a metropolitcan area of three million), Denver sees a
   large patronage:

#+BEGIN_SRC ipython :session :results output raw org :noweb yes :exports results :tangle ./politics.py
print("Patronage of Denver, Colorado: {}".format(len(usa[usa.region == "denver, CO"])))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
Patronage of Denver, Colorado: 1988
#+END_SRC

   For the reasons mentioned before, deriving state usage measurements for regions
   and subregions is too difficult to bother with. However, we can get a feeling
   for this anomoly by comparing it to another region, "new york city". The "new
   york city" region, which is expansive enough as to include metropolitan areas
   like "new jersey", "long island", "fairfield", etc, has /significantly/ /fewer/
   posts for the week of data extracted, at 1006 posts:

#+BEGIN_SRC ipython :session :noweb yes :exports code :results none :tangle ./politics.py
  # From census bureau, to the nearest 1000 people
  pop_denver_proper = 649000.0 
  pop_denver_metro = 2814000.0
  pop_nyc_proper = 8550000.0  
  pop_nyc_metro = 20200000.0

  # Enumerate the NYC subregions. More than you might think.
  nyc_subregions = usa.groupby("region").get_group(
      "new york city").subregion.unique().tolist()
  num_nyc_posts = len(usa[usa.region == "new york city"])
  num_denver_posts = len(usa[usa.region == "denver, CO"])

  den_nyc_rat_prop =  (num_denver_posts/pop_denver_proper) /     \
                      (num_nyc_posts/pop_nyc_proper)

  den_nyc_rat_metro =  (num_denver_posts/pop_denver_metro)/     \
                       (num_nyc_posts/pop_nyc_metro)
#+END_SRC

#+BEGIN_SRC ipython :session :results output org :noweb yes :exports results :tangle ./politics.py
  print(("{0} posts in NYC spread over:\n{1}" + 
        ",\nand {2}.").format(num_nyc_posts, 
                              ',\n'.join('{}'.format(r) for r in nyc_subregions[:-1]), 
                              nyc_subregions[-1]))
  print(("\nConsidering city propers, we can say that Denver has ~{0:.1f}x the usage rate\nof " +
           "New York City. Adjusting for census estimates for metropolitan areas, it\nwould " + 
           "seem that Denver's usage is ~{1:.1f}x that of NYC's.").format(den_nyc_rat_prop, 
                                                                          den_nyc_rat_metro))
#+END_SRC
#+RESULTS:
#+BEGIN_SRC org
2016 posts in NYC spread over:
manhattan,
brooklyn,
queens,
bronx,
staten island,
new jersey,
long island,
westchester,
and fairfield.

Considering city propers, we can say that Denver has ~13.0x the usage rate
of New York City. Adjusting for census estimates for metropolitan areas, it
would seem that Denver's usage is ~7.1x that of NYC's.
#+END_SRC

   This is a remarkably popular region, clearly. I suspect that this extreme usage
   rate has to do with the state granularity CL assigned to the state of
   Colorado. They might want to consider providing more regions. However, we also
   see that the usage of the Denver metropolitan area is proportionally less
   extreme compared to NYC's metropolitan area usage, which might cast some doubt
   on how much Denver needs more division among it's subregions. Suffice it to say,
   Denver is wildly popular for CL politics.

*** District of Columbia

    While I found Colorado to be an inexplicable anamoly, it was also justifiably
    accurate. District of Columbia, having an incredibly low Republican voting rate
    of ~4%, and the usage similar to Colorado's, coupled with it's unclear
    geographic distinction and population, meant its results were too extreme and
    variable to consider in analysis. Besides, it's not even a real state...

** Patronage

#+BEGIN_SRC ipython :session :exports none :results none :tangle ./politics.py
# The range of fifty states (one to fifty, duh)
x = np.arange(len(state_usage))
#+end_src

#+begin_src ipython :session :file ./img/py6320oYD.png :exports results :tangle ./politics.py
ax = plt.subplot(111)
ax.spines["top"].set_visible(False)
ax.spines["right"].set_visible(False)

ax.get_xaxis().tick_bottom()
ax.get_yaxis().tick_left()

plt.xlabel("States", fontsize=12)
plt.ylabel("Patronage", fontsize=12)

plt.suptitle('Patronage by state in order of population', fontsize=14)

plt.bar(x, state_usage.sort("population").patronage, color="#550000")
#+END_SRC

#+RESULTS:
[[file:./img/py6320oYD.png]]

   We can get a feel for the usage distribution by taking a look at the
   following sample from the ~state_usage~ table:

#+BEGIN_SRC ipython :session :results output raw drawer :noweb yes :exports results :tangle ./politics.py
  print_df(pd.concat([state_usage[:5].round(3),
                       state_usage_space,
                       state_usage[-5:].sort_values("usage").round(3)]))
#+END_SRC

#+RESULTS:
:RESULTS:
|              | patronage | population |  usage |
|--------------+-----------+------------+--------|
| Nevada       |      2067 |    2700551 |    1.0 |
| Colorado     |      3425 |    5029196 |  0.881 |
| Oregon       |      2590 |    3831074 |  0.874 |
| Hawaii       |       756 |    1360301 |  0.705 |
| Montana      |       470 |     989415 |  0.592 |
| *SPACE*        |    ------ |     ------ | ------ |
| Vermont      |        34 |     625741 |    0.0 |
| South Dakota |        71 |     814180 |  0.046 |
| North Dakota |        60 |     672591 |  0.049 |
| New Jersey   |       800 |    8791894 |  0.052 |
| Wyoming      |        52 |     563626 |  0.053 |
:END:

   Seemingly some correlation between low population and low usage is
   evident. However, the states for which the politics board is most popular are
   also fairly small. It may be that the popularity doesn't relate to state
   size, directly, but to political orientation, which itself correlates with
   state population (states are smaller in Middle America). I suspect that
   political discussion is most charged currently in Democratic states, where
   discenting opinion is that which is held by the triumphant party. It may also
   be that the board popularity relationship to patronage is non-linear. This
   correlation is explored more by some political investigation.

** Usage
#+BEGIN_SRC ipython :session :file ./img/py6320LXp.png :exports results :tangle ./politics.py 
ax = plt.subplot(111)
ax.spines["top"].set_visible(False)
ax.spines["right"].set_visible(False)

ax.get_xaxis().tick_bottom()
ax.get_yaxis().tick_left()

plt.xlabel("Usage", fontsize=12)
plt.ylabel("States", fontsize=12)

plt.suptitle('Usage Distribution for CL politics board', fontsize=14)

plt.hist(state_usage.usage,
         color="#661111", bins=17)
#+END_SRC

#+RESULTS:
[[file:./img/py6320LXp.png]]

   These are the PDF estimations for normalized patronage, population, usage. They
   are estimations, so they extend beyond 0 and 1 on the graph. Usage distribution
   is the ratio distribution of patronage and population.

#+BEGIN_SRC ipython :session :file ./img/py6320jfT.png :exports both :tangle ./politics.py
  # Plot normalized state usage measures
  state_usage_min_zero = state_usage - state_usage.min()
  state_usage_range = state_usage.max() - state_usage.min()
  norm_usage = state_usage_min_zero / state_usage_range

  norm_usage.plot(kind="density", 
                  title="Normalized PDF estimations",
                  sharey=True)
#+END_SRC

#+RESULTS:
[[file:./img/py6320jfT.png]]

   We can see that usage has less variance than patronage and population,
   which we should expect. Perhaps it is somewhat more than expected,
   however.

#+BEGIN_SRC ipython :session :results output raw drawer :noweb yes :exports results :tangle ./politics.py
  stats = pd.DataFrame({"mean": norm_usage.mean(),
                        "median": norm_usage.median()})
  print("Mean/median of normalized state usage metrics:")
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  Mean/median of normalized state usage metrics:
  :END:
 
  #+BEGIN_SRC ipython :session :results output raw drawer :noweb yes :exports results :tangle ./politics.py
  print_df(stats)
#+end_src
#+RESULTS:
:RESULTS:
|            |     mean |   median |
|------------+----------+----------|
| patronage  | 0.202137 | 0.101378 |
| population | 0.152608 | 0.105552 |
| usage      | 0.292417 | 0.223591 |
:END:

   Here we can see illustrated what's been already hinted at: the states with the
   most and least usage are generally less populated and less patronaged, and, of
   course, there is a tight correlation between patronage and population. In the
   graph, redness relates to usage positively. The most red and most yellow dots
   are all in the least populated states/least patroned boards. We also see that
   generally, states that see more posts also tend to have higher usage. 

#+BEGIN_SRC ipython :session :file ./img/py6320Yhv.png :exports results :tangle ./politics.py
colors = cm.YlOrRd(state_usage.usage)

ax.spines["top"].set_visible(False)
ax.spines["right"].set_visible(False)

ax.get_xaxis().tick_bottom()
ax.get_yaxis().tick_left()

plt.ylabel("Patronage", fontsize=12)
plt.xlabel("Population", fontsize=12)

plt.suptitle('Patronage vs Population, heatmapped by Usage', fontsize=12)


plt.scatter(state_usage.population, state_usage.patronage, color=colors)
#+END_SRC
#+RESULTS:
[[file:./img/py6320Yhv.png]]

   My speculation is that activity on a social board, to a point,
   disproportionately encourages more activity. That is, if having more posts to
   look at means also a greater liklihood that a viewer will be inspired to make a
   post of their own, then the relationship between the raw number of posts on a
   message board and the number of prospective posters (which I'm supposing is
   proportional to state population) is greater than linear. That is, fewer posts
   means you, as a spectator, will be less likely to feel a desire to post, and
   therefore, a message board with few posts will see fewer new posts than a
   message board with many posts.

** Politics
*** Posts over Trumpism  :ignore:

   It seems that the distribution of posts is weighted on the Democrat's
   side of the spectrum:

#+BEGIN_SRC ipython :session :file ./img/py22415X0p.png :exports results :tangle ./politics.py
  post_politics = usa.join(states.trumpism, how="outer", on="state")
  post_politics.trumpism.plot(kind="hist", bins=20, color=["#FF9911"], 
                              title="Distribution of posts by politics")
#+END_SRC
#+RESULTS:
[[file:./img/py22415X0p.png]]

   However, Democratic registration outweighs Rebpublican voting rates
   slightly. We can visualize this preference a bit differently by
   finding the average post trumpism, and comparing it to national voting
   trends:

#+BEGIN_SRC ipython :session :exports code :results none :tangle ./politics.py
  avg_post_trumpism = post_politics.trumpism.mean()
  trump_votes = voting.trump.sum()
  clinton_votes = voting.clinton.sum()
  national_trumpism = trump_votes/float((trump_votes + clinton_votes))
#+END_SRC

   It's a bit more clear here that the skew of trumpism distribution is weighted a
   bit on the left, though the mean is quite close to what's expected, at about 48%
   of Trump+Clinton votes. The skewness of distribution is expected, and in line
   with my original hypothesis. In general, it would seem the most divided states
   see the most traffic, with less divided states being prominently Democratic. The
   mean in preserved by what seems to be in states that Trump won by a relatively
   small margin.

#+BEGIN_SRC ipython :session :exports results :results org output  :tangle ./politics.py
  # Some printing
  print(("Mean trumpism: {:.2f} Trump voters seem to show " + 
         "{:+.2f}% representation\non CL politics vs General " + 
         "Election results.").format(
             (avg_post_trumpism*100), 
             (avg_post_trumpism/national_trumpism)*100-100))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
Mean trumpism: 48.64 Trump voters seem to show -0.71% representation
on CL politics vs General Election results.
#+END_SRC

   An alternative representation that may make this skew a bit more apparent:

#+BEGIN_SRC ipython :session :file ./img/py26878eDX.png :exports results  :tangle ./politics.py
  post_trumpism_tot = post_politics.trumpism.plot(
      kind="density", 
      title="PDF estimation of Trumpism w/ mean",
      sharey=True)
  plt.axvline(post_politics.trumpism.mean(), color='r', linestyle='dashed', linewidth=.5)
  #+END_SRC

#+RESULTS:
[[file:./img/py26878eDX.png]]

*** Usage vs Trumpism

   We can see the correlations between patronage, population, and usage,
   here. We of course expect correlation between patronage and population
   to be quite high: states with more people generally have more
   posts. Below, positive correlation is pictured by redness, while
   negative is pictures by blueness. Darkness visualizes closeness.

#+BEGIN_SRC ipython :session :file ./img/py2241F8fd.png :exports results :tangle ./politics.py
  corr = states.filter(["patronage", "usage", "trumpism", "population"]).corr()
  fig, ax = plt.subplots(figsize=(4, 4))
  ax.matshow(corr, cmap=plt.cm.seismic)
  plt.xticks(range(len(corr.columns)), corr.columns);
  plt.yticks(range(len(corr.columns)), corr.columns);
#+END_SRC

#+RESULTS:
[[file:./img/py2241F8fd.png]]

   Note the correlation between trumpism and usage. Also, the correlation
   between patronage and usage coincides with how you'd expect boards
   with the least diversity to be disproportionately unfrequented. Boards
   with few posts become ghost towns. Here are the pearson correlation
   numbers behinds the colors:

#+BEGIN_SRC ipython :session :results output raw drawer :noweb yes :exports results :tangle ./politics.py
print_df(corr, rnd=3)
#+END_SRC
#+RESULTS:
:RESULTS:
|            | patronage |  usage | trumpism | population |
|------------+-----------+--------+----------+------------|
| patronage  |         1 |  0.327 |   -0.354 |       0.89 |
| usage      |     0.327 |      1 |   -0.268 |     -0.025 |
| trumpism   |    -0.354 | -0.268 |        1 |     -0.344 |
| population |      0.89 | -0.025 |   -0.344 |          1 |
:END:

* Text Qualities

  Text usage is interesting to consider, but difficult to evaluate
  semantically. While sampling encourages some compelling thoughts about
  the data, proving any derivative ideas is a bit difficult. The
  following is an effort to support the introduction of this blog post.

** Words :ignore:

   Popular English words are excluded from the analysis. Words like
   "the", "re", "and", etc., don't contribute interestingly. Popular
   words were grabbed from http://www.world-english.org/english500.htm,
   and a couple were added as needed (e.g., "re" appears all the time).

#+BEGIN_SRC ipython :session :exports none :tangle ./politics.py
  pop_english_words = ["the", "re", "a", "s",
                       "t", "i", "of", "to",
                       "and", "and", "in", "is",
                       "it", "you", "that", "he",
                       "was", "for", "on", "are",
                       "with", "as", "I", "his",
                       "they", "be", "at", "one",
                       "have", "this", "from", "or",
                       "had", "by", "hot", "but",
                       "some", "what", "there", "we",
                       "can", "out", "other", "were",
                       "all", "your", "shit", "when",
                       "up", "use", "word", "how",
                       "said", "an", "each", "she",
                       "which", "do", "their", "time",
                       "if", "will", "way", "about", "thought"
                       "many", "fuck", "then", "them",
                       "would", "write", "like", "so",
                       "these", "her", "long", "make",
                       "thing", "see", "him", "two",
                       "has", "look", "more", "day",
                       "could", "go", "come", "did",
                       "my", "sound", "no", "most",
                       "number", "who", "over", "know",
                       "water", "than", "call", "first",
                       "people", "may", "down", "side",
                       "been", "now", "find"]
#+END_SRC

#+RESULTS:

#+BEGIN_SRC ipython :session :exports none :tangle ./politics.py :results none
  from collections import Counter

  def post_words(df, no_pop=False):
      wds = re.findall(r'\w+', df.title.apply(lambda x: x + " ").sum())
      if no_pop:
          # pop_english_words is a list of the most popular (and boring) English
          # words. E.g., "and", "to", "the", etc.
          wds = [word for word in wds if word.lower() not in pop_english_words]
      return  wds

  def words(df=usa, no_pop=False):
      # word counts across all posts
      wds = post_words(df, no_pop)
      word_counts = Counter([word.lower() for word in wds])
      wd_counts = zip(*[[word, count] for word, count in word_counts.iteritems()])
      corpus = pd.Series(wd_counts[1], index=wd_counts[0]).rename("counts")

      return corpus.sort_values(ascending=False)
#+END_SRC

#+BEGIN_SRC ipython :session :exports none :tangle ./politics.py :results none :cache yes
# Probably don't care about stupid common words.
# `words' function grabs all the words from df, with option to exclude popular words
posts_corpus = words(df=usa, no_pop=True)

usa_words_full = post_words(df=usa)
usa_words = post_words(df=usa, no_pop=True)

posts_sum = " ".join(usa_words) # good estimate of sum of all posts, minus popular words
#+END_SRC

** Substrings                                                       :ignore:

#+BEGIN_SRC ipython :session :exports none :tangle ./politics.py :results none
  #
  # Find substrings in posts
  #

  def find_strs(substr, df=usa):
      """
      Get all titles from usa that have substr in their post title. Add some data on capitalization.
      """

      find = lambda s: (1 if re.search(substr, s, re.IGNORECASE) else np.nan)

      return df.title[df.title.map(find) == 1].rename("*" + substr + "*", inplace=True)

  def categ_strs(findings):
      """
      Return a list of
      """
      s = findings.name[1:-1]
      find = lambda sub, string: (1 if re.search(sub, string) else np.nan)

      proper = findings.apply(lambda x: find(s[0].upper() + s[1:].lower(), x)).rename("proper")
      cap = findings.apply(lambda x: find(s.upper(), x)).rename("uppercase")
      low = findings.apply(lambda x: find(s.lower(), x)).rename("lower")

      return pd.concat([proper, cap, low], axis=1)

  def eval_strs(string, df=usa):
      findings = find_strs(string, df)
      return categ_strs(findings).join(findings)
#+END_SRC

** Unicode

   I was curious about non-ascii usage, and so I used the following code to grab
   them:

#+BEGIN_SRC ipython :session :exports code :tangle ./politics.py
def check_ascii(post):
    """
    Determines whether a title is encodable as ascii
    """
    try:
        post.encode('ascii')
        return True
    except UnicodeError:
        return False
ascii_posts = usa[usa.title.apply(check_ascii)]
nonascii_posts = usa[~usa.title.apply(check_ascii)]
distinct_states = nonascii_posts["state"].unique()
#+END_SRC
#+RESULTS:

   The number of posts containing non-ascii characters was surprisingly small:

#+BEGIN_SRC ipython :session   :exports results :results output org :noweb yes :tangle ./politics.py
print ("{0:,} of {1:,} total posts were non-ascii ({2:.2f}%), confined to {3} "
       + "states.").format(len(nonascii_posts),
                       len(usa),
                       len(nonascii_posts)/float(len(usa)) * 100,
                       len(distinct_states))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
392 of 79,462 total posts were non-ascii (0.49%), confined to 26 states.
#+END_SRC

   We can look at the main outlier for the posts by checking out Pennsylvania:

#+BEGIN_SRC ipython :session  :exports code :tangle ./politics.py :results none
  pennsylvania = nonascii_posts[nonascii_posts["state"] == "Pennsylvania"]
  pennsylvania.groupby("region").count()
  penn_lenn = float(len(pennsylvania.title))
  post_uniqueness = (penn_lenn-pennsylvania.title.nunique())/penn_lenn * 100
#+END_SRC

   We can use a SequenceMatcher to test the similarity of the strings in the pool:

#+BEGIN_SRC ipython :session  :exports code :tangle ./politics.py
  import itertools
  from difflib import SequenceMatcher
  def avg_similarity(posts):
    def similarity(a, b):
      return SequenceMatcher(None, a, b).ratio()
    sim_sum = 0
    title_product = itertools.product(posts.title, posts.title)
    for title_pair in title_product:
      sim_sum += similarity(*title_pair)
    avg_sim = sim_sum/(len(posts)**2)
    return avg_sim
#+END_SRC

#+RESULTS:

   Running this over all non-ascii posts to get an idea of how much silliness is
   going on with these posts:

#+BEGIN_SRC ipython :session :exports results :results output org :noweb yes :tangle ./politics.py :cache yes
    print(("The average similarity of all non-ascii posts is " +
           "{:.2f}, while that \nof only those in Pennsylvania is " +
           "{:.2f}. The average for all posts in\nall regions is " +
           "{:.2f}.")).format(avg_similarity(nonascii_posts),
                              avg_similarity(pennsylvania),
                              avg_similarity(usa.sample(200)))
#+END_SRC

#+RESULTS[2f3dffa2f757c0a80e292c245bfdb5a8afb660a0]:
#+BEGIN_SRC org
The average similarity of all non-ascii posts is 0.19, while that 
of only those in Pennsylvania is 0.37. The average for all posts in
all regions is 0.18.
#+END_SRC

   It would therefore seem that a single Trump memester, making use of a
   handful of unicode symbols, is responsible for this chaos in
   Pennsylvania. I suspect that these crazy unicode posts are mostly
   done by a very small set of people in general, though there is no
   good way to tell, as CL is completely anonymous.

** Liberals vs Conservatives
*** intro :ignore:

    Investigating the discrepency between Democrat/Republican word usage, we see
    some discrepencies in the most used common words.

#+BEGIN_SRC ipython :session :exports code :results none :tangle ./politics.py 
  # Grab some words
  lib_words = words(df=post_politics[post_politics.trumpism < .45],
                    no_pop=True).rename("libs")
  conserv_words = words(df=post_politics[post_politics.trumpism > .55],
                        no_pop=True).rename("conservs")
#+end_src


#+begin_src ipython :session :exports none :results none :tangle ./politics.py
  # THIS IS BROKEN AND BAD. Placeholder code
  rat = lambda df: df.libs/df.conservs
  ratio = pd.DataFrame().join([lib_words[lib_words >= 10],
                               conserv_words[conserv_words >= 10]],
                              how="outer").apply(rat, axis=1).dropna()
  ratio = ratio.rename("dem/rep ratio")

  lib_con_ratio = pd.DataFrame(posts_corpus).join(ratio.sort_values(ascending=False),
                                                  how="inner")
#+end_src

#+BEGIN_SRC ipython :session :exports results :results value raw org :noweb yes :tangle ./politics.py
lib_con_ratio[:10]
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
            counts  dem/rep ratio
thought        595      19.080000
tv             231      14.545455
global         339      11.583333
world          596      10.941176
top            166       9.600000
wake           198       9.090909
government     350       8.550000
dnc            133       8.400000
life           255       8.375000
york           166       8.250000
#+END_SRC

*** Words :ignore:
    :PROPERTIES:
    :ATTACH_DIR_INHERIT: t
    :END:

    We find that "tax", "speech", and "russian" among those words with large
    preference in "liberal" states. Some random sampling of such posts:

#+BEGIN_SRC ipython :session :results output raw drawer :noweb yes  :exports results :cache yes
  print_df(pd.DataFrame(pd.concat([find_strs("tax"),
                                   find_strs("speech"),
                                   find_strs("russian")]).rename(
                                       "title")).sample(5), 
           rnd=3)
#+END_SRC
#+RESULTS[ca9ac3dfac3c1e86a2cc0ef814b44c3d60e1fba5]:
:RESULTS:
|       | title                                                                  |
|-------+------------------------------------------------------------------------|
| 49531 | President's Speech On Illegal Immigration                              |
| 51181 | 5 Trillion Taxpayers Dollars Given to War Industrial Complex aka NWO   |
| 54820 | Re:  Coalition for Free Speech                                         |
| 64197 | Hate The Constitution, God, Free Speech, White People & Enjoy Lying?   |
| 20885 | re,  Democrats Freaking Out Because They Know Tax Cuts Will Help Our . |
:END:

   Looking at general word usage, we see how often President Obama and President
   Trump are discussed. Note that "hillary" and "clinton" are surprisingly not
   mentioned as much as you might think. "Clinton", in fact, is mentioned less
   freqeuntly than "Donald". It may be that a month after the election, "hillary"
   talk has already begun to significantly subside. It's impossible to know for
   sure, as CL does not hold on to their posts for longer than a week.

#+BEGIN_SRC ipython :session :file ./img/py31406ImT.png :exports results :tangle ./politics.py
p = posts_corpus[:25].sort_values(ascending=True)

ax = p.plot(kind="bar", color="#662200", grid=True)

ax.spines["top"].set_visible(False)
ax.spines["right"].set_visible(False)

ax.get_xaxis().tick_bottom()
ax.get_yaxis().tick_left()

plt.ylabel("Occurences", fontsize=12)

plt.suptitle('Word usages', fontsize=14)

ax.spines["top"].set_visible(False)
ax.spines["right"].set_visible(False)

ax.get_xaxis().tick_bottom()
ax.get_yaxis().tick_left()
#+END_SRC

#+RESULTS[4cfeb62c1d4cb9d2e0ccc865f9f60fd806d810e9]:
[[file:./img/py31406ImT.png]]

#+BEGIN_SRC ipython :session :exports results    :tangle ./politics.py
 # Splitting a series into chunks such that values.sum() = val (or as close
 # as possible, greedily) so we can wee how the diversity of words is
 # distributed:
 def splicer(ss, val):
   indices = ss.index.tolist()
   if len(indices) <= 1:
     return pd.Series(ss[index[0]], index=[[indices[0]]])
   left = [ss.index[0]]
   right = ss.index[1:].tolist()
   s = ss[left[0]]
   while s < val and len(right) > 0:
     i = right.pop(0)
     left.append(i)
     s += ss[i]
   return [ss.filter(left)] + (splicer(ss.filter(right), val) if len(right) > 0 else [])
#+END_SRC
#+RESULTS[44365645107e2b7164001f81c43a81afbf66cd00]:
*** TODO Correct bad graph                                         :noexport:
#+BEGIN_SRC ipython :session :file ./img/pyF7JjmI.png :exports results :tangle ./politics.py
 chunks = splicer(posts_corpus, posts_corpus.iloc[0])
 ax = plt.subplot()
 
 ax.spines["top"].set_visible(False)
 ax.spines["right"].set_visible(False)

 ax.get_xaxis().tick_bottom()
 ax.get_yaxis().tick_left()

 plt.ylabel("", fontsize=12)
 plt.suptitle('', fontsize=14)

 ax.spines["top"].set_visible(False)
 ax.spines["right"].set_visible(False)

 ax.get_xaxis().tick_bottom()
 ax.get_yaxis().tick_left()

 plt.bar(np.arange(0, len(chunks)), np.array([len(c) for c in chunks]))
 
#+END_SRC
#+RESULTS:
[[file:./img/pyF7JjmI.png]]
*** TODO Diversity of words vs trumpism                            :noexport:
*** "trumps"
**** intro :ignore:

#+BEGIN_SRC ipython :session :exports none :tangle ./politics.py :results none :cache yes
trumps = eval_strs("trump").join(usa.state, how="inner")
trumps_by_state = trumps.groupby("state").count().join(states).drop(["clinton", "trump"], axis=1)
up_over_trumps = (trumps_by_state.uppercase/trumps_by_state["*trump*"]).rename("uppercase usage")
prop_over_trumps = (trumps_by_state.proper/trumps_by_state["*trump*"]).rename("propercase usage")
trumps_over_pat = (trumps_by_state["*trump*"]/trumps_by_state.patronage).rename("trumps usage")
trumps_by_state = trumps_by_state.join([prop_over_trumps, up_over_trumps, trumps_over_pat], how="outer")
#+END_SRC

**** Politics :ignore:

The more pro-Trump your state, the less likely you are to use "TRUMP" over
"Trump". Below is a visual depicting this ratio, by states in order of
trumpism. We can see that states on the right of the graph tend to have a low
ratio of upper to proper. 

#+BEGIN_SRC ipython :session :file ./img/py6320cup.png :exports results :tangle ./politics.py
  trumps_vs_trumpism = trumps_by_state.sort_values(
      "trumpism", ascending=True).filter(["propercase usage",
                          "uppercase usage"])

  trumps_vs_trumpism.plot(kind="bar", stacked=True, figsize=(10, 5))

  ax = plt.subplot()

  ax.spines["top"].set_visible(False)
  ax.spines["right"].set_visible(False)

  ax.get_xaxis().tick_bottom()
  ax.get_yaxis().tick_left()

  plt.xlabel("States, in order of trumpism")

  ax.spines["top"].set_visible(False)
  ax.spines["right"].set_visible(False)

  ax.get_xaxis().tick_bottom()
  ax.get_yaxis().tick_left()
#+END_SRC

#+RESULTS:
[[file:./img/py6320cup.png]]

Looking at the distribution of "trump" posts across trumpism looks
much the same as the distribution of all posts across trumpism:

#+BEGIN_SRC ipython :session :file ./img/py268781zz.png :exports results :tangle ./politics.py
    post_politics.trumpism.plot(kind="density", linewidth=0.8)

    ax = plt.subplot()

    ax.spines["top"].set_visible(False)
    ax.spines["right"].set_visible(False)

    ax.get_xaxis().tick_bottom()
    ax.get_yaxis().tick_left()

    plt.ylabel("Occurences", fontsize=12)

    ax.spines["top"].set_visible(False)
    ax.spines["right"].set_visible(False)

    ax.get_xaxis().tick_bottom()
    ax.get_yaxis().tick_left()

    trumps_trumpism = trumps.join(post_politics.trumpism)

    trumps_trumpism.trumpism.plot(kind="density", 
                                  title="PDF of trumpism for "  +  
                                  "posts containing 'Trump'",
                                  linewidth=2)
    plt.axvline(trumps_trumpism.trumpism.mean(), color='r',
                linestyle='dashed', linewidth=.5)
#+END_SRC

#+RESULTS:
[[file:./img/py268781zz.png]]

However, Democratic states seem to have relatively strong preferance
for using "TRUMP" versus "Trump". Below's graph depicts this skew,
which is made more noticible by the considerable left-shift of the
mean:

#+BEGIN_SRC ipython :session :file ./img/py26878b0D.png :exports results :tangle ./politics.py
  cap_trumps = trumps_trumpism[trumps_trumpism.uppercase > 0]

  ax = plt.subplot()

  ax.spines["top"].set_visible(False)
  ax.spines["right"].set_visible(False)

  ax.get_xaxis().tick_bottom()
  ax.get_yaxis().tick_left()

  ax.spines["top"].set_visible(False)
  ax.spines["right"].set_visible(False)

  ax.get_xaxis().tick_bottom()
  ax.get_yaxis().tick_left()

  cap_trumps.trumpism.plot(kind="density", 
                           title="PDF of trumpism for posts " \
                           "containing 'TRUMP'",
                           color='blue', linewidth=1.5)
  plt.axvline(cap_trumps.trumpism.mean(), color='r',
              linestyle='dashed', linewidth=.5)
#+END_SRC

#+RESULTS:
[[file:./img/py26878b0D.png]]

It isn't clear why there seems to be preference for capitalization of "TRUMP"
among Dem states; are they mostly angry and disparaging, supportive, or a bit of
both? Some random sampling of particularly liberal states might provide some
clues:

#+BEGIN_SRC ipython :session :exports code :tangle ./politics.py
  liberal_sample = trumps_trumpism[trumps_trumpism.trumpism < .45].sample(5)
#+END_SRC  

  #+RESULTS:

#+BEGIN_SRC ipython :session :exports results :results output drawer :noweb yes :cache yes :tangle ./politics.py
  print("Selecting states that are espectially " \
        "anti-trump:\n")
  print_df(pd.DataFrame(liberal_sample["*trump*"]))

  print("Politically liberal states composing " +
        "the above sampling:\n{}.".format(
             ", ".join("{}".format(r) for r in liberal_sample.state.unique())))
#+END_SRC

#+RESULTS[4d43db8a66ff42fe03b7b14043e725beb92bfb7e]:
:RESULTS:
Selecting states that are espectially anti-trump:

|       | *trump*                                                                |
|-------+------------------------------------------------------------------------|
| 51300 | VLADIMIR PUTIN HAS AN AMAZING VALENTINE'S DAY PRESENT FOR DONALD TRUMP |
| 31818 | RE,RE Why trump doesn't beleive CIA                                    |
| 54689 | Trump is my President                                                  |
| 31645 | Wheat and Tares Prophecy Dream- Trump/Clinton God/Satan                |
| 52482 | HOW LONG WILL tRump LAST ???                                           |
Politically liberal states composing the above sampling:
California, New Jersey, Massachusetts.
:END:

*** "hillary"
*** "liberal" vs "conservative"
**** *Usage*
"liberal" is used far more often than "conservative". The pluralizations,
respectively, are comparitively not quite as distinguished, but still quite
different.  Below are the instance rate ratios of "liberal" and "conservative"
in their various forms.
#+BEGIN_SRC ipython :session :exports results :results output org :noweb yes :tangle ./politics.py
  liberal = float(posts_corpus["liberal"])
  liberal_p = float(posts_corpus["liberals"])
  conserv = float(posts_corpus["conservative"])
  conserv_p = float(posts_corpus["conservatives"])

  print ("liberal/conservative: {0:.2f}\n" +
         "liberals/conservatives: {1:.2f}\n" +
         "liberal(s)/conservative(s): {2:.2f}" +
         "") .format(liberal/conserv,
                     liberal_p/conserv_p,
                     (liberal+liberal_p)/(conserv+conserv_p))

#+END_SRC
#+RESULTS:
#+BEGIN_SRC org
liberal/conservative: 6.68
liberals/conservatives: 5.58
liberal(s)/conservative(s): 6.24
#+END_SRC

**** *Pluralization*
The singular version of "conservative" is used a bit more than half as much as
the pluralization. By contrast, the singular version of "liberal" is used more
than twice as much as the pluralization:
#+BEGIN_SRC ipython :session :exports results :results output org :noweb yes :tangle ./politics.py
  print("*singular/plural*\n" +
        "'conservative': {0:.3f}\n" +
        "'liberal': " +
        "{1:.3f}").format(posts_corpus["conservative"]/float(posts_corpus["conservatives"]),
                          posts_corpus["liberal"]/float(posts_corpus["liberals"]))

#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
,*singular/plural*
'conservative': 1.495
'liberal': 1.789
#+END_SRC

**** *Capitalization*
We here see that there is a great preference for capitalization of "liberal"
vs. "conservative". "'liberal' preference" refers to the capitalization rates of
"liberal"/"conservative".
#+BEGIN_SRC ipython :session :exports code :results none :tangle ./politics.py
  libs = eval_strs("liberal").sum(numeric_only=True)
  conservs = eval_strs("conservative").sum(numeric_only=True)

  lib_con_rates = (libs/libs.sum()) / (conservs/conservs.sum())
  lib_con_rates.rename("'liberal'/'conservative' usage", inplace=True)

  lib_con_cap_rat = pd.DataFrame([(libs/conserv).rename(
      "# 'liberal' per 'conservative'"), lib_con_rates])
#+END_SRC

#+BEGIN_SRC ipython :session :exports results :results output raw drawer :noweb yes :tangle ./politics.py
print_df(pd.DataFrame(lib_con_cap_rat))
#+END_SRC
#+RESULTS:
:RESULTS:
|                                |   proper | uppercase |   lower |
|--------------------------------+----------+-----------+---------|
| # 'liberal' per 'conservative' |  5.81618 |  0.683824 | 4.47794 |
| 'liberal'/'conservative' usage | 0.901983 |  0.987068 | 1.16706 |
:END:

** Semantics
I figured that a natural way to go about investigating sentiment would be
semantic analysis. I quickly decided that this was, with it's present
implementation at least, not the way to go about it. The following code will run
semantic analysis using the popular NLTK package. The results are as dubious as
my implementation.
#+BEGIN_SRC ipython :session :exports code :tangle ./politics.py
  from textblob import TextBlob

  def semants(text):
      blob = TextBlob(text)
      ss = 0
      for sentence in blob.sentences:
          ss += sentence.sentiment.polarity
      return float(ss)/len(blob.sentences)

  # package does not like non-ascii encodings
  trumps_ascii = trumps[trumps["*trump*"].apply(check_ascii)]


  usa_sentiment = post_politics.join(ascii_posts.title.apply(
      semants).rename("sentiment"))
  trumps_sentiment = usa_sentiment.filter(trumps_ascii.index, axis=0)
#+END_SRC

#+RESULTS[f9c165e005384b105d899de515d25e9a2578b73a]:

Unconvincing results:
#+BEGIN_SRC ipython :session :exports both :results output org :noweb yes :tangle ./politics.py
  zero_sents = len(usa_sentiment[usa_sentiment.sentiment == 0])
  print(('Number of posts with 0 sentiment: {0:,} ' + 
         '({1:.2f}%).').format(zero_sents, 
                               float(zero_sents)/len(usa_sentiment)*100))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
Number of posts with 0 sentiment: 52,774 (66.41%).
#+END_SRC

* Conclusion :noexport:
Overall, I've been quite satisfied with the process of deploying a scrapy
program. Denver, Colorado, was by far the most perplexing outlier of the
dataset, one which I am interested to rescrape in the near future. I was also
suprised to find such high usage rates among democratic states, which seemed to
be somehow related to the exorbitent rates at which "liberal" used over
"conservative".

* Notes about this document
This document is, in its original form, an emacs org-mode organizational markup
document that supports interactive programming and exporting quite
thoroughly. It exports to a variety of formats (html, latex, markdown, etc), and
in this case, was exported directly to html. It's quite powerful, and allows me
to tailor what headers are exported, what code is exported, what code results,
to what interpreter the code talks, how it's formated, etc. The original
document, if viewed in org-mode in emacs, is quite a bit larger, containing all
of the code used for the project, most of which is not shown in markdown
exports. Therefore, if you view this document on github, you will see a
truncated version much like the version you are likely viewing now. You can view
on github, a .ipynb and a .py export are available for the complete code of the
document. Obviously, they won't include the organization and commentary. You can
look at the raw contents of the .org file if curious (github will export
primitively to html by default for display), or check out this [[http://kozikow.com/2016/05/21/very-powerful-data-analysis-environment-org-mode-with-ob-ipython/comment-page-1/#comment-240][blog on
interactive python programming in emacs org-mode]].
* Meta  :noexport:
** Trump Word Cloud
#+BEGIN_SRC ipython :session :file :exports results :tangle ./politics.py  
  from os import path
  from PIL import Image

  from wordcloud import WordCloud

  d = path.dirname(".")

  plt.figure(num=None, figsize=(10, 8))

  trump_mask = np.array(Image.open(path.join(d, "img/Trump_silhouette.png")))

  wc = WordCloud(background_color="white", max_words=2000, mask=trump_mask)

  wc.generate(posts_sum)

  wc.to_file(path.join(d, "img/Trump_test.png"))

  plt.imshow(wc)
  plt.axis("off")
  plt.figure()
  plt.imshow(trump_mask, cmap=plt.cm.gray)
  plt.axis("off")

  plt.show()

#+END_SRC
#+RESULTS[36252510400e47ae15b37acc15a3f03f4ef80328]:
: <matplotlib.figure.Figure at 0x7fd057b580d0>
