* Introduction
#+BEGIN_SRC ipython :session :file  :exports both
%matplotlib inline
import numpy as np
import pandas as pd
import scipy
from scipy import stats
import matplotlib as mpl
import matplotlib.pyplot as plt

import pprint as pp
import pickle
import re

pd.options.display.max_colwidth = 1000
#+END_SRC

#+RESULTS:

* Preparing Data
** Grab Data
#+BEGIN_SRC ipython :session :file  :exports both 
# read us data collected by craigcrawler 
usa_raw = pd.read_csv("data/us.csv", index_col=0)
post_count_total_raw = len(usa_raw)
post_count_by_state_raw = usa_raw.groupby("state").count()["title"]#.sort_values(ascending=False)
post_count_by_region_raw = usa_raw.groupby("region").count()["title"]#.sort_values(ascending=False)

print ("\n{0:,} total posts exctracted from {3:,} regions over {4} "+ 
       "state. The most popular\nstate was {1}, and the most " + 
       "popular region was, surprisingly, {2}.").format(post_count_total_raw,
                                                        post_count_by_state_raw.index[0],
                                                        post_count_by_region_raw.index[0],
                                                        len(post_count_by_region_raw),
                                                        len(post_count_by_state_raw))


#+END_SRC
#+RESULTS:
** U.S. Census 2010
- US census data for 2010 from the census bureau
- Census data is not labelled exactly as my data is. Some states are named a little differently,
and regions are almost never named similarly. These have to be resolved.

*** Geo Keys
#+BEGIN_SRC ipython :session :file  :exports both
# Keys for geography stuff. Table is an index table.
# These keys are used as index for census table.
GEO_NAME = "GEO.display-label"
GEO_KEY = "GEO.id"
state_keys = pd.read_csv("data/census/DEC_10_DP_G001_with_ann.csv")[1:].set_index(GEO_KEY)

state_keys = state_keys.filter([GEO_NAME])[:52]
state_keys = state_keys[state_keys[GEO_NAME]!= "Puerto Rico"]
#+END_SRC

#+RESULTS:

*** Census Data
#+BEGIN_SRC ipython :session :file  :exports both
  # keys for the census data. Only really care about two of them (there are hundreds):
  TOT_NUM_ID = "HD01_S001" # total number key
  TOT_PER_ID = "HD02_S001" # total percent key

  census = pd.read_csv("data/census/DEC_10_DP_DPDP1_with_ann.csv")[1:].set_index(GEO_KEY)

  census = census.filter([TOT_NUM_ID])
  census = census.join(state_keys, how="right")
  census.columns = ["population", "state"]
  census.set_index("state", inplace=True)
  

  
  def correct_stat(s):
      """
      Some states have extra information for population. 
      Example: 25145561(r48514)
      """
      loc = s.find("(")
      return int(s[:loc] if loc > 0 else s)

  census.population = census.population.apply(correct_stat)
#+END_SRC

#+RESULTS:

[5 rows x 375 columns]
** U.S. 2016 Election
#+BEGIN_SRC ipython :session :file  :exports both
  import requests
  from scrapy import Selector

  atlas_url = "http://uselectionatlas.org/RESULTS/data.php?year=2016&datatype=national&def=1&f=1&off=0&elect=0"
  atlas_source = requests.get(atlas_url).text
  select = Selector(text=atlas_source).xpath('//*[@id="datatable"]/tbody/tr')

  convert = lambda s: int(s.replace(',', ''))
  vote_names = map(str, select.xpath('td[3]/a/text()').extract())
  # Correct name for DC
  vote_names[8] = "District of Columbia"
  clinton_votes = map(convert, select.xpath('td[17]/text()').extract())
  trump_votes = map(convert, select.xpath('td[18]/text()').extract())

  gen_votes = pd.DataFrame({"clinton": clinton_votes, "trump": trump_votes}, index=vote_names)

  trump_favor = pd.DataFrame(gen_votes["trump"]/gen_votes.sum(axis=1), columns=["trumpism"], index=vote_names)  
  voting = gen_votes.join(trump_favor).sort_values("trumpism", ascending=False)  

  # for pretty printing
  space = pd.DataFrame([["------", "------", "------"]],index=["*SPACE*"], columns=voting.columns) 
  pd.concat([voting[:5], space, voting[-5:].sort_values("trumpism")])
#+END_SRC

#+RESULTS:
#+begin_example
                      clinton    trump   trumpism
Wyoming                 55973   174419   0.757053
West Virginia          188794   489371   0.721611
North Dakota            93758   216794   0.698092
Oklahoma               420375   949136   0.693047
Idaho                  189765   409055   0.683102
*SPACE*                ------   ------     ------
District of Columbia   282830    12723  0.0430481
Hawaii                 266891   128847   0.325587
California            8753788  4483810   0.338718
Vermont                178573    95369   0.348136
Massachusetts         1995196  1090893   0.353487
#+end_example

** Preprocess Data

Some preprocessing to check data corrupted files
#+BEGIN_SRC ipython :session :file  :exports both
  print "Data tests... \n\nAssertions Passed\n\n"

  # Confirm all expected regions and states present
  assert len(usa_raw["state"].unique()) == 52 # expected number of states
  assert len(usa_raw["region"].unique()) == 416  # expected number of regions
 
  # Confirm that there are no posts without regions/states. Not all CL 
  # regions have subregions, so it's okay for null subregions.
  assert len(usa_raw[usa_raw["state"].isnull()].index) == 0
  assert len(usa_raw[usa_raw["region"].isnull()].index) == 0

  # Find regions/subregions for which there are no posts
  postless_regions = usa_raw[usa_raw["title"].isnull()]  
  postless_regions_times = usa_raw[usa_raw["date"].isnull()]

  # not actually an effective test, but good enough
  assert len(postless_regions) == len(postless_regions_times)

  print(("{0:,} regions/subregions over {1} states without " + 
         "any posts.").format(len(postless_regions), postless_regions["state"].nunique()))  
#+END_SRC

#+RESULTS:

Drop unneeded data
#+BEGIN_SRC ipython :session :file  :exports both
# Drop empty regions.
usa = usa_raw.dropna(subset=["title", "date"], how="any", axis=0)
assert len(postless_regions) == len(usa_raw)-len(usa)

# Get rid of territories (Guam, Puerto Rico)
usa = usa[usa["state"] != "Territories"]
#+END_SRC

#+RESULTS:

Confirm Census Data
#+BEGIN_SRC ipython :session :file  :exports both
assert set(usa.state.unique()) == set(census.index) and len(usa.state.unique() == len(census.index))

print "Census data complete"
#+END_SRC

#+RESULTS:

Confirm Election Data
#+BEGIN_SRC ipython :session :file  :exports both
assert set(usa.state.unique()) == set(voting.index) and len(usa.state.unique() == len(voting.index))

print "Voting data complete"
#+END_SRC
#+RESULTS:
* Unicode
ascii vs. unicode usage. 
#+BEGIN_SRC ipython :session :file  :exports both
def check_ascii(post):
    """
    Determines whether a title is encodable as ascii
    """
    try:
        post.encode('ascii')
        return True
    except UnicodeError:
        return False

ascii_titles_tv = usa.title.apply(check_ascii)
nonascii_posts = usa[~ascii_titles_tv]

distinct_states = nonascii_posts["state"].unique()
print ("{0:,} of {1:,} total posts were non-ascii ({2:.2f}%), confined to {3} "
       + "states.").format(len(nonascii_posts),
                       len(usa),
                       len(nonascii_posts)/float(len(usa)) * 100,
                       len(distinct_states))
#+END_SRC

#+RESULTS:

** Pennsylvania
Pennsylvania has was the preeminent outlier in non-ascii usage per-state
#+BEGIN_SRC ipython :session :file  :exports both
nonascii_states_count = nonascii_posts.groupby(
    "state").title.nunique().sort_values(ascending=False)
print "\nTop ten most popular unicode states:"
print nonascii_states_count[:10]

pennsylvania = nonascii_posts[nonascii_posts["state"] == "Pennsylvania"]
print pennsylvania["title"].tolist()[0]

print("\nA single Trump memester seems to be responsible for the chaos " +
      "in Pennsylvania.\n" + "I suspect that these crazy unicode posts " +
      "are mostly done by a very small\nset of people, though there is " +
      "no way to tell.")
print "\nRandom sample of 5 non-ascii Pennsylvania posts"
print pennsylvania["title"][:5]

pennsylvania.groupby("region").count()

post_uniqueness = pennsylvania.title.nunique()/float(len(pennsylvania.title))
#+END_SRC

#+RESULTS:
=                   title  date  state  subregion
region                                          
harrisburg, PA        11    11     11          0
lancaster, PA         11    11     11          0
philadelphia           1     1      1          0
pittsburgh, PA         1     1      1          0
reading, PA           10    10     10          0
state college, PA     11    11     11          0
york, PA              11    11     11          0
==<pandas.core.groupby.DataFrameGroupBy object at 0x7fa5c0d57250>
==<pandas.core.groupby.DataFrameGroupBy object at 0x7fa5f43f5050>
==Series([], dtype: int64)
==Empty DataFrame
Columns: [title, date, state, region]
Index: []
=* 
*** Colorado
#+BEGIN_SRC ipython :session :file  :exports both
print "\n\n{0} regions in Colorado".format(usa[usa['state'] == "Colorado"]["region"].nunique())
#+END_SRC

#+RESULTS:

* State Popularity
** Usage
#+BEGIN_SRC ipython :session :file  :exports both
patronage = pd.DataFrame(usa.groupby('state').size(), columns=["patronage"])

print "\nTop ten most popular states"
print usage_by_state[:10]
#+END_SRC 
#+RESULTS:

** Normalization
#+BEGIN_SRC ipython :session :file  :exports both
    cl_by_state = patronage.join(census, how="inner")
    normalized_usage = cl_by_state.apply(
        lambda df: df["patronage"] / float(df["population"]), axis=1)

    # Weight for mean usage = 1.000
    weight = float(census.population.mean()/patronage.mean())/1.0605
    weighted_normal_usage = pd.DataFrame((normalized_usage * weight),
                                    columns=["normalized"])

    state_usage = pd.concat([cl_by_state, weighted_normal_usage],
                            axis=1).sort_values("patronage",
                                                ascending=False)
#+END_SRC
#+RESULTS:

** Analysis
*** Patronage
#+BEGIN_SRC ipython :session :file /home/dodge/workspace/craig-politics/py6320LXp.png :exports both
pat = state_usage.sort_values("patronage", ascending=True)
x = np.arange(len(pat))
p1 = pat.patronage

plt.bar(x, p1)
#+END_SRC

#+RESULTS:
[[file:/home/dodge/workspace/craig-politics/py6320LXp.png]]

#+BEGIN_SRC ipython :session :file /home/dodge/workspace/craig-politics/py6320lr1.png :exports both
p2 = pat.normalized

plt.bar(x, p2)
#+END_SRC

#+RESULTS:
[[file:/home/dodge/workspace/craig-politics/py6320lr1.png]]

#+BEGIN_SRC ipython :session :file /home/dodge/workspace/craig-politics/py6320Yhv.png :exports both
p2 = state_usage.population.sort_values()
p2.x = np.arange(len(p2))
p2.y = p2.values

plt.plot(p2.x, p2.y)
#+END_SRC

#+RESULTS:
[[file:/home/dodge/workspace/craig-politics/py6320Yhv.png]]

*** Normalized Patronage
I expect population to relate to patronage linearly.
#+BEGIN_SRC ipython :session :file /tmp/image.png  :exports both
# Getting rid of California
p1 = state_usage.filter(["population", "patronage"]).sort_values("population", ascending=False)[1:]

plt.plot(p1["population"], p1["patronage"])
#+END_SRC
#+RESULTS:
[[file:/tmp/image.png]]
#+BEGIN_SRC ipython ipython :session :file /tmp/population2.png  :exports both
p2 = state_usage.filter(["population", "normalized"]).sort_values("population", ascending=False)[2:]

plt.plot(p2["population"], p2["normalized"])
#+END_SRC

#+RESULTS:
[[file:/tmp/population2.png]]

