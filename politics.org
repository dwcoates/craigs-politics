#+TITLE: A look into Craigslist politics
#+OPTIONS: toc:nil


# <h1 align="center"><font color="0066FF" size=110%>Simple Notebook</font></h1>


* Introduction
#+BEGIN_SRC ipython :session :file  :exports both :tangle ./politics.py
%matplotlib inline
import numpy as np
import pandas as pd
import scipy
from scipy import stats
import matplotlib as mpln
import matplotlib.pyplot as plt

import pprint as pp
import pickle
import re

pd.options.display.max_colwidth = 1000
#+END_SRC

#+RESULTS:

#+BEGIN_SRC ipython :session :file  :exports both
  def easy_default_plt():
      fig = plt.figure() # Create matplotlib figure
      
      ax = fig.add_subplot(111) # Create matplotlib axes
      ax.spines['top'].set_visible(False)

      ax.get_xaxis().tick_bottom()
      ax.get_yaxis().tick_left()

      plt.rc('grid', linestyle="+", color='black')
      plt.grid()

      return fig
#+END_SRC
#+RESULTS:

* Preparing Data
** Grab CL Data
#+BEGIN_SRC ipython :session :file  :results output raw drawer :noweb yes :exports both  :tangle ./politics.py
# read us data collected by craigcrawler 
usa_raw = pd.read_csv("data/us.csv", index_col=0)
post_count_total_raw = len(usa_raw)
post_count_by_state_raw = usa_raw.groupby("state").count()["title"]#.sort_values(ascending=False)
post_count_by_region_raw = usa_raw.groupby("region").count()["title"]#.sort_values(ascending=False)

print ("\n{0:,} total posts exctracted from {3:,} regions over {4} "+ 
       "state. The most popular\nstate was {1}, and the most " + 
       "popular region was, surprisingly, {2}.").format(post_count_total_raw,
                                                        post_count_by_state_raw.index[0],
                                                        post_count_by_region_raw.index[0],
                                                        len(post_count_by_region_raw),
                                                        len(post_count_by_state_raw))

#+END_SRC
#+RESULTS:
:RESULTS:

38,692 total posts exctracted from 416 regions over 52 state. The most popular
state was Alabama, and the most popular region was, surprisingly, SF bay area.
:END:
** U.S. Census 2010
- US census data for 2010 from the census bureau
- Census data is not labelled exactly as my data is. Some states are named a little differently,
and regions are almost never named similarly. These have to be resolved.
*** Geo Keys
#+BEGIN_SRC ipython :session :file  :exports both :tangle ./politics.py
# Keys for geography stuff. Table is an index table.
# These keys are used as index for census table.
GEO_NAME = "GEO.display-label"
GEO_KEY = "GEO.id"
state_keys = pd.read_csv("data/census/DEC_10_DP_G001_with_ann.csv")[1:].set_index(GEO_KEY)

state_keys = state_keys.filter([GEO_NAME])[:52]
state_keys = state_keys[state_keys[GEO_NAME]!= "Puerto Rico"]
#+END_SRC

#+RESULTS:

*** Census Data
#+BEGIN_SRC ipython :session :file  :exports both :tangle ./politics.py
  # keys for the census data. Only really care about two of them (there are hundreds):
  TOT_NUM_ID = "HD01_S001" # total number key
  TOT_PER_ID = "HD02_S001" # total percent key

  census_all = pd.read_csv("data/census/DEC_10_DP_DPDP1_with_ann.csv")[1:].set_index(GEO_KEY)

  census_all = census_all.filter([TOT_NUM_ID])
  census_all = census_all.join(state_keys, how="right")
  census_all.columns = ["population", "state"]
  census_all.set_index("state", inplace=True)
    
  def correct_stat(s):
      """
      Some states have extra information for population. 
      Example: 25145561(r48514)
      """
      loc = s.find("(")
      return int(s[:loc] if loc > 0 else s)

  census_all.population = census_all.population.apply(correct_stat)
  
  census = census_all.drop("District of Columbia")
#+END_SRC

#+RESULTS:

[5 rows x 375 columns]
** U.S. 2016 Election
#+BEGIN_SRC ipython :session :file :results output raw drawer :noweb yes :exports both :tangle ./politics.py
  import requests
  from scrapy import Selector

  atlas_url = "http://uselectionatlas.org/RESULTS/data.php?year=2016&datatype=national&def=1&f=1&off=0&elect=0"
  atlas_source = requests.get(atlas_url).text
  select = Selector(text=atlas_source).xpath('//*[@id="datatable"]/tbody/tr')

  convert = lambda s: int(s.replace(',', ''))
  vote_names = map(str, select.xpath('td[3]/a/text()').extract())
  # Correct name for DC
  vote_names[8] = "District of Columbia"
  clinton_votes = map(convert, select.xpath('td[17]/text()').extract())
  trump_votes = map(convert, select.xpath('td[18]/text()').extract())

  gen_votes = pd.DataFrame({"clinton": clinton_votes, "trump": trump_votes}, index=vote_names)

  trump_favor = pd.DataFrame(gen_votes["trump"]/gen_votes.sum(axis=1), columns=["trumpism"], index=vote_names)  
  voting = gen_votes.join(trump_favor).sort_values("trumpism", ascending=False)  
  voting = voting.drop("District of Columbia")

  # for pretty printing
  voting_space = pd.DataFrame([["------", "------", "------"]],index=["*SPACE*"], columns=voting.columns) 
  print(pd.concat([voting[:5], voting_space, voting[-5:].sort_values("trumpism")]))
#+END_SRC

#+RESULTS:
:RESULTS:
               clinton    trump  trumpism
Wyoming          55973   174419  0.757053
West Virginia   188794   489371  0.721611
North Dakota     93758   216794  0.698092
Oklahoma        420375   949136  0.693047
Idaho           189765   409055  0.683102
*SPACE*         ------   ------    ------
Hawaii          266891   128847  0.325587
California     8753788  4483810  0.338718
Vermont         178573    95369  0.348136
Massachusetts  1995196  1090893  0.353487
Maryland       1677928   943169  0.359838
:END:

** Preprocess Data
Some preprocessing to check data corrupted files
#+BEGIN_SRC ipython :session :file :results output raw drawer :noweb yes :exports both :tangle ./politics.py
  print "Data tests... \n\nAssertions Passed\n\n"

  # Confirm all expected regions and states present
  assert len(usa_raw["state"].unique()) == 52 # expected number of states
  assert len(usa_raw["region"].unique()) == 416  # expected number of regions
 
  # Confirm that there are no posts without regions/states. Not all CL 
  # regions have subregions, so it's okay for null subregions.
  assert len(usa_raw[usa_raw["state"].isnull()].index) == 0
  assert len(usa_raw[usa_raw["region"].isnull()].index) == 0

  # Find regions/subregions for which there are no posts
  postless_regions = usa_raw[usa_raw["title"].isnull()]  
  postless_regions_times = usa_raw[usa_raw["date"].isnull()]

  # not actually an effective test, but good enough
  assert len(postless_regions) == len(postless_regions_times)

  print(("{0:,} regions/subregions over {1} states without " + 
         "any posts.").format(len(postless_regions), postless_regions["state"].nunique()))  
#+END_SRC

#+RESULTS:
:RESULTS:
Data tests... 

Assertions Passed


58 regions/subregions over 32 states without any posts.
:END:

Drop unneeded data
#+BEGIN_SRC ipython :session :file  :exports both :tangle ./politics.py
# Drop empty regions.
usa = usa_raw.dropna(subset=["title", "date"], how="any", axis=0)
assert len(postless_regions) == len(usa_raw)-len(usa)

# Get rid of territories (Guam, Puerto Rico)
usa = usa[usa["state"] != "Territories"]
usa = usa[usa["state"] != "District of Columbia"]
#+END_SRC

#+RESULTS:

Confirm Census Data
#+BEGIN_SRC ipython :session :results output raw drawer :noweb yes  :exports both :tangle ./politics.py
assert set(usa.state.unique()) == set(census.index) and len(usa.state.unique() == len(census.index))

print "Census data complete"
#+END_SRC

#+RESULTS:
:RESULTS:
Census data complete
:END:

Confirm Election Data
#+BEGIN_SRC ipython :session :file :results output raw drawer :noweb yes :exports both :tangle ./politics.py
assert set(usa.state.unique()) == set(voting.index) and len(usa.state.unique() == len(voting.index))

print "Voting data complete"
#+END_SRC
#+RESULTS:
:RESULTS:
Voting data complete
:END:
* State Popularity
** Data
*** Grab Data
#+BEGIN_SRC ipython :session :file :results output raw drawer :noweb yes :exports both :tangle ./politics.py
  patronage = pd.DataFrame(usa.groupby('state').size(), columns=["patronage"]).sort_values(
      "patronage",ascending=False)

  print "Top ten most frequented states:\n{}".format(patronage[:10])
#+END_SRC 
#+RESULTS:
:RESULTS:
Top ten most frequented states:
              patronage
state                  
California         3808
Florida            3594
Texas              3147
New York           2341
Colorado           1982
Pennsylvania       1918
Arizona            1405
Ohio               1401
Washington         1378
Michigan           1366
:END:
State Usage table
#+BEGIN_SRC ipython :session :file :results output raw drawer :noweb yes :exports both :tangle ./politics.py
    cl_by_state = patronage.join(census, how="inner")
    usage = cl_by_state.apply(
        lambda df: df["patronage"] / float(df["population"]), axis=1)

    # Weight for max = 1.000
    usage_weighted = (usage - usage.min())/(usage.max() - usage.min())
    weighted_usage = pd.DataFrame((usage_weighted),
                                   columns=["usage"])

    state_usage = pd.concat([cl_by_state, weighted_usage],
                            axis=1).sort_values("usage",
                                                ascending=False)

#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython :session :file  :exports both
states = state_usage.join(voting, how="left").sort_values("usage")[:50]
#+END_SRC
#+RESULTS:

Useful for displaying several splices of a dataframe as a concatenation
#+BEGIN_SRC ipython :session :file  :results output raw drawer :noweb yes :exports both
  state_usage_space = pd.DataFrame([["------", "------", "------"]],index=["*SPACE*"],
                                   columns=state_usage.columns)

  print(pd.concat([state_usage[:5], state_usage_space, state_usage[-5:].sort_values("usage")]))
#+END_SRC
#+RESULTS:
:RESULTS:
             patronage population       usage
Colorado          1982    5029196           1
Hawaii             445    1360301     0.81696
Montana            286     989415     0.71289
Oregon            1094    3831074    0.703323
Nevada             770    2700551    0.702141
*SPACE*         ------     ------      ------
North Dakota        19     672591           0
Vermont             18     625741  0.00141296
Kansas             106    2853118   0.0243361
Wyoming             22     563626   0.0294766
New Jersey         400    8791894   0.0471436
:END:

** Analysis
*** Colorodo
#+BEGIN_SRC ipython :session :file ./img/py6320p6Z.png :exports both :tangle ./politics.py
top_five = state_usage.sort_values("patronage")[-5:][::-1]
fig = plt.figure() # Create matplotlib figure

ax = fig.add_subplot(111) # Create matplotlib axes
ax.spines['top'].set_visible(False)
ax2 = ax.twinx() # Create another axes that shares the same x-axis as ax.
width = 0.2

top_five.patronage.plot(kind='bar', color='#992255', ax=ax, width=width, position=1)
top_five.population.plot(kind='bar', color='#CC7733', ax=ax2, width=width, position=0)

ax.set_ylabel('Patronage')
ax2.set_ylabel('Population')

ax.get_xaxis().tick_bottom()
ax.get_yaxis().tick_left()
plt.rc('grid', linestyle="+", color='black')
plt.grid()
plt.show()
#+END_SRC

#+RESULTS:
[[file:./img/py6320p6Z.png]]

*** Patronage
#+BEGIN_SRC ipython :session :file ./img/py6320oYD.png :exports both :tangle ./politics.py
x = np.arange(len(state_usage))

plt.bar(x, state_usage.sort("population").patronage)
#+END_SRC

#+RESULTS:
[[file:./img/py6320oYD.png]]

*** Usage
**** Distribution
#+BEGIN_SRC ipython :session :file ./img/py6320LXp.png :exports both :tangle ./politics.py
ax = plt.subplot(111)  
ax.spines["top"].set_visible(False)  
ax.spines["right"].set_visible(False)  
    
ax.get_xaxis().tick_bottom()  
ax.get_yaxis().tick_left()  

plt.xlabel("Usage", fontsize=12)  
plt.ylabel("States", fontsize=12)     

plt.suptitle('Politics Usage Distribution', fontsize=14) 

plt.hist(state_usage.usage,
         color="#661111", bins=17)  
#+END_SRC

#+RESULTS:
[[file:./img/py6320LXp.png]]

**** Normalized state usage distributions
#+BEGIN_SRC ipython :session :file ./img/py6320jfT.png :exports both :tangle ./politics.py
norm_usage = (state_usage - state_usage.min()) / (state_usage.max() - state_usage.min())
norm_usage.plot(kind="density", title="Normalized PDF estimations", sharey=True)
#+END_SRC

#+RESULTS:
[[file:./img/py6320jfT.png]]

#+BEGIN_SRC ipython :session :file ./img/py6320Yhv.png :exports both :tangle ./politics.py
plt.plot(x, state_usage.population.sort_values().values)
#+END_SRC
#+RESULTS:
[[file:./img/py6320Yhv.png]]
I expect population to relate to patronage linearly.
#+BEGIN_SRC ipython :session :file ./img/py63201RB.png :exports both :tangle ./politics.py
# Getting rid of California
p1 = state_usage.sort_values("population", ascending=False)[5:]

plt.bar(p1["population"], p1["usage"])
#+END_SRC
 #+RESULTS:
 [[file:./img/py63201RB.png]]

*** Politics
**** Posts over Trumpism
#+BEGIN_SRC ipython :session :file /home/dodge/workspace/craig-politics/img/py22415X0p.png :exports both
post_politics = usa.join(states.trumpism, how="outer", on="state")
post_politics.filter(["trumpism", "state"]).plot(kind="hist", bins=10, color=["#FF9911"])
#+END_SRC
#+RESULTS:
[[file:/home/dodge/workspace/craig-politics/img/py22415X0p.png]]

**** Trumpism vs Usage
#+BEGIN_SRC ipython :session :file /home/dodge/workspace/craig-politics/img/py22415KxX.png :exports both
  post_politics.filter(["trumpism", "usage"]).plot(kind="hist")
#+END_SRC

#+RESULTS:
[[file:/home/dodge/workspace/craig-politics/img/py22415KxX.png]]

**** States/Usage 
Note the correlation between trumpism and usage
#+BEGIN_SRC ipython :session :results output raw drawer :noweb yes :exports both :tangle ./politics.py
print(states.filter(["patronage", "usage" , "normalized", "trumpism"]).corr())
#+END_SRC :tangle ./politics.py

#+RESULTS:
:RESULTS:
           patronage     usage  trumpism
patronage   1.000000  0.336453 -0.363133
usage       0.336453  1.000000 -0.301936
trumpism   -0.363133 -0.301936  1.000000
:END:

* Text Qualities
** Data
*** Words
Most popular words in English. Grabbed from http://www.world-english.org/english500.htm
#+BEGIN_SRC ipython :session :file  :exports both :tangle ./politics.py
pop_english_words = ["the", "re", "a", "s", "t", "i", "of", "to", "and", "and", "in", "is", "it", "you", "that", "he", "was", "for", "on", "are", "with", "as", "I", "his", "they", "be", "at", "one", "have", "this", "from", "or", "had", "by", "hot", "but", "some", "what", "there", "we", "can", "out", "other", "were", "all", "your", "shit", "when", "up", "use", "word", "how", "said", "an", "each", "she", "which", "do", "their", "time", "if", "will", "way", "about", "many", "fuck", "then", "them", "would", "write", "like", "so", "these", "her", "long", "make", "thing", "see", "him", "two", "has", "look", "more", "day", "could", "go", "come", "did", "my", "sound", "no", "most", "number", "who", "over", "know", "water", "than", "call", "first", "people", "may", "down", "side", "been", "now", "find"]
#+END_SRC

#+RESULTS:

#+BEGIN_SRC ipython :session :file ./img/py63203mB.png :exports both :tangle ./politics.py
  def post_words(df, no_pop=False):
      words = re.findall(r'\w+', df.title.apply(lambda x: x + " ").sum())
      if no_pop:
          # pop_english_words is a list of the most popular (and boring) English
          # words. E.g., "and", "to", "the", etc.
          words = [word for word in words if word not in pop_english_words]
      return  words

  def words(df=usa, no_pop=False):
      # word counts across all posts
      words = post_words(df, no_pop)
      word_counts = Counter([word.lower() for word in words])
      wcs = zip(*[[word, count] for word, count in word_counts.iteritems()])

      corpus = pd.Series(wcs[1], index=wcs[0]).rename("counts")

      return corpus.sort_values(ascending=False)
#+END_SRC
Probably don't care about stupid common words 
#+BEGIN_SRC ipython :session :file ./img/py6320H0c.png :exports both :tangle ./politics.py
from collections import Counter 

posts_corpus = words(df=usa, no_pop=True)

usa_words_full = post_words(df=usa)
usa_words = post_words(df=usa, no_pop=True)

posts_sum = " ".join([word for word in usa_words_full if word.lower() not in pop_english_words])
#+END_SRC
*** Substrings
Find substrings in posts
#+BEGIN_SRC ipython :session :file ./img/py6320WhL.png :exports both  :tangle ./politics.py
  def find_strs(substr, df=usa):
      """
      Get all titles from usa that have substr in their post title. Add some data on capitalization.
      """
      
      find = lambda s: (1 if re.search(substr, s, re.IGNORECASE) else np.nan)

      return df.title[df.title.map(find) == 1].rename("*" + substr + "*", inplace=True)

  def categ_strs(findings):
      """
      Return a list of 
      """
      s = findings.name[1:-1]
      find = lambda sub, string: (1 if re.search(sub, string) else np.nan)

      proper = findings.apply(lambda x: find(s[0].upper() + s[1:].lower(), x)).rename("proper")
      cap = findings.apply(lambda x: find(s.upper(), x)).rename("uppercase")
      low = findings.apply(lambda x: find(s.lower(), x)).rename("lower")

      return pd.concat([proper, cap, low], axis=1)

  def eval_strs(string, df=usa):
      findings = find_strs(string, df)
      return categ_strs(findings).join(findings)
#+END_SRC
** Analysis
*** General Language
Assign to each post their expected value for political leaning
#+BEGIN_SRC ipython :session :file ./img/py6320CcH.png :exports both :tangle ./politics.py
  post_politics = usa.join(voting, on="state").join(
      find_strs("trump"), how="inner")
#+END_SRC
#+BEGIN_SRC ipython :session :file  :exports both :tangle ./politics.py
  lib_words = words(df=post_politics[post_politics.trumpism < .45], no_pop=True).rename("libs")
  conserv_words = words(df=post_politics[post_politics.trumpism > .55], no_pop=True).rename("conservs")  
#+end_src

#+RESULTS:
Ratio
#+BEGIN_SRC ipython :session :results output raw drawer :noweb yes :exports both :tangle ./politics.py

  rat = lambda df: df.libs/df.conservs
  ratio = pd.DataFrame().join([lib_words[lib_words >= 10], conserv_words[conserv_words >= 10]],
                                      how="outer").apply(rat, axis=1).dropna()
  ratio = ratio.rename("dem/rep ratio")
  lib_con_ratio = pd.DataFrame(posts_corpus).join(ratio.sort_values(ascending=False), how="inner")
  lib_con_ratio.sort("dem/rep ratio", ascending=False, inplace=True)
  print(lib_con_ratio[:10])
  #lib_con_ratio = posts_corpus.join(lib_con_ratio.sort_values(ascending=False), on="words")
#+END_SRC

#+RESULTS:
:RESULTS:
           counts  dem/rep ratio
against       346       5.000000
won           320       4.461538
sign          262       3.363636
voted         223       2.375000
not           993       2.000000
get           480       1.615385
trump        5071       1.366133
america       784       1.363636
appoints       37       1.083333
president     654       1.075758
:END:

*** Trumps
**** Patronage
#+BEGIN_SRC ipython :session :file ./img/py6320Qlq.png :exports both :tangle ./politics.py
trumps = eval_strs("trump").join(usa.state, how="inner")
trumps_by_state = trumps.groupby("state").count().join(states).drop(["clinton", "trump"], axis=1)
up_over_trumps = (trumps_by_state.uppercase/trumps_by_state["*trump*"]).rename("uppercase usage")
prop_over_trumps = (trumps_by_state.proper/trumps_by_state["*trump*"]).rename("propercase usage")
trumps_over_pat = (trumps_by_state["*trump*"]/trumps_by_state.patronage).rename("trumps usage")
trumps_by_state = trumps_by_state.join([prop_over_trumps, up_over_trumps, trumps_over_pat], how="outer")
#+END_SRC
**** Politics
The more pro-Trump your state, the less likely you are to use "Trump" over "TRUMP"
#+BEGIN_SRC ipython :session :file ./img/py6320cup.png :exports both :tangle ./politics.py
trumps_vs_trumpism = trumps_by_state.filter(["trumpism", "propercase usage", "uppercase usage", "trumps usage"]).sort_values("trumps usage", ascending=True)[1:]

pd.DataFrame.hist(trumps_vs_trumpism, bins=50)
#plt.hist([prop_over_cap.trumpism, prop_over_cap[""]], bins=30)
#+END_SRC

#+RESULTS:
[[file:./img/py6320cup.png]]

#+BEGIN_SRC ipython :session :exports both :results output raw drawer :noweb yes
trump_posts = usa.join(voting, on="state").join(find_strs("trump"), how="outer")

print "Selecting states that are espectially anti-trump:\n{0}".format(trump_posts[trump_posts.trumpism < .4].title.sample(10))

print "\nPolitically liberal states composing the above sampling:\n{0}".format(trump_posts[trump_posts.trumpism < .4].groupby("state").sum().index.tolist())
#+END_SRC

#+RESULTS:
:RESULTS:
Selecting states that are espectially anti-trump:
13151                                                  2016 ELECTION RESULTS
14688                                                   Mrs Brady found dead
28531     United Nations Exposed For  Corruption Military Industrial Complex
27489                                               Compromise is our friend
28973                                                      LAND OF CONFUSION
14159                                CLINTON CRIME FAMILY MUST BE TAKEN DOWN
11717                                        Any type of cellphones oddities
11438                                         It's a Beautiful Day in the OC
4054          RE-APX. 90% OF AMERICANS ON DISABILITY ARE NOT REALLY DISABLED
12576    Life as War for Wall St Profit Baby Butchers Zuck Invest in Defense
Name: title, dtype: object

Politically liberal states composing the above sampling:
['California', 'Hawaii', 'Maryland', 'Massachusetts', 'New York', 'Vermont']
:END:

**** Trump Language
#+BEGIN_SRC ipython :session :file ./img/py63202C2.png :exports both :tangle ./politics.py
# not really ever used
trump_words = ["liberals",
               "conservatives",
               "centipede",
               "cuck",
               "maga",
               "regressive left",
               "shillary",
               "sjw",
               "triggered"]
#+END_SRC
**** word cloud
#+BEGIN_SRC ipython :session :file ./img/py6320RCC.png :exports both :tangle ./politics.py
from os import path
from PIL import Image

from wordcloud import WordCloud, STOPWORDS

d = path.dirname(".")

trump_mask = np.array(Image.open(path.join(d, "img/Trump_silhouette.png")))

stopwords = set(STOPWORDS)

wc = WordCloud(background_color="white", max_words=2000, mask=trump_mask,
               stopwords=stopwords)


# generate word cloud
wc.generate(posts_sum)

# save to file
wc.to_file(path.join(d, "img/Trump_test.png"))

# show
plt.imshow(wc)
plt.axis("off")
plt.figure()
plt.imshow(trump_mask, cmap=plt.cm.gray)
plt.axis("off")
plt.show()
#+END_SRC

#+RESULTS:
[[file:./img/py6320RCC.png]]

*** Unicode
ascii vs. unicode usage. 
#+BEGIN_SRC ipython :session :file  :exports both :tangle ./politics.py
def check_ascii(post):
    """
    Determines whether a title is encodable as ascii
    """
    try:
        post.encode('ascii')
        return True
    except UnicodeError:
        return False

ascii_titles_tv = usa.title.apply(check_ascii)
ascii_posts = usa[ascii_titles_tv]
nonascii_posts = usa[~ascii_titles_tv]

distinct_states = nonascii_posts["state"].unique()
print ("{0:,} of {1:,} total posts were non-ascii ({2:.2f}%), confined to {3} "
       + "states.").format(len(nonascii_posts),
                       len(usa),
                       len(nonascii_posts)/float(len(usa)) * 100,
                       len(distinct_states))
#+END_SRC

#+RESULTS:

**** Pennsylvania
Pennsylvania has was the preeminent outlier in non-ascii usage per-state
#+BEGIN_SRC ipython :session :file  :exports both :tangle ./politics.py :results output raw drawer :noweb yes 
nonascii_states_count = nonascii_posts.groupby(
    "state").title.nunique().sort_values(ascending=False)
print "\nTop ten most popular unicode states:"
print nonascii_states_count[:10]

pennsylvania = nonascii_posts[nonascii_posts["state"] == "Pennsylvania"]
print pennsylvania["title"].tolist()[0]

print("\nA single Trump memester seems to be responsible for the chaos " +
      "in Pennsylvania.\n" + "I suspect that these crazy unicode posts " +
      "are mostly done by a very small\nset of people, though there is " +
      "no way to tell.")
print "\nRandom sample of 5 non-ascii Pennsylvania posts"
print pennsylvania["title"][:5]

pennsylvania.groupby("region").count()

post_uniqueness = pennsylvania.title.nunique()/float(len(pennsylvania.title))

post_uniqueness
#+END_SRC

#+RESULTS:
:RESULTS:

Top ten most popular unicode states:
state
Pennsylvania    23
New York         9
Maryland         7
Florida          5
California       5
Arizona          4
Texas            4
Washington       4
New Jersey       3
Alabama          2
Name: title, dtype: int64
ðŸ™ŠðŸ™‰The ZOMBIES Are ComingðŸ™‰ðŸ™Š

A single Trump memester seems to be responsible for the chaos in Pennsylvania.
I suspect that these crazy unicode posts are mostly done by a very small
set of people, though there is no way to tell.

Random sample of 5 non-ascii Pennsylvania posts
18398           ðŸ™ŠðŸ™‰The ZOMBIES Are ComingðŸ™‰ðŸ™Š
18410    ðŸ’¥DONALD J. TRUMPðŸ’¥[Need a Tissue Anyone]
18418           ðŸ�µðŸ™‰The Zombies Are ComingðŸ�µðŸ™‰
18426       ðŸ‘‘HAPPY NEW YEARSðŸ‘‘DONALD J. TRUMPðŸ‘‘
18430           ðŸ™ŠðŸ™‰The ZOMBIES Are ComingðŸ™‰ðŸ™Š
Name: title, dtype: object
:END:

***** Colorado
#+BEGIN_SRC ipython :session :file  :exports both :tangle ./politics.py :results output raw drawer :noweb yes
print "{0} regions in Colorado".format(usa[usa['state'] == "Colorado"]["region"].nunique())
#+END_SRC

#+RESULTS:
:RESULTS:
7 regions in Colorado
:END:
#+BEGIN_SRC ipython :session :results output raw drawer :noweb yes :exports both :tangle ./politics.py
print(len(usa.groupby("state")["title"].agg(sum)["Kansas"]))
#+END_SRC

#+RESULTS:
:RESULTS:
3112
:END:

      
*** Semantics
#+BEGIN_SRC ipython :session :file ./img/py63201WL.png :exports both :tangle ./politics.py
  from textblob import TextBlob

  def semants(text):
      blob = TextBlob(text)
      ss = 0
      for sentence in blob.sentences:
          ss += sentence.sentiment.polarity

      return float(ss)/len(blob.sentences)
#+END_SRC
#+BEGIN_SRC ipython :session :file ./img/py63202Qe.png :exports both :tangle ./politics.py
semantics = ascii_posts.title.map(lambda x: semants(x)).rename("semants")
semant = eval_strs("trump", df=ascii_posts).join(pd.DataFrame(semantics))
sems_usa = semant.join(usa, how="inner")
trumps_semantics = sems_usa.groupby("state").mean().join(voting, how="inner").sort_values("semants").corr()
#+END_SRC
#+BEGIN_SRC ipython :session :file ./img/py6320Dbk.png :exports both :tangle ./politics.py
total_semants = usa.join(semantics, how="outer").groupby("state").mean().join(voting).sort_values("semants").corr()
#+END_SRC
#+BEGIN_SRC ipython :session :file /home/dodge/workspace/craig-politics/img/py22415wVX.png :exports both

#+END_SRC

#+RESULTS:
[[file:/home/dodge/workspace/craig-politics/img/py22415wVX.png]]

