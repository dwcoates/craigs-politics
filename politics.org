#+HTML_HEAD: <link href="/home/dodge/.emacs.d/leuven-theme.css" rel="stylesheet">
#+TITLE: Scraping the Bottom of the Barrel

#+OPTIONS: toc:nil num:nil
#+TABLFM: $0;%0.3f


# <h1 align="center"><font color="0066FF" size=110%>Simple Notebook</font></h1>

* TODO stuff todo [5/6] :noexport:
** DONE Corpus is broken. Including non-pop words
** DONE Make thesis more clear

** DONE Stop using the word "generally"
** DONE Consider hiding code for diagrams. It isnt interesting.
** TODO Make sure diagrams are properly detailed [0/1]
*** TODO The correlation diagram needs to say describe color value

** DONE Add a sample of the data for the introduction

** TODO Find next highest number of words equal to trump instances
** TODO Add small description of scraping process with sample code
** TODO Fix how D.C. is removed
in voting, and in preprocessing, and in census
** TODO Add sources for Denver/NYC population stuff
- how to do this?
** TODO Population vs Patronage graph
- should be a scatter plot, where the color of the dots is a greyscale of usage.
- That or a 2d histogram
** TODO Demonstrate trumpism by population vs trumpism by posts
- basically demonstrates liberal usage of craigslist politics
** TODO lib words vs conserv words needs a revamp
- see "THIS IS BROKEN AND BAD"
** TODO How can I weight the dems for trumpism distribution?
dems show up more in posts, but like, there are more of them. Wait,
not there aren't. They're about half of the country, right? Why am I
weighting again? Maybe just for good measure, but really, I can get
away with only a couple of points between them
* Setup Code :noexport:
General settings, packages and functions.
#+BEGIN_SRC ipython :session :exports results :tangle ./politics.py
  %matplotlib inline
  import numpy as np
  import scipy
  from scipy import stats
  import matplotlib as mpln
  import matplotlib.pyplot as plt
  import matplotlib.cm as cm
  import pandas as pd

  from tabulate import tabulate

  import pprint as pp
  import pickle
  import re

  pd.options.display.max_colwidth = 1000

  def print_df(df, headers="keys", rnd=100, dis_parse=False):
      """
      Pretty print DataFrame in an org table. Org tables are good.
      They also export nicely.
      """
      print(tabulate(df.round(rnd),
                     tablefmt="orgtbl",
                     headers=headers,
                     disable_numparse=dis_parse))
#+END_SRC
#+RESULTS:

* Introduction
For my web scraping project, I've chosen to extract some of the
politics data from craigslist.org. My original ambition, though it
proved difficult to affirm, was to prove a small, non-existant, or
negative correllation of pro-trump chatter to expected
conservatism. That is, I suspected that, somewhat counter-intuitively,
the politics sections of more conservative states would a
disproportionately less likely source of pro-trump posts. My basis for
this suspicion was my general observation that less regulated areas
for discussion on the internet tend to be very attractive to those
members of a publically socially disparaged minority. Recognizing
that, among other clues, Trump supporters in largely pro-Clinton
geographic areas are disparaged for their support in amounts
disproportionate to their surprisingly high representation, it
followed that I could expect a surprising amount of pro-Trump (mostly
trollish) chatter in mostly liberal places (e.g., New York City). The
positive sentiment aspect of that hypothesis proved to be difficult to
convincingly affirm. More generally, also I sought to analyze the
trends of politcs discussion of craiglist, mostly in the area of text
usage (capitalization, word frequency, etc) vs political leaning.

[[./img/Trump_cloud_proper.png]]

* Methodology
To extract data from craigslist, I used the Python Scrapy package,
which was probably overkill. Originally, I intended to collect post
bodies as well as the titles, however this would require about 100
times as many request, too many for me to reponsibly exectute in a
reasonable amount of time. I resigned to limiting myself to titles,
which involved about 500 requests, spread over 5 hours, to obtain
roughly 40,000 posts titles/times. For each of these titles, there is
a corresponding state and region, with some regions additionally
divided into subregions (the New York City region, for example,
consists of Brooklyn, Queens, Manhattan, etc). Each post, its time and
its geographical origin are represented with a single row in a 40k row
Pandas DataFrame, ~usa~. Data corruption was not an issue, as the CL
layout is quite uniform, though I did need to take into account data
redundancy (e.g., occaisionally "regions" are also "subregions" of
sibling regions). To make use of the extracted post title data, I
employed the 2010 U.S. census, which is available from
http://www.census.gov, as well as the 2016 election results data,
which I scraped from http://uselectionatlas.org/ using a BeautifulSoup
extraction script.

* Preparing data
** TODO Craigcrawler :noexport:
** Grab CL Data  :ignore:
Data is read from file that craigcrawler built
#+BEGIN_SRC ipython :session :exports none :tangle ./politics.py
usa_raw = pd.read_csv("data/us.csv", index_col=0)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC ipython :session :file :exports none  :tangle ./politics.py
post_count_total_raw = len(usa_raw)
post_count_by_state_raw = usa_raw.groupby("state").count()["title"]#.sort_values(ascending=False)
post_count_by_region_raw = usa_raw.groupby("region").count()["title"]#.sort_values(ascending=False)
#+END_SRC

#+RESULTS:

Data synopsis:
#+BEGIN_SRC ipython :session :file  :results output raw drawer :noweb yes :exports results  :tangle ./politics.py
  print ("\n{0:,} total posts exctracted from {3:,} regions over {4} "+
         "state. The most popular\nstate was {1}, and the most " +
         "popular region was, surprisingly, {2}.").format(post_count_total_raw,
                                                          post_count_by_state_raw.index[0],
                                                          post_count_by_region_raw.index[0],
                                                          len(post_count_by_region_raw),
                                                          len(post_count_by_state_raw))
#+END_SRC
#+RESULTS:
:RESULTS:

38,692 total posts exctracted from 416 regions over 52 state. The most popular
state was Alabama, and the most popular region was, surprisingly, SF bay area.
:END:
*** ~usa~ Sample
Sample of posts in the ~usa~ DataFrame before preprocessing, which is the DF for storing all CL politics posts:
#+BEGIN_SRC ipython :session :exports results :results output raw drawer :noweb yes
# This can fail because tabulate can't handle unicode.
# There's only about a 2.5% chance if fails on a given execution, though.
print_df(usa_raw.sample(3), rnd=3)
#+END_SRC
#+RESULTS:
:RESULTS:
|       | title                                                 | date             | state      | region         | subregion |
|-------+-------------------------------------------------------+------------------+------------+----------------+-----------|
| 36652 | Re: Idiot Supports Trump                              | 2016-11-28 17:09 | Texas      | laredo, TX     |       nan |
| 14574 | Re Obama legacy.....more Marxism..                    | 2016-12-26 23:43 | California | ventura county |       nan |
| 32783 | Little PANSYASS McVeigh and The Redneck Nation SUCKS! | 2016-12-28 07:06 | Washington | seattle-tacoma |    tacoma |
:END:

** U.S. Census 2010
*** Geo Keys   :noexport:
#+BEGIN_SRC ipython :session  :exports none :tangle ./politics.py
# Keys for geography stuff. Table is an index table.
# These keys are used as index for census table.
GEO_NAME = "GEO.display-label"
GEO_KEY = "GEO.id"

state_keys = pd.read_csv("data/census/DEC_10_DP_G001_with_ann.csv")[1:].set_index(GEO_KEY)

state_keys = state_keys.filter([GEO_NAME])[:52]
state_keys = state_keys[state_keys[GEO_NAME]!= "Puerto Rico"]
#+END_SRC

#+RESULTS:

*** Census Data
#+BEGIN_SRC ipython :session :exports none :tangle ./politics.py
  # keys for the census data. Only really care about two of them (there are hundreds):
  TOT_NUM_ID = "HD01_S001" # total number key
  TOT_PER_ID = "HD02_S001" # total percent key
#+end_src

#+RESULTS:


#+begin_src ipython :session  :exports none :tangle ./politics.py
  census_all = pd.read_csv("data/census/DEC_10_DP_DPDP1_with_ann.csv")[1:].set_index(GEO_KEY)
#+end_src

#+RESULTS:

#+begin_src ipython :session  :exports none :tangle ./politics.py
  census_all = census_all.filter([TOT_NUM_ID])
  census_all = census_all.join(state_keys, how="right")
  census_all.columns = ["population", "state"]
  census_all.set_index("state", inplace=True)

  def correct_stat(s):
      """
      Some states have extra information for population.
      Example: 25145561(r48514)
      """
      loc = s.find("(")
      return int(s[:loc] if loc > 0 else s)

  census_all.population = census_all.population.apply(correct_stat)

  census = census_all.drop("District of Columbia")
#+end_src

#+RESULTS:

Census data is collected from U.S. Census Bureau for 2010 census. Here's a sample:
#+begin_src ipython :session :results output raw drawer :noweb yes :exports results :tangle ./politics.py
print_df(census.sample(4), rnd=3)
#+END_SRC

#+RESULTS:
:RESULTS:
| state     |  population |
|-----------+-------------|
| Utah      | 2.76388e+06 |
| Nebraska  | 1.82634e+06 |
| Minnesota | 5.30392e+06 |
| Nevada    | 2.70055e+06 |
:END:
** U.S. 2016 Election
The 2016 Election results will be useful. They are grabbed from a really nice site, [[http://uselectionatlas.org/RESULTS/data.php?year%3D2016&datatype%3Dnational&def%3D1&f%3D1&off%3D0&elect%3D0][uselectionsatlas.org]].
#+BEGIN_SRC ipython :session :exports code :tangle ./politics.py
  import requests
  from scrapy import Selector

  atlas_url = ("http://uselectionatlas.org/RESULTS/data.php?year" +
               "=2016&datatype=national&def=1&f=1&off=0&elect=0")
  atlas_source = requests.get(atlas_url).text
  select = Selector(text=atlas_source).xpath('//*[@id="datatable"]/tbody/tr')

  convert = lambda s: int(s.replace(',', ''))
  vote_names = map(str, select.xpath('td[3]/a/text()').extract())
  # Correct name for DC
  vote_names[8] = "District of Columbia"
  clinton_votes = map(convert, select.xpath('td[17]/text()').extract())
  trump_votes = map(convert, select.xpath('td[18]/text()').extract())

  gen_votes = pd.DataFrame({"clinton": clinton_votes, "trump": trump_votes},
                           index=vote_names)

  # Dub a states Rebublican vote rate "trumpism"
  trump_favor = pd.DataFrame(gen_votes["trump"]/gen_votes.sum(axis=1),
                             columns=["trumpism"],
                             index=vote_names)
  voting = gen_votes.join(trump_favor).sort_values("trumpism", ascending=False)
  voting = voting.drop("District of Columbia")
#+end_src

#+RESULTS:

Sample of voting table:
#+begin_src ipython :session :results output raw drawer :noweb yes :exports results :tangle ./politics.py
  # for pretty printing
  voting_space = pd.DataFrame([["------", "------", "------"]],index=["*SPACE*"],
                              columns=voting.columns)
  print_df(pd.concat([voting[:3].round(3), voting_space, voting[-3:].round(3).sort_values("trumpism")]),
           rnd=3)
#+END_SRC

#+RESULTS:
:RESULTS:
|               | clinton |   trump | trumpism |
|---------------+---------+---------+----------|
| Wyoming       |   55973 |  174419 |    0.757 |
| West Virginia |  188794 |  489371 |    0.722 |
| North Dakota  |   93758 |  216794 |    0.698 |
| *SPACE*       |  ------ |  ------ |   ------ |
| Hawaii        |  266891 |  128847 |    0.326 |
| California    | 8753788 | 4483810 |    0.339 |
| Vermont       |  178573 |   95369 |    0.348 |
:END:

** Preprocess Data
A small bit of preprocessing to check data for corruption and
unexpected results was necessary. There was no missing data, and no
corruption. I suspected that I might encounter some amount of
redundancy, but the extractor was written to exclude duplicated links,
and it happened to be the case that CL keeps areas unique across highly
related (sub)regions (e.g., "long island" subregion and "long island,
NY" region seem like they might be the same, but are actually
completely distinct).
#+BEGIN_SRC ipython :session :exports none :tangle ./politics.py
  print "Data tests... \n\nAssertions Passed\n\n"

  # Confirm all expected regions and states present
  assert len(usa_raw["state"].unique()) == 52 # expected number of states (D.C., Territories)
  assert len(usa_raw["region"].unique()) == 416  # expected number of regions

  # Confirm that there are no posts without regions/states. Not all CL
  # regions have subregions, so it's okay for null subregions.
  assert len(usa_raw[usa_raw["state"].isnull()].index) == 0
  assert len(usa_raw[usa_raw["region"].isnull()].index) == 0

  # Find regions/subregions for which there are no posts
  postless_regions = usa_raw[usa_raw["title"].isnull()]
  postless_regions_times = usa_raw[usa_raw["date"].isnull()]

  # Not actually a good test, but good enough
  assert len(postless_regions) == len(postless_regions_times)
#+end_src

#+RESULTS:

#+begin_src ipython :session :results output raw drawer :noweb yes :exports none :tangle ./politics.py
  print(("{0:,} regions/subregions over {1} states without " +
         "any posts.").format(len(postless_regions), postless_regions["state"].nunique()))
#+END_SRC

#+RESULTS:
:RESULTS:
58 regions/subregions over 32 states without any posts.
:END:

#+BEGIN_SRC ipython :session  :exports none :tangle ./politics.py
# Drop empty regions. Some regions are too small to have any posts.
usa = usa_raw.dropna(subset=["title", "date"], how="any", axis=0)
assert len(postless_regions) == len(usa_raw)-len(usa)

# Get rid of territories (Guam, Puerto Rico).
usa = usa[usa["state"] != "Territories"]
# Get rid of "District of Columbia"
usa = usa[usa["state"] != "District of Columbia"]
#+END_SRC
#+RESULTS:

#+BEGIN_SRC ipython :session  :exports none :tangle ./politics.py
# Confirm census data
assert set(usa.state.unique()) == set(census.index) and len(usa.state.unique() == len(census.index))

print "Census data complete"
#+end_src

#+RESULTS:

#+BEGIN_SRC ipython :session :exports none :tangle ./politics.py
# Confirm election data
assert set(usa.state.unique()) == set(voting.index) and len(usa.state.unique() == len(voting.index))

print "Voting data complete"
#+end_src

#+RESULTS:

* State Usage
** intro :ignore:
Although the post data has attached a fairly fine-grain geographical
description, I found the CL regions in general to not line up well
with any census bureau categories. Moreover, even in the lucky event
of such name correspondence, the division of regions was at least
questionable. For example, by far the datasets most prominent ~state~
outlier, District of Columbia, has a census population of about 600k,
yet a practical metropolitan area population in the several millions,
a disparity that grossly skews its contributions to state-wide
statistics. Because of reasons like this, regions and subregions were
largely found to be unmanageably tedious to consider seriously in any
analysis. States, however, having relatively little difference  between
practical occupancy and census population, and have indisputable
borders, are ideal for inspection (other than D.C., of course).
** Terms
1. ~patronage~
   Patronage is the raw number of posts on a politics board.
2. ~usage~
   Usage is my measure for a states proportional interest in the
   politics board. It is simply the normalized ratio of patronage and
   state population.
3. ~Trumpism~
   Trumpism is the name for a states republican vote percentage in the
   general election. It is used as a rough measure of how pro-Trump
   rate of a given state, and is a column in the ~voting~ DataFrame,
   which is comprised of scraped data on the 2016 General Election
   results.
** Organize Data :ignore:
#+BEGIN_SRC ipython :session :results output raw drawer :noweb yes :exports none :tangle ./politics.py
  patronage = pd.DataFrame(usa.groupby('state').size(), columns=["patronage"]).sort_values(
      "patronage",ascending=False)

  print("Top ten most frequented states:\n")
  print_df(patronage[:10])
#+END_SRC

#+RESULTS:
:RESULTS:
Top ten most frequented states:

| state        |   patronage |
|--------------+-------------|
| California   |        3808 |
| Florida      |        3594 |
| Texas        |        3147 |
| New York     |        2341 |
| Colorado     |        1982 |
| Pennsylvania |        1918 |
| Arizona      |        1405 |
| Ohio         |        1401 |
| Washington   |        1378 |
| Michigan     |        1366 |
:END:

The ~state_usage~ table is the census table concatenated with
~patronage~ and ~usage~, grouped by ~state~.
#+BEGIN_SRC ipython :session :exports none :tangle ./politics.py
  cl_by_state = patronage.join(census, how="inner")
  usage = cl_by_state.apply(
      lambda df: df["patronage"] / float(df["population"]), axis=1)

  # Weight for max = 1.000
  usage_weighted = (usage - usage.min())/(usage.max() - usage.min())
  weighted_usage = pd.DataFrame((usage_weighted),
                                 columns=["usage"])
  state_usage = pd.concat([cl_by_state, weighted_usage],
                          axis=1).sort_values("usage",
                                              ascending=False)
#+end_src

#+RESULTS:

#+BEGIN_SRC ipython :session :results output raw drawer :noweb yes :exports none
  # Just some printing

  # Useful for displaying several splices of a dataframe as a concatenation
  state_usage_space = pd.DataFrame([["------", "------", "------"]],index=["*SPACE*"],
                                   columns=state_usage.columns)

  print_df(state_usage.sample(3))
#+END_SRC
#+RESULTS:
:RESULTS:
| state    | patronage |  population |    usage |
|----------+-----------+-------------+----------|
| Nebraska |       244 | 1.82634e+06 | 0.287964 |
| New York |      2341 | 1.93781e+07 | 0.252993 |
| Montana  |       286 |      989415 |  0.71289 |
:END:

*** ~states~ Sample

Joining ~state_usage~ with ~voting~ gives us a decent top down view of
state political tendencies on CL.
#+BEGIN_SRC ipython :session :exports code
  states = state_usage.join(voting, how="left").sort_values("usage")
#+END_SRC

#+RESULTS:

#+BEGIN_SRC ipython :session :exports results :results output raw drawer :noweb yes
  print_df(states.sample(3), dis_parse=True)
#+END_SRC
#+RESULTS:
:RESULTS:
| state         | patronage | population |          usage |  clinton |    trump |       trumpism |
|---------------+-----------+------------+----------------+----------+----------+----------------|
| New Hampshire |     260.0 |    1316470 | 0.462618491522 | 348526.0 | 345790.0 | 0.498029715576 |
| New Mexico    |     428.0 |    2059179 | 0.490914182965 | 385234.0 | 319666.0 | 0.453491275358 |
| Wyoming       |      22.0 |     563626 | 0.029476604392 |  55973.0 | 174419.0 | 0.757053196292 |
:END:
** Outliers
There are two major outlying states in the dataset: Colorodo and
District of Columbia.
*** Colorodo
We can see from the following that Colorado is an extreme outlier,
being the fifth most patroned state, yet the 23rd most populous.
#+BEGIN_SRC ipython :session :file ./img/py6320WCb.png :exports results
top_five = state_usage.sort_values("patronage")[-5:][::-1]
fig = plt.figure() # Create matplotlib figure

ax = fig.add_subplot(111) # Create matplotlib axes
ax2 = ax.twinx() # Create another axes that shares the same x-axis as ax.

width = 0.2

top_five.patronage.plot(kind='bar', color='#992255', ax=ax, width=width, position=1)
top_five.population.plot(kind='bar', color='#CC7733', ax=ax2, width=width, position=0)

ax.set_ylabel('Patronage')
ax2.set_ylabel('Population')

plt.show()
#+END_SRC

#+RESULTS:
[[file:./img/py6320WCb.png]]

Denver, as a region, is also especially large. Despite having a
population of 650,000 people (and a metropolitcan area of 3 million),
Denver sees a patronage of 1187.
#+BEGIN_SRC ipython :session :results output raw drawer :noweb yes  :exports none
print(len(usa[usa.region == "denver, CO"]))
#+END_SRC

#+RESULTS:
:RESULTS:
1187
:END:

By comparison, the "new york city" region, which is expansive enough
as to include metropolitan area subregions like "new jersey", "long island",
"fairfield", etc, has fewer posts, at 1006.
#+BEGIN_SRC ipython :session :results output raw drawer :noweb yes :exports results
  nyc_subregions = usa.groupby("region").get_group(
      "new york city").subregion.unique().tolist()
  num_nyc_posts = len(usa[usa.region == "new york city"])
  den_nyc_rat = (num_nyc_posts/8406000.0)/(len(usa[usa.region == "denver, CO"])/649495.0)

  print("{} posts in NYC spread over ".format(num_nyc_posts) +
         ', '.join('{}'.format(r) for r in nyc_subregions[:-1]) +
        (", and {}. This is ~{:.1f}% the usage rate of " +
         "Denver").format(nyc_subregions[-1], den_nyc_rat*100))
#+END_SRC
#+RESULTS:
:RESULTS:
1006 posts in NYC spread over manhattan, brooklyn, queens, bronx, staten island, new jersey, long island, westchester, and fairfield. This is ~6.5% the usage rate of Denver
:END:

This is a remarkably popular region, clearly. I suspect that it has to
do with the region granularity CL mostly likely arbitrarily assigned
to the state. They might want to consider providing mode regions to
the state of Colorado.
*** District of Columbia
While I found Colorado to be an inexplicable anamoly, it was also
justifiably accurate. District of Columbia, having a Republican voting
rate of ~4% and the usage similar to that of Colorado, coupled with
it's unclear geographic distinction and population, meant its results
were too extreme and variable to not mostly exclude from the
dataset. Besides, it's not even a real state...

** Patronage
#+BEGIN_SRC ipython :session :exports none :tangle ./politics.py
# The range of fifty states (one to fifty, duh)
x = np.arange(len(state_usage))
#+end_src

#+RESULTS:

#+begin_src ipython :session :file ./img/py6320oYD.png :exports results :tangle ./politics.py
ax = plt.subplot(111)
ax.spines["top"].set_visible(False)
ax.spines["right"].set_visible(False)

ax.get_xaxis().tick_bottom()
ax.get_yaxis().tick_left()

plt.xlabel("States", fontsize=12)
plt.ylabel("Patronage", fontsize=12)

plt.suptitle('Patronage by state in order of population', fontsize=14)

plt.bar(x, state_usage.sort("population").patronage, color="#550000")
#+END_SRC

#+RESULTS:
[[file:./img/py6320oYD.png]]

** Usage
We can get a feel for the usage distribution by taking a look at the
following sample from the ~state_usage~ table:
#+BEGIN_SRC ipython :session :results output raw drawer :noweb yes  :exports results
  print_df(pd.concat([state_usage[:5].round(3),
                       state_usage_space,
                       state_usage[-5:].sort_values("usage").round(3)]))
#+END_SRC
#+RESULTS:
:RESULTS:
|              | patronage | population |  usage |
|--------------+-----------+------------+--------|
| Colorado     |      1982 |    5029196 |    1.0 |
| Hawaii       |       445 |    1360301 |  0.817 |
| Montana      |       286 |     989415 |  0.713 |
| Oregon       |      1094 |    3831074 |  0.703 |
| Nevada       |       770 |    2700551 |  0.702 |
| *SPACE*      |    ------ |     ------ | ------ |
| North Dakota |        19 |     672591 |    0.0 |
| Vermont      |        18 |     625741 |  0.001 |
| Kansas       |       106 |    2853118 |  0.024 |
| Wyoming      |        22 |     563626 |  0.029 |
| New Jersey   |       400 |    8791894 |  0.047 |
:END:

Seemingly some correlation between low population and low usage is
evident from this table. However, the states for which the politics
board is most popular are also fairly small. It may be that the
popularity doesn't relate to state size, directly, but to political
orientation, which itself correlated with state population. I suspect
that political discussion is most charged currently in Democratic
states, where discenting opinion is the that held by the triumphant
party. It may also be that board popularity relation to patronage is
non-linear. This correlation is explored more by some political
investigation.
#+BEGIN_SRC ipython :session :file ./img/py6320LXp.png :exports results :tangle ./politics.py
ax = plt.subplot(111)
ax.spines["top"].set_visible(False)
ax.spines["right"].set_visible(False)

ax.get_xaxis().tick_bottom()
ax.get_yaxis().tick_left()

plt.xlabel("Usage", fontsize=12)
plt.ylabel("States", fontsize=12)

plt.suptitle('Usage Distribution for CL politics board', fontsize=14)

plt.hist(state_usage.usage,
         color="#661111", bins=17)
#+END_SRC

#+RESULTS:
[[file:./img/py6320LXp.png]] 

We can take a look at usage, the ratio distribution of patronage and
population, by normalizing patronage and popoulation and overlaying
their distribution estimations:
#+BEGIN_SRC ipython :session :exports code :tangle ./politics.py
  min_norm = state_usage - state_usage.min()
  range_norm = state_usage.max() - state_usage.min()
  norm_usage = min_norm / range_norm
#+END_SRC

#+BEGIN_SRC ipython :session :file ./img/py6320jfT.png :exports results :tangle ./politics.py
  norm_usage.plot(kind="density", 
                  title="Normalized PDF estimations w/ means",
                  sharey=True)

  ax = plt.subplot(111)

  ax.spines["top"].set_visible(False)
  ax.spines["right"].set_visible(False)

  ax.get_xaxis().tick_bottom()
  ax.get_yaxis().tick_left()

  pat_mean = norm_usage.mean().patronage
  pop_mean = norm_usage.mean().population
  usage_mean = norm_usage.mean().usage

  plt.axvline(pat_mean, color='b', linestyle='dashed', linewidth=.5)
  plt.axvline(pop_mean, color='g', linestyle='dashed', linewidth=.5)
  plt.axvline(usage_mean, color='r', linestyle='dashed', linewidth=.5)


#+END_SRC

#+RESULTS:
[[file:./img/py6320jfT.png]]

These are estimations, so they extend beyond 0 and 1 on the
graph.

We can see that usage has less variance than patronage and population,
which we should expect. Perhaps it is somewhat more than expected,
however.

#+BEGIN_SRC ipython :session :results output raw drawer :noweb yes :exports results
  stats = pd.DataFrame({"mean": norm_usage.mean(),
                        "median": norm_usage.median()})
  print("Mean/median of normalized state usage metrics:\n{0}")
  print_df(stats)
#+end_src
#+RESULTS:
:RESULTS:
Mean/median of normalized state usage metrics:
{0}
|            |     mean |    median |
|------------+----------+-----------|
| patronage  | 0.197488 | 0.0915567 |
| population | 0.152608 | 0.105552  |
| usage      | 0.264764 | 0.20374   |
:END:


Here we can see illustrated what's been already hinted at: the states
with the most and least usage are generally less populated and less
patronaged, and, of course, there is a tight correlation between
patronage and population. In the graph, redness relates to usage
positively. The most red and most yellow dots are all in the least
populated states/least patroned boards.


#+BEGIN_SRC ipython :session :file ./img/py6320Yhv.png :exports results :tangle ./politics.py
colors = cm.YlOrRd(state_usage.usage)

ax.spines["top"].set_visible(False)
ax.spines["right"].set_visible(False)

ax.get_xaxis().tick_bottom()
ax.get_yaxis().tick_left()

plt.ylabel("Patronage", fontsize=12)
plt.xlabel("Population", fontsize=12)

plt.suptitle('Patronage vs Population, heatmapped by Usage', fontsize=12)


plt.scatter(state_usage.population, state_usage.patronage, color=colors)
#+END_SRC
#+RESULTS:
[[file:./img/py6320Yhv.png]]
** Politics
*** Posts over Trumpism  :ignore:
It seems that the distribution of posts is weighted on the Democrat's
side of the spectrum:
#+BEGIN_SRC ipython :session :file ./img/py22415X0p.png :exports results
  post_politics = usa.join(states.trumpism, how="outer", on="state")
  post_politics.trumpism.plot(kind="hist", bins=20, color=["#FF9911"], 
                              title="Distribution of posts by politics")
#+END_SRC
#+RESULTS:
[[file:./img/py22415X0p.png]]

However, Democratic registration outweighs Rebpublican voting rates
slightly. We can visualize this preference a bit differently by
finding the average post trumpism, and comparing it to national voting
trends:
#+BEGIN_SRC ipython :session :exports code
  avg_post_trumpism = post_politics.trumpism.mean()
  trump_votes = voting.sum().trump
  clinton_votes = voting.sum().clinton
  national_trumpism = trump_votes/(trump_votes + clinton_votes)
#+END_SRC

#+RESULTS:

It's a bit more clear here that the skew of trumpism distribution is
weighted a bit on the left, though the mean is quite close to what's
expected, at about 48% of Trump+Clinton votes. The skewness of
distribution is expected, and in line with my original hypothesis;
more liberal states can expect some discent from the socially charged
Republican minority, while very Trump states have less inspiration for
outcry. In general, it would seem the most divided states see the most
traffic, with those less divided being prominently liberal. The even mean
in preserved by what seems to be in states that Trump won by a
relatively small margin.
#+BEGIN_SRC ipython :session :exports results :results output raw :results org output 
  # Some printing
  print(("Posts have a mean trumpism of {0:.2f}% and a median of " + 
         "{1:.2f}%. Trump voters\nseem to show {2:+.2f}%  " + 
         "representation on CL politics vs General Election\n" +
         "results.").format(avg_post_trumpism*100,
                            post_politics.trumpism.median(),
                            (avg_post_trumpism/national_trumpism)*100-100))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
Posts have a mean trumpism of 48.42% and a median of 0.50%. Trump voters
seem to show -1.17%  representation on CL politics vs General Election
results.
#+END_SRC

An alternative representation that may make this skew a bit more apparent:
#+BEGIN_SRC ipython :session :file ./img/py26878eDX.png :exports results 
  ax = plt.subplot(111)

  ax.spines["top"].set_visible(False)
  ax.spines["right"].set_visible(False)

  ax.get_xaxis().tick_bottom()
  ax.get_yaxis().tick_left()

  post_trumpism_tot = post_politics.trumpism.plot(
      kind="density", 
      title="PDF estimation of Trumpism w/ mean",
      sharey=True)
  plt.axvline(post_politics.trumpism.mean(), color='r', linestyle='dashed', linewidth=.5)
  #+END_SRC

#+RESULTS:
[[file:./img/py26878eDX.png]]
*** Usage vs Trumpism
We can see the correlations between patronage, population, and usage,
here. We of course expect correlation between patronage and population
to be quite high: states with more people generally have more
posts. However we see W Below, positive correlation is pictured by
redness, while negative is pictures by blueness. Darkness visualizes
closeness.
#+BEGIN_SRC ipython :session :file ./img/py2241F8fd.png :exports results
  corr = states.filter(["patronage", "usage", "trumpism", "population"]).corr()
  fig, ax = plt.subplots(figsize=(4, 4))
  ax.matshow(corr, cmap=plt.cm.seismic)
  plt.xticks(range(len(corr.columns)), corr.columns);
  plt.yticks(range(len(corr.columns)), corr.columns);
#+END_SRC

#+RESULTS:
[[file:./img/py2241F8fd.png]]

Note the correlation between trumpism and usage. Also, the correlation
between patronage and usage coincides with how you'd expect boards
with the least diversity to be disproportionately unfrequented. Boards
with few posts become ghost towns. Here are the pearson correlation
numbers behinds the colors:

#+BEGIN_SRC ipython :session :results output raw drawer :noweb yes :exports results
print_df(corr, rnd=3)
#+END_SRC
#+RESULTS:
:RESULTS:
|            | patronage |  usage | trumpism | population |
|------------+-----------+--------+----------+------------|
| patronage  |         1 |  0.336 |   -0.363 |      0.895 |
| usage      |     0.336 |      1 |   -0.302 |     -0.008 |
| trumpism   |    -0.363 | -0.302 |        1 |     -0.344 |
| population |     0.895 | -0.008 |   -0.344 |          1 |
:END:

* Text Qualities
Text usage is interesting to consider, but difficult to evaluate
semantically. While sampling provides some surprising ideas about the
data, proving any derivative ideas is a bit difficult. The following
is and effort to support the introduction of this blog post.
** Words
pop_english_words is a list of the most popular words in
English. Grabbed from http://www.world-english.org/english500.htm.
#+BEGIN_SRC ipython :session :exports none :tangle ./politics.py
pop_english_words = ["the", "re", "a", "s", "t", "i", "of", "to", "and", "and", "in", "is", "it", "you", "that", "he", "was", "for", "on", "are", "with", "as", "I", "his", "they", "be", "at", "one", "have", "this", "from", "or", "had", "by", "hot", "but", "some", "what", "there", "we", "can", "out", "other", "were", "all", "your", "shit", "when", "up", "use", "word", "how", "said", "an", "each", "she", "which", "do", "their", "time", "if", "will", "way", "about", "many", "fuck", "then", "them", "would", "write", "like", "so", "these", "her", "long", "make", "thing", "see", "him", "two", "has", "look", "more", "day", "could", "go", "come", "did", "my", "sound", "no", "most", "number", "who", "over", "know", "water", "than", "call", "first", "people", "may", "down", "side", "been", "now", "find"]
#+END_SRC

#+RESULTS:

#+BEGIN_SRC ipython :session :exports none :tangle ./politics.py
  from collections import Counter

  def post_words(df, no_pop=False):
      wds = re.findall(r'\w+', df.title.apply(lambda x: x + " ").sum())
      if no_pop:
          # pop_english_words is a list of the most popular (and boring) English
          # words. E.g., "and", "to", "the", etc.
          wds = [word for word in wds if word.lower() not in pop_english_words]
      return  wds

  def words(df=usa, no_pop=False):
      # word counts across all posts
      wds = post_words(df, no_pop)
      word_counts = Counter([word.lower() for word in wds])
      wd_counts = zip(*[[word, count] for word, count in word_counts.iteritems()])
      corpus = pd.Series(wd_counts[1], index=wd_counts[0]).rename("counts")

      return corpus.sort_values(ascending=False)
#+END_SRC

#+RESULTS:

Probably don't care about stupid common words.
#+BEGIN_SRC ipython :session :exports none :tangle ./politics.py
# words function grabs all the words from df, with option to exclude popular words
posts_corpus = words(df=usa, no_pop=True)

usa_words_full = post_words(df=usa)
usa_words = post_words(df=usa, no_pop=True)

posts_sum = " ".join(usa_words) # good estimate of sum of all posts, minus popular words
#+END_SRC

#+RESULTS:

** Substrings
Find substrings in posts
#+BEGIN_SRC ipython :session :exports none :tangle ./politics.py
  def find_strs(substr, df=usa):
      """
      Get all titles from usa that have substr in their post title. Add some data on capitalization.
      """

      find = lambda s: (1 if re.search(substr, s, re.IGNORECASE) else np.nan)

      return df.title[df.title.map(find) == 1].rename("*" + substr + "*", inplace=True)

  def categ_strs(findings):
      """
      Return a list of
      """
      s = findings.name[1:-1]
      find = lambda sub, string: (1 if re.search(sub, string) else np.nan)

      proper = findings.apply(lambda x: find(s[0].upper() + s[1:].lower(), x)).rename("proper")
      cap = findings.apply(lambda x: find(s.upper(), x)).rename("uppercase")
      low = findings.apply(lambda x: find(s.lower(), x)).rename("lower")

      return pd.concat([proper, cap, low], axis=1)

  def eval_strs(string, df=usa):
      findings = find_strs(string, df)
      return categ_strs(findings).join(findings)
#+END_SRC

#+RESULTS:
** Analysis :ignore:
*** intro :ignore:
   :PROPERTIES:
   :ATTACH_DIR_INHERIT: t
   :END:
We find that "against", "how", and "won" have extreme preference for
"liberal" states. The reasons are in fact not obvious. Some random
sampling of such posts reveals possibly surprisingly pro-Trump
sentiment:
#+BEGIN_SRC ipython :session :results output raw drawer :noweb yes  :exports results
  print_df(pd.DataFrame(pd.concat([find_strs("thought"),
                                   find_strs("usa"),
                                   find_strs("won")]).rename(
                                       "title")).sample(5), 
           rnd=3)
#+END_SRC
#+RESULTS:
:RESULTS:
|       | title                                                              |
|-------+--------------------------------------------------------------------|
| 36480 | Dick Cheney--Do The USA A favor                                    |
|  3772 | RE Proof of massive Democrats voter fraud thousands voter fraud .. |
| 32136 | defense industry aka military industrial complex owns usa media    |
| 31492 | re Why Trump won, Americans are stupid                             |
| 28156 | Thought for the Day                                                |
:END:

Looking at the general word sentiment, we see clearly has vastly
Disproportionately, Trump and Obama are discussed. Note that "hillary"
and "clinton" are surprisingly not mentioned as much as you might
think. "Clinton", in fact, is mentioned less freqeuntly than
"Donald". It may be that a month after the election, "hillary" talk
has already begun to significantly subside. It's impossible to know
for sure, as CL does not hold on to their posts for longer than a
week.
#+BEGIN_SRC ipython :session :file ./img/py31406ImT.png :exports results :cache yes
p = posts_corpus[:25].sort_values(ascending=True)

ax = p.plot(kind="bar", color="#662200", grid=True)

ax.spines["top"].set_visible(False)
ax.spines["right"].set_visible(False)

ax.get_xaxis().tick_bottom()
ax.get_yaxis().tick_left()

plt.ylabel("Occurences", fontsize=12)

plt.suptitle('Word usages', fontsize=14)

ax.spines["top"].set_visible(False)
ax.spines["right"].set_visible(False)

ax.get_xaxis().tick_bottom()
ax.get_yaxis().tick_left()
#+END_SRC

#+RESULTS[4cfeb62c1d4cb9d2e0ccc865f9f60fd806d810e9]:
[[file:./img/py31406ImT.png]]

Split a series into chunks such that values.sum() = val (or as close
as possible, greedily) so we can wee how the diversity of words is distributed:
#+BEGIN_SRC ipython :session :exports results :cache yes  
  def splicer(ss, val):
    indices = ss.index.tolist()
    if len(indices) <= 1:
      return pd.Series(ss[index[0]], index=[[indices[0]]])

    left = [ss.index[0]]
    right = ss.index[1:].tolist()

    s = ss[left[0]]
    while s < val and len(right) > 0:
      i = right.pop(0)
      left.append(i)
      s += ss[i]
    return [ss.filter(left)] + (splicer(ss.filter(right), val) if len(right) > 0 else [])
#+END_SRC

#+RESULTS[994f902e65368e5e1227028a7cb4ed24a84822a7]:

#+BEGIN_SRC ipython :session :file ./img/pyF7JjmI.png :exports results
  chunks = splicer(posts_corpus, posts_corpus.iloc[0])

  ax = plt.subplot()
  
  ax.spines["top"].set_visible(False)
  ax.spines["right"].set_visible(False)

  ax.get_xaxis().tick_bottom()
  ax.get_yaxis().tick_left()

  plt.ylabel("Occurences", fontsize=12)

  plt.suptitle('', fontsize=14)

  ax.spines["top"].set_visible(False)
  ax.spines["right"].set_visible(False)

  ax.get_xaxis().tick_bottom()
  ax.get_yaxis().tick_left()

  plt.bar(np.arange(0, len(chunks)), np.array([len(c) for c in chunks]))

  
#+END_SRC

#+RESULTS:
[[file:./img/pyF7JjmI.png]]

*** "trumps"
**** Patronage :ignore:
#+BEGIN_SRC ipython :session :exports none :tangle ./politics.py
trumps = eval_strs("trump").join(usa.state, how="inner")
trumps_by_state = trumps.groupby("state").count().join(states).drop(["clinton", "trump"], axis=1)
up_over_trumps = (trumps_by_state.uppercase/trumps_by_state["*trump*"]).rename("uppercase usage")
prop_over_trumps = (trumps_by_state.proper/trumps_by_state["*trump*"]).rename("propercase usage")
trumps_over_pat = (trumps_by_state["*trump*"]/trumps_by_state.patronage).rename("trumps usage")
trumps_by_state = trumps_by_state.join([prop_over_trumps, up_over_trumps, trumps_over_pat], how="outer")
#+END_SRC

#+RESULTS:

**** Politics :ignore:
The more pro-Trump your state, the less likely you are to use "Trump" over "TRUMP"
#+BEGIN_SRC ipython :session :file ./img/py6320cup.png :exports results :tangle ./politics.py
  trumps_vs_trumpism = trumps_by_state.sort_values(
      "trumpism", ascending=True).filter(["propercase usage",
                          "uppercase usage"])

  trumps_vs_trumpism.plot(kind="bar", stacked=True, figsize=(10, 5))

  ax = plt.subplot()

  ax.spines["top"].set_visible(False)
  ax.spines["right"].set_visible(False)

  ax.get_xaxis().tick_bottom()
  ax.get_yaxis().tick_left()

  plt.xlabel("States, in order of trumpism")

  ax.spines["top"].set_visible(False)
  ax.spines["right"].set_visible(False)

  ax.get_xaxis().tick_bottom()
  ax.get_yaxis().tick_left()
#+END_SRC

#+RESULTS:
[[file:./img/py6320cup.png]]

Looking at the distribution of "trump" posts across trumpism looks
much the same as the distribution of all posts across trumpism:
#+BEGIN_SRC ipython :session :file .img/py268781zz.png :exports results
    post_politics.trumpism.plot(kind="density", linewidth=0.8)

    ax = plt.subplot()

    ax.spines["top"].set_visible(False)
    ax.spines["right"].set_visible(False)

    ax.get_xaxis().tick_bottom()
    ax.get_yaxis().tick_left()

    plt.ylabel("Occurences", fontsize=12)

    ax.spines["top"].set_visible(False)
    ax.spines["right"].set_visible(False)

    ax.get_xaxis().tick_bottom()
    ax.get_yaxis().tick_left()

    trumps_trumpism = trumps.join(post_politics.trumpism)

    trumps_trumpism.trumpism.plot(kind="density", 
                                  title="PDF of trumpism for "  +  
                                  "posts containing 'Trump'",
                                  linewidth=2)
    plt.axvline(trumps_trumpism.trumpism.mean(), color='r',
                linestyle='dashed', linewidth=.5)
#+END_SRC

#+RESULTS:
[[file:.img/py268781zz.png]]


Democratic states seem to prefer using "TRUMP" much more than "Trump":
#+BEGIN_SRC ipython :session :file ./img/py26878b0D.png :exports results
  cap_trumps = trumps_trumpism[trumps_trumpism.uppercase > 0]

  ax = plt.subplot()

  ax.spines["top"].set_visible(False)
  ax.spines["right"].set_visible(False)

  ax.get_xaxis().tick_bottom()
  ax.get_yaxis().tick_left()

  ax.spines["top"].set_visible(False)
  ax.spines["right"].set_visible(False)

  ax.get_xaxis().tick_bottom()
  ax.get_yaxis().tick_left()

  cap_trumps.trumpism.plot(kind="density", 
                           title="PDF of trumpism for posts " \
                           "containing 'TRUMP'",
                           color='blue', linewidth=1.5)
  plt.axvline(cap_trumps.trumpism.mean(), color='r',
              linestyle='dashed', linewidth=.5)
#+END_SRC

#+RESULTS:
[[file:./img/py26878b0D.png]]

It isn't clear why there seems to be preference for capitalization of
"TRUMP" among Dem states; are mostly angry and disparaging,
supportive, or a bit of both? Some random sampling of particularly
liberal states might provide some clues:
#+BEGIN_SRC ipython :session :exports code
  liberal_sample = trump_posts[trump_posts.trumpism < .45].sample(5)
  #+END_SRC  

  #+RESULTS:

  #+BEGIN_SRC ipython :session :exports results :results output org drawer :noweb yes
  print("Selecting states that are espectially " \
        "anti-trump:\n")
  print_df(pd.DataFrame(liberal_sample.title))

  print("Politically liberal states composing " +
        "the above sampling:\n{}.".format(
             ", ".join("{}".format(r) for r in liberal_sample.state.unique())))
#+END_SRC

#+RESULTS:
:RESULTS:
Selecting states that are espectially anti-trump:

|       | title                                                                  |
|-------+------------------------------------------------------------------------|
|  9333 | Donald Trumpet                                                         |
|  2472 | Trump win shatters Obama legacy                                        |
| 12350 | re2:Why will voters turn against Trump? (berkeley)                     |
| 10915 | 2RE: Not in Chico, are you proud of Trump's policies? (berkeley) (oakl |
| 32367 | (tiny hands, pussy grabber) Trump has to be in court in December 2016  |
Politically liberal states composing the above sampling:
Oregon, Illinois, California, Washington.
:END:

*** Unicode
I was curious about non-ascii usage, and so I used to following code
to catch them.
#+BEGIN_SRC ipython :session :exports code
def check_ascii(post):
    """
    Determines whether a title is encodable as ascii
    """
    try:
        post.encode('ascii')
        return True
    except UnicodeError:
        return False

ascii_posts = usa[usa.title.apply(check_ascii)]
nonascii_posts = usa[~usa.title.apply(check_ascii)]
distinct_states = nonascii_posts["state"].unique()
#+END_SRC
The number of posts containing non-ascii characters was surprisingly small:
#+BEGIN_SRC ipython :session   :exports results :results output raw drawer :noweb yes
print ("{0:,} of {1:,} total posts were non-ascii ({2:.2f}%), confined to {3} "
       + "states.").format(len(nonascii_posts),
                       len(usa),
                       len(nonascii_posts)/float(len(usa)) * 100,
                       len(distinct_states))
#+END_SRC
#+RESULTS:
:RESULTS:
219 of 38,324 total posts were non-ascii (0.57%), confined to 22 states.
:END:
However, influence for these posts can be seen by looking at the main
outlier, Pennsylvania:
#+BEGIN_SRC ipython :session  :exports code :tangle ./politics.py :results output raw drawer :noweb yes
  pennsylvania = nonascii_posts[nonascii_posts["state"] == "Pennsylvania"]
  pennsylvania.groupby("region").count()
  penn_lenn = float(len(pennsylvania.title))

  post_uniqueness = (penn_lenn-pennsylvania.title.nunique())/penn_lenn * 100
#+END_SRC

#+BEGIN_SRC ipython :session  :exports results :tangle ./politics.py :results output raw drawer :noweb yes
  print("{:.2f}% of non-ascii Pennsylvania posts " + 
        "are completely unique.".format(post_uniqueness))
#+END_SRC

#+RESULTS:
:RESULTS:
58.93% of non-ascii Pennsylvania posts are completely unique.
:END:

We can use a SequenceMatcher to test the similarity of the strings in
the pool:
#+BEGIN_SRC ipython :session  :exports code
  import itertools
  from difflib import SequenceMatcher

  def avg_similarity(posts):
    def similarity(a, b):
      return SequenceMatcher(None, a, b).ratio()

    sim_sum = 0
    title_product = itertools.product(posts.title, posts.title)
    for title_pair in title_product:
      sim_sum += similarity(*title_pair)

    avg_sim = sim_sum/(len(posts)**2)
    return avg_sim
#+END_SRC

#+RESULTS:

We then can run this over all non-ascii posts to get an idea of how
much silliness is going on with these posts:
#+BEGIN_SRC ipython :session :exports results :results output raw drawer :noweb yes :cache yes
    print(("The average similarity of all non-ascii posts is " +
           "{:.2f} while that \nof only those in Pennsylvania is " +
           "{:.2f}. The average for all posts in\nall regions is " +
           "{:.2f}.")).format(avg_similarity(nonascii_posts),
                              avg_similarity(pennsylvania),
                              avg_similarity(usa.sample(200)))
#+END_SRC
#+RESULTS[476dd7f4a7ffdd0b489da55c7e6b1b846251b992]:
:RESULTS:
The average similarity of all non-ascii posts is 0.19 while that 
of only those in Pennsylvania is 0.38. The average for all posts in
all regions is 0.18.
:END:

It would seem that a single Trump memester is responsible for this
chaos in Pennsylvania. I suspect that these crazy unicode posts are
mostly done by a very small set of people in general, though there is
no good way to tell. 
*** Politics [0/2]
**** TODO Diversity of words vs trumpism
**** "liberals" vs "conservatives"
***** Pluralization
The singular version of "conservative" is used a bit more than half as
much as the pluralization. By contrast, the singular version of
"liberal" is used more than twice as much as the pluralization. I
suspect this is because "liberal" is a perjorative in common
nomenclature, while "conservative" doesn't really hold the same weight
as an insult:
#+BEGIN_SRC ipython :session :exports results :results output raw drawer :noweb yes
print(" singular/plural:\n" +
      "'conservative': {0:.3f}\n" +
      "'liberal': " +
      "{1:.3f}").format(posts_corpus["conservative"]/float(posts_corpus["conservatives"]),
                          posts_corpus["liberal"]/float(posts_corpus["liberals"]))

#+END_SRC
#+RESULTS:
:RESULTS:
 singular/plural:
'conservative': 0.628
'liberal': 2.198
:END:
***** Usage
"liberal" is used far more often than "conservative". The
pluralizations, respectively, are comparitively not quite as
distinguished. This is expected, for previously mentioned reasons;
pluralizations may still be used as a means to negatively generalize.
#+BEGIN_SRC ipython :session :exports results :results output raw drawer :noweb yes
  liberal = float(posts_corpus["liberal"])
  liberal_p = float(posts_corpus["liberals"])
  conserv = float(posts_corpus["conservative"])
  conserv_p = float(posts_corpus["conservatives"])

  print ("liberal/conservative: {0:.2f}\n" +
         "liberals/conservatives: {1:.2f}\n" +
         "liberal(s)/conservative(s): {2:.2f}" +
         "") .format(liberal/conserv,
                     liberal_p/conserv_p,
                     (liberal+liberal_p)/(conserv+conserv_p))

#+END_SRC
#+RESULTS:
:RESULTS:
liberal/conservative: 18.07
liberals/conservatives: 5.16
liberal(s)/conservative(s): 10.14
:END:
***** Capitalization
We here see that, among democrats, "liberal" is capitalized at a rate
13x greater than the rate of capitalization of "conservative". We also
see that lowecase usage preference is completely neglible.
#+BEGIN_SRC ipython :session :exports code
lib_cap = eval_strs("trump").sum(numeric_only=True)
conserv_cap = eval_strs("liberal").sum(numeric_only=True)

lib_con_cap_rat = (lib_cap/conserv_cap).rename("Dem/Rep capital rates for 'trump'")
#+END_SRC

#+RESULTS:

#+BEGIN_SRC ipython :session :exports results :results output raw drawer :noweb yes
print_df(pd.DataFrame(lib_con_cap_rat))
#+END_SRC
#+RESULTS:
:RESULTS:
|           | Dem/Rep capital rates for 'trump' |
|-----------+-----------------------------------|
| proper    |                           10.5951 |
| uppercase |                           13.4286 |
| lower     |                           1.07721 |
:END:

**** Semantics
I figured that a natural way to go about proving my hypothesis
outlined in this blog's introduction would be semantic analysis. I
quickly decided that this was, with it's present implementation, at
least, not the way to go about it. The following code will run
semantic analysis using the popular NLTK package. The results are
dubious.
#+BEGIN_SRC ipython :session :exports code :cache yes
    from textblob import TextBlob

    def semants(text):
        blob = TextBlob(text)
        ss = 0
        for sentence in blob.sentences:
            ss += sentence.sentiment.polarity
        return float(ss)/len(blob.sentences)

    # package does not like non-ascii encodings
    trumps_ascii = trumps[trumps["*trump*"].apply(check_ascii)]


    usa_sentiment = post_politics.join(ascii_posts.title.apply(semants).rename("sentiment"))
    trumps_sentiment= usa_sentiment.filter(trumps_sentiment.index, axis=0)
#+END_SRC

#+RESULTS[f9c165e005384b105d899de515d25e9a2578b73a]:

However, the results, and general output of the semantic analyzer,
were quite unconvincing, even if only interpreted as a binary measure:
#+BEGIN_SRC ipython :session :exports both :results output raw drawer :noweb yes
  #    print_df(usa_sentiment.filter(["title", "sentiment"]).sample(5))
  zero_sents = len(usa_sentiment[usa_sentiment.sentiment == 0])
  print("Number of posts with 0 " \
        "sentiment: {0:,} ({1:.2f}%).".format(zero_sents, 
                                       float(zero_sents)/len(usa_sentiment)*100))
#+END_SRC

#+RESULTS:
:RESULTS:
Number of posts with 0 sentiment: 25,632 (66.88%).
:END:

* Conclusion
The distribution posts and the favor of those posts across the
politics sections is somewhat surprising. I suspect that this is
evidence of cultural normalization in the face of
resistance+anonimity: faceless, nameless interaction coupled with
outspokenness against relatively strict local social norms. This has
proven more difficult to prove than I initially suspected. While any
amount of ransom sampling of the posts allows me to be confident in
this theory, convincing proof would most likely involve a tedious,
exhausive effort.

* Notes about this document
This document is, in its original form, an emacs org-mode
organizational markup document that supports interactive programming
and exporting quite thoroughly. It exports to a variety of formats
(html, latex, markdown, etc). It's quite powerful, and allows me to
tailor what headers are exported, what code is exported, what code
results, etc. The original document, if viewed in org-mode in emacs,
is quite a bit larger, containing all of the code used for the
project, most of which is not shown in markdown exports. Therefore, if
you view this document on github, you will see a truncated version
much like the version you are likely viewing now. You can view on
github a .ipynb and a .py export for the complete code of the
document. Obviously, they won't include the organization and
commentary. You can look at the raw contents of the .org file if
curious (github will export primitively to html by default for
display), or check out this [[http://kozikow.com/2016/05/21/very-powerful-data-analysis-environment-org-mode-with-ob-ipython/comment-page-1/#comment-240][blog on interactive python programming in
emacs org-mode]].
* Meta  :noexport:
** Trump Word Cloud
#+BEGIN_SRC ipython :session :file :exports results :tangle ./politics.py :cache yes
  from os import path
  from PIL import Image

  from wordcloud import WordCloud

  d = path.dirname(".")

  plt.figure(num=None, figsize=(10, 8))

  trump_mask = np.array(Image.open(path.join(d, "img/Trump_silhouette.png")))

  wc = WordCloud(background_color="white", max_words=2000, mask=trump_mask)

  wc.generate(posts_sum)

  wc.to_file(path.join(d, "img/Trump_test.png"))

  plt.imshow(wc)
  plt.axis("off")
  plt.figure()
  plt.imshow(trump_mask, cmap=plt.cm.gray)
  plt.axis("off")

  plt.show()

#+END_SRC
#+RESULTS[36252510400e47ae15b37acc15a3f03f4ef80328]:
: <matplotlib.figure.Figure at 0x7f5358d0a110>
