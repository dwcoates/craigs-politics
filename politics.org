#+TITLE: Babel Powered Jupyter Notebook
#+OPTIONS: toc:nil


# <h1 align="center"><font color="0066FF" size=110%>Simple Notebook</font></h1>


* Introduction
#+BEGIN_SRC ipython :session :file  :exports both :tangle ./politics.py
%matplotlib inline
import numpy as np
import pandas as pd
import scipy
from scipy import stats
import matplotlib as mpln
import matplotlib.pyplot as plt

import pprint as pp
import pickle
import re

pd.options.display.max_colwidth = 1000
#+END_SRC

#+RESULTS:
* Preparing Data
** Grab CL Data
#+BEGIN_SRC ipython :session :file  :exports both  :tangle ./politics.py
# read us data collected by craigcrawler 
usa_raw = pd.read_csv("data/us.csv", index_col=0)
post_count_total_raw = len(usa_raw)
post_count_by_state_raw = usa_raw.groupby("state").count()["title"]#.sort_values(ascending=False)
post_count_by_region_raw = usa_raw.groupby("region").count()["title"]#.sort_values(ascending=False)

print ("\n{0:,} total posts exctracted from {3:,} regions over {4} "+ 
       "state. The most popular\nstate was {1}, and the most " + 
       "popular region was, surprisingly, {2}.").format(post_count_total_raw,
                                                        post_count_by_state_raw.index[0],
                                                        post_count_by_region_raw.index[0],
                                                        len(post_count_by_region_raw),
                                                        len(post_count_by_state_raw))

#+END_SRC
#+RESULTS:
** U.S. Census 2010
- US census data for 2010 from the census bureau
- Census data is not labelled exactly as my data is. Some states are named a little differently,
and regions are almost never named similarly. These have to be resolved.
*** Geo Keys
#+BEGIN_SRC ipython :session :file  :exports both :tangle ./politics.py
# Keys for geography stuff. Table is an index table.
# These keys are used as index for census table.
GEO_NAME = "GEO.display-label"
GEO_KEY = "GEO.id"
state_keys = pd.read_csv("data/census/DEC_10_DP_G001_with_ann.csv")[1:].set_index(GEO_KEY)

state_keys = state_keys.filter([GEO_NAME])[:52]
state_keys = state_keys[state_keys[GEO_NAME]!= "Puerto Rico"]
#+END_SRC

#+RESULTS:

*** Census Data
#+BEGIN_SRC ipython :session :file  :exports both :tangle ./politics.py
  # keys for the census data. Only really care about two of them (there are hundreds):
  TOT_NUM_ID = "HD01_S001" # total number key
  TOT_PER_ID = "HD02_S001" # total percent key

  census = pd.read_csv("data/census/DEC_10_DP_DPDP1_with_ann.csv")[1:].set_index(GEO_KEY)

  census = census.filter([TOT_NUM_ID])
  census = census.join(state_keys, how="right")
  census.columns = ["population", "state"]
  census.set_index("state", inplace=True)
    
  def correct_stat(s):
      """
      Some states have extra information for population. 
      Example: 25145561(r48514)
      """
      loc = s.find("(")
      return int(s[:loc] if loc > 0 else s)

  census.population = census.population.apply(correct_stat)
  
  census = census.drop("District of Columbia")
#+END_SRC

#+RESULTS:

[5 rows x 375 columns]
** U.S. 2016 Election
#+BEGIN_SRC ipython :session :file  :exports both :tangle ./politics.py
  import requests
  from scrapy import Selector

  atlas_url = "http://uselectionatlas.org/RESULTS/data.php?year=2016&datatype=national&def=1&f=1&off=0&elect=0"
  atlas_source = requests.get(atlas_url).text
  select = Selector(text=atlas_source).xpath('//*[@id="datatable"]/tbody/tr')

  convert = lambda s: int(s.replace(',', ''))
  vote_names = map(str, select.xpath('td[3]/a/text()').extract())
  # Correct name for DC
  vote_names[8] = "District of Columbia"
  clinton_votes = map(convert, select.xpath('td[17]/text()').extract())
  trump_votes = map(convert, select.xpath('td[18]/text()').extract())

  gen_votes = pd.DataFrame({"clinton": clinton_votes, "trump": trump_votes}, index=vote_names)

  trump_favor = pd.DataFrame(gen_votes["trump"]/gen_votes.sum(axis=1), columns=["trumpism"], index=vote_names)  
  voting = gen_votes.join(trump_favor).sort_values("trumpism", ascending=False)  
  voting = voting.drop("District of Columbia")

  # for pretty printing
  space = pd.DataFrame([["------", "------", "------"]],index=["*SPACE*"], columns=voting.columns) 
  pd.concat([voting[:5], space, voting[-5:].sort_values("trumpism")])
#+END_SRC

#+RESULTS:
#+begin_example
               clinton    trump  trumpism
Wyoming          55973   174419  0.757053
West Virginia   188794   489371  0.721611
North Dakota     93758   216794  0.698092
Oklahoma        420375   949136  0.693047
Idaho           189765   409055  0.683102
*SPACE*         ------   ------    ------
Hawaii          266891   128847  0.325587
California     8753788  4483810  0.338718
Vermont         178573    95369  0.348136
Massachusetts  1995196  1090893  0.353487
Maryland       1677928   943169  0.359838
#+end_example

** Preprocess Data
Some preprocessing to check data corrupted files
#+BEGIN_SRC ipython :session :file  :exports both :tangle ./politics.py
  print "Data tests... \n\nAssertions Passed\n\n"

  # Confirm all expected regions and states present
  assert len(usa_raw["state"].unique()) == 52 # expected number of states
  assert len(usa_raw["region"].unique()) == 416  # expected number of regions
 
  # Confirm that there are no posts without regions/states. Not all CL 
  # regions have subregions, so it's okay for null subregions.
  assert len(usa_raw[usa_raw["state"].isnull()].index) == 0
  assert len(usa_raw[usa_raw["region"].isnull()].index) == 0

  # Find regions/subregions for which there are no posts
  postless_regions = usa_raw[usa_raw["title"].isnull()]  
  postless_regions_times = usa_raw[usa_raw["date"].isnull()]

  # not actually an effective test, but good enough
  assert len(postless_regions) == len(postless_regions_times)

  print(("{0:,} regions/subregions over {1} states without " + 
         "any posts.").format(len(postless_regions), postless_regions["state"].nunique()))  
#+END_SRC

#+RESULTS:

Drop unneeded data
#+BEGIN_SRC ipython :session :file  :exports both :tangle ./politics.py
# Drop empty regions.
usa = usa_raw.dropna(subset=["title", "date"], how="any", axis=0)
assert len(postless_regions) == len(usa_raw)-len(usa)

# Get rid of territories (Guam, Puerto Rico)
usa = usa[usa["state"] != "Territories"]
usa = usa[usa["state"] != "District of Columbia"]
#+END_SRC

#+RESULTS:

Confirm Census Data
#+BEGIN_SRC ipython :session :file  :exports both :tangle ./politics.py
assert set(usa.state.unique()) == set(census.index) and len(usa.state.unique() == len(census.index))

print "Census data complete"
#+END_SRC

#+RESULTS:

Confirm Election Data
#+BEGIN_SRC ipython :session :file  :exports both :tangle ./politics.py
assert set(usa.state.unique()) == set(voting.index) and len(usa.state.unique() == len(voting.index))

print "Voting data complete"
#+END_SRC
#+RESULTS:
* State Popularity
** Data
*** Grab Data
#+BEGIN_SRC ipython :session :file  :exports both :tangle ./politics.py
patronage = pd.DataFrame(usa.groupby('state').size(), columns=["patronage"])

print "\nTop ten most popular states"
print usage_by_state[:10]
#+END_SRC 
#+RESULTS:

*** Normalization
#+BEGIN_SRC ipython :session :file  :exports both :tangle ./politics.py
    cl_by_state = patronage.join(census, how="inner")
    usage = cl_by_state.apply(
        lambda df: df["patronage"] / float(df["population"]), axis=1)

    # Weight for mean usage = 1.000
    weight = weight/weight.max() # maximum popularity is 1
    weighted_usage = pd.DataFrame((usage/usage.max()),
                                   columns=["popularity"])

    state_usage = pd.concat([cl_by_state, weighted_usage],
                            axis=1).sort_values("patronage",
                                                ascending=False)
#+END_SRC
#+RESULTS:

** Analysis
*** Patronage
#+BEGIN_SRC ipython :session :file /home/dodge/workspace/craig-politics/img/py6320LXp.png :exports both :tangle ./politics.py
pat = state_usage.sort_values("patronage", ascending=True)
x = np.arange(len(pat))

plt.bar(x, pat.popularity)
#+END_SRC

#+RESULTS:
[[file:/home/dodge/workspace/craig-politics/img/py6320LXp.png]]

#+BEGIN_SRC ipython :session :file /home/dodge/workspace/craig-politics/img/py6320oYD.png :exports both :tangle ./politics.py
x = np.arange(len(pat))

plt.bar(x, pat.population)
#+END_SRC

#+RESULTS:
[[file:/home/dodge/workspace/craig-politics/img/py6320oYD.png]]

*** Popularity
#+BEGIN_SRC ipython :session :file /home/dodge/workspace/craig-politics/img/py6320lr1.png :exports both :tangle ./politics.py
plt.bar(x, pat.popularity)
#+END_SRC
#+RESULTS:
[[file:/home/dodge/workspace/craig-politics/img/py6320lr1.png]]
#+BEGIN_SRC ipython :session :file /home/dodge/workspace/craig-politics/img/py6320Yhv.png :exports both :tangle ./politics.py
plt.plot(x, state_usage.population.sort_values().values)
#+END_SRC
#+RESULTS:
[[file:/home/dodge/workspace/craig-politics/img/py6320Yhv.png]]
I expect population to relate to patronage linearly.
#+BEGIN_SRC ipython :session :file /tmp/image.png  :exports both :tangle ./politics.py
# Getting rid of California
p1 = state_usage.sort_values("population", ascending=False)[5:]

plt.bar(p1["population"], p1["popularity"])
#+END_SRC
 #+RESULTS:
[[file:/tmp/image.png]]

*** Politics
#+BEGIN_SRC ipython ipython :session :file /tmp/population2.png  :exports both :tangle ./politics.py
states = state_usage.join(voting, how="left").sort_values("popularity")[:50]
plt.hist([states.popularity, states.trumpism], bins=30)
#+END_SRC
#+RESULTS:
[[file:/tmp/population2.png]]

Note the correlation between trumpism and popularity
#+BEGIN_SRC ipython :session :file /home/dodge/workspace/craig-politics/img/py6320k_K.png :exports both
print states.filter(["patronage", "popularity", "normalized", "trumpism"]).corr()
#+END_SRC :tangle ./politics.py
* Text Qualities
** Data
Find strings in posts
#+BEGIN_SRC ipython :session :file /home/dodge/workspace/craig-politics/img/py6320WhL.png :exports both  :tangle ./politics.py
  def find_strs(substr, df=usa):
      """
      Get all titles from usa that have substr in their post title. Add some data on capitalization.
      """
      
      find = lambda s: (1 if re.search(substr, s, re.IGNORECASE) else np.nan)

      return df.title[df.title.map(find) == 1].rename("*" + substr + "*", inplace=True)

  def categ_strs(findings):
      """
      Return a list of 
      """
      s = findings.name[1:-1]
      find = lambda sub, string: (1 if re.search(sub, string) else np.nan)

      proper = findings.apply(lambda x: find(s[0].upper() + s[1:].lower(), x)).rename("proper")
      cap = findings.apply(lambda x: find(s.upper(), x)).rename("uppercase")
      low = findings.apply(lambda x: find(s.lower(), x)).rename("lower")

      return pd.concat([proper, cap, low], axis=1)

  def eval_strs(string, df=usa):
      findings = find_strs(string, df)
      return categ_strs(findings).join(findings)


#+END_SRC
** Analysis
*** General Language
Most popular words in English. Grabbed from http://www.world-english.org/english500.htm
#+BEGIN_SRC ipython :session :file  :exports both
pop_english_words = ["the", "re", "a", "s", "t", "i", "of", "to", "and", "and", "in", "is", "it", "you", "that", "he", "was", "for", "on", "are", "with", "as", "I", "his", "they", "be", "at", "one", "have", "this", "from", "or", "had", "by", "hot", "but", "some", "what", "there", "we", "can", "out", "other", "were", "all", "your", "when", "up", "use", "word", "how", "said", "an", "each", "she", "which", "do", "their", "time", "if", "will", "way", "about", "many", "then", "them", "would", "write", "like", "so", "these", "her", "long", "make", "thing", "see", "him", "two", "has", "look", "more", "day", "could", "go", "come", "did", "my", "sound", "no", "most", "number", "who", "over", "know", "water", "than", "call", "first", "people", "may", "down", "side", "been", "now", "find"]
#+END_SRC

#+RESULTS:

#+BEGIN_SRC ipython :session :file /home/dodge/workspace/craig-politics/py63203mB.png :exports both
  from collections import Counter

  # word counts across all posts
  words = re.findall(r'\w+', usa.title.apply(lambda x: x + " ").sum())
  word_counts = Counter([word.lower() for word in words])
  wcs = zip(*[[word, count] for word, count in word_counts.iteritems()])

  word_counts = pd.Series(wcs[1], index=wcs[0]).rename("word counts")
  word_counts.sort(ascending=False)
#+END_SRC
*** Trumps
**** Patronage
#+BEGIN_SRC ipython :session :file /home/dodge/workspace/craig-politics/img/py6320Qlq.png :exports both :tangle ./politics.py
trumps = eval_strs("trump").join(usa.state, how="inner")
trumps_by_state = trumps.groupby("state").count().join(states).drop(["clinton", "trump"], axis=1)
up_over_trumps = (trumps_by_state.uppercase/trumps_by_state["*trump*"]).rename("uppercase usage")
prop_over_trumps = (trumps_by_state.proper/trumps_by_state["*trump*"]).rename("propercase usage")
trumps_over_pat = (trumps_by_state["*trump*"]/trumps_by_state.patronage).rename("trumps usage")
trumps_by_state = trumps_by_state.join([prop_over_trumps, up_over_trumps, trumps_over_pat], how="outer")
#+END_SRC
**** Politics
The more pro-Trump your state, the less likely you are to use "Trump" over "TRUMP"
#+BEGIN_SRC ipython :session :file /home/dodge/workspace/craig-politics/img/py6320cup.png :exports both :tangle ./politics.py
trumps_vs_trumpism = trumps_by_state.filter(["trumpism", "propercase usage", "uppercase usage", "trumps usage"]).sort_values("trumps usage", ascending=True)[1:]

pd.DataFrame.hist(trumps_vs_trumpism, bins=50)
#plt.hist([prop_over_cap.trumpism, prop_over_cap[""]], bins=30)
#+END_SRC

#+RESULTS:
[[file:/home/dodge/workspace/craig-politics/img/py6320cup.png]]

#+BEGIN_SRC ipython :session :file /home/dodge/workspace/craig-politics/py6320U3u.png :exports both
trump_posts = usa.join(voting, on="state").join(find_strs("trump"), how="inner")

print "Selecting states that are espectially anti-trump:\n{0}".format(t[t.trumpism < .4].title.sample(10))

print "\nPolitically liberal states composing the above sampling:\n{0}".format(t[t.trumpism < .4].groupby("state").sum().index.tolist())
#+END_SRC
**** Trump Language
#+BEGIN_SRC ipython :session :file /home/dodge/workspace/craig-politics/img/py63202C2.png :exports both :tangle ./politics.py
trump_words = ["liberals",
               "conservatives",
               "centipede",
               "cuck",
               "maga",
               "regressive left",
               "shillary",
               "sjw",
               "triggered"]

#+END_SRC
**** Wordcloud
#+BEGIN_SRC ipython :session :file /home/dodge/workspace/craig-politics/py6320RCC.png :exports both
from os import path
from PIL import Image

from wordcloud import WordCloud, STOPWORDS

d = path.dirname("/home/dodge/workspace/craig-politics/")

trump_mask = np.array(Image.open(path.join(d, "Trump_silhouette.png")))

stopwords = set(STOPWORDS)

wc = WordCloud(background_color="black", max_words=2000, mask=alice_mask,
               stopwords=stopwords)

# generate word cloud
wc.generate(usa.title.apply(lambda x: x + " ").sum())

# save to file
wc.to_file(path.join(d, "Trump_test.png"))

# show
plt.imshow(wc)
plt.axis("off")
plt.figure()
plt.imshow(alice_mask, cmap=plt.cm.gray)
plt.axis("off")
plt.show()
#+END_SRC

#+RESULTS:
[[file:/home/dodge/workspace/craig-politics/py6320RCC.png]]

*** Unicode
ascii vs. unicode usage. 
#+BEGIN_SRC ipython :session :file  :exports both :tangle ./politics.py
def check_ascii(post):
    """
    Determines whether a title is encodable as ascii
    """
    try:
        post.encode('ascii')
        return True
    except UnicodeError:
        return False

ascii_titles_tv = usa.title.apply(check_ascii)
ascii_posts = usa[ascii_titles_tv]
nonascii_posts = usa[~ascii_titles_tv]

distinct_states = nonascii_posts["state"].unique()
print ("{0:,} of {1:,} total posts were non-ascii ({2:.2f}%), confined to {3} "
       + "states.").format(len(nonascii_posts),
                       len(usa),
                       len(nonascii_posts)/float(len(usa)) * 100,
                       len(distinct_states))
#+END_SRC

#+RESULTS:

**** Pennsylvania
Pennsylvania has was the preeminent outlier in non-ascii usage per-state
#+BEGIN_SRC ipython :session :file  :exports both :tangle ./politics.py
nonascii_states_count = nonascii_posts.groupby(
    "state").title.nunique().sort_values(ascending=False)
print "\nTop ten most popular unicode states:"
print nonascii_states_count[:10]

pennsylvania = nonascii_posts[nonascii_posts["state"] == "Pennsylvania"]
print pennsylvania["title"].tolist()[0]

print("\nA single Trump memester seems to be responsible for the chaos " +
      "in Pennsylvania.\n" + "I suspect that these crazy unicode posts " +
      "are mostly done by a very small\nset of people, though there is " +
      "no way to tell.")
print "\nRandom sample of 5 non-ascii Pennsylvania posts"
print pennsylvania["title"][:5]

pennsylvania.groupby("region").count()

post_uniqueness = pennsylvania.title.nunique()/float(len(pennsylvania.title))
#+END_SRC

#+RESULTS:
=                   title  date  state  subregion
region                                          
harrisburg, PA        11    11     11          0
lancaster, PA         11    11     11          0
philadelphia           1     1      1          0
pittsburgh, PA         1     1      1          0
reading, PA           10    10     10          0
state college, PA     11    11     11          0
york, PA              11    11     11          0
==<pandas.core.groupby.DataFrameGroupBy object at 0x7fa5c0d57250>
==<pandas.core.groupby.DataFrameGroupBy object at 0x7fa5f43f5050>
==Series([], dtype: int64)
==Empty DataFrame
Columns: [title, date, state, region]
Index: []
=* 
***** Colorado
#+BEGIN_SRC ipython :session :file  :exports both :tangle ./politics.py
print "\n\n{0} regions in Colorado".format(usa[usa['state'] == "Colorado"]["region"].nunique())
#+END_SRC

#+RESULTS:


#+BEGIN_SRC ipython :session :file /home/dodge/workspace/craigp-olitics/img/py6320XN2.png :exports both :tangle ./politics.py
posts = usa.groupby("state")["title"].agg(sum)["Kansas"]
#+END_SRC
      
*** Semantics
#+BEGIN_SRC ipython :session :file /home/dodge/workspace/craig-politics/img/py63201WL.png :exports both :tangle ./politics.py
  from textblob import TextBlob

  def semants(text):
      blob = TextBlob(text)
      ss = 0
      for sentence in blob.sentences:
          ss += sentence.sentiment.polarity

      return float(ss)/len(blob.sentences)
#+END_SRC
#+BEGIN_SRC ipython :session :file /home/dodge/workspace/craig-politics/img/py63202Qe.png :exports both :tangle ./politics.py
semantics = ascii_posts.title.map(lambda x: semants(x)).rename("semants")
semant = eval_strs("trump", df=ascii_posts).join(pd.DataFrame(semantics))
sems_usa = semant.join(usa, how="inner")
trumps_semantics = sems_usa.groupby("state").mean().join(voting, how="inner").sort_values("semants").corr()
#+END_SRC
#+BEGIN_SRC ipython :session :file /home/dodge/workspace/craig-politics/img/py6320Dbk.png :exports both :tangle ./politics.py
total_semants = usa.join(semantics, how="outer").groupby("state").mean().join(voting).sort_values("semants").corr()
#+END_SRC
